<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Lecture 4</title>
<meta name="author" content="(Adolfo De Unánue)"/>

<link rel="stylesheet" href="http://cdn.jsdelivr.net/reveal.js/2.5.0/css/reveal.css"/>
<link rel="stylesheet" href="http://cdn.jsdelivr.net/reveal.js/2.5.0/css/theme/night.css" id="theme"/>
<link rel="stylesheet" href="itam-org-reveal.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'http://cdn.jsdelivr.net/reveal.js/2.5.0/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro' rel='stylesheet' type='text/css'>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide">
<h1>Lecture 4</h1>
<h2>Adolfo De Unánue</h2>
<h2><a href="mailto:adolfo.deunanue@itam.mx">adolfo.deunanue@itam.mx</a></h2>
<h2></h2>
</section>

<section>
<section id="slide-sec-1" data-background="#000fff">
<h2 id="sec-1">Docker</h2>

</section>
<section id="slide-sec-1-1">
<h3 id="sec-1-1">Obtener la imagen</h3>
<pre><code data-trim>

docker pull nanounanue/docker-hadoop
</code></pre>


</section>
<section id="slide-sec-1-2">
<h3 id="sec-1-2">Ejecutar un contenedor</h3>
<pre><code data-trim>


docker run -ti --rm \
  -v /ruta/a/tus/datos/:/home/itam/data \
  -p 2122:2122 -p 2181:2181 -p 39534:39534 -p 9000:9000 \
  -p 50070:50070 -p 50010:50010 -p 50020:50020 -p 50075:50075 \
  -p 50090:50090 -p 8030:8030 -p 8031:8031 -p 8032:8032 \
  -p 8033:8033 -p 8088:8088 -p 8040:8040 -p 8042:8042 \
  -p 13562:13562 -p 47784:47784 -p 10020:10020 -p 19888:19888 \
  -p 8000:8000 \
nanounanue/docker-hadoop /bin/zsh
</code></pre>


</section>
<section id="slide-sec-1-3">
<h3 id="sec-1-3">Contenedor con Hadoop</h3>
<pre><code data-trim>

docker run -ti --name hadoop-pseudo \
  -v /ruta/a/tus/datos/:/home/itam/data \
  -p 2122:2122 -p 2181:2181 -p 39534:39534 -p 9000:9000 \
  -p 50070:50070 -p 50010:50010 -p 50020:50020 -p 50075:50075 \
  -p 50090:50090 -p 8030:8030 -p 8031:8031 -p 8032:8032 \
  -p 8033:8033 -p 8088:8088 -p 8040:8040 -p 8042:8042 \
  -p 13562:13562 -p 47784:47784 -p 10020:10020 -p 19888:19888 \
  -p 8000:8000 -p 9999:9999 \
nanounanue/docker-hadoop
</code></pre>


</section>
<section id="slide-sec-1-4">
<h3 id="sec-1-4">Reiniciar el contenedor</h3>
<pre><code data-trim>

docker start -ai hadoop-pseudo
</code></pre>


</section>
<section id="slide-sec-1-5">
<h3 id="sec-1-5">Conectarse a un contenedor funcionando</h3>
<ul>
<li>Esto podría ser útil para ver, por ejemplo <code>logs</code> o arrancar varios clientes.</li>

<li>Averigua el número del contenedor</li>

</ul>

<pre><code data-trim>

docker ps -a
</code></pre>



<pre><code data-trim>
docker exec -it <CONTENEDOR_ID> /bin/zsh
</code></pre>


<p>
o en nuestro caso:
</p>

<pre><code data-trim>
docker exec -it hadoop_pseudo /bin/zsh
</code></pre>


<ul>
<li><b>Nota</b>: <i>Recuerda que con los primeros 4 dígitos del contenedor basta para identificarlo.</i></li>

</ul>



</section>
<section id="slide-sec-1-6">
<h3 id="sec-1-6">Navegador Web</h3>
<ul>
<li><a href="http://127.0.0.1:50090">Consola de Yarn</a></li>

<li><a href="http://127.0.0.1:50070">Consola de HDFS</a></li>

<li><a href="http://0.0.0.0:8000">HUE - Hadoop User Experience</a>
<ul>
<li><i>Desactivado</i></li>

</ul></li>

</ul>




</section>
<section id="slide-sec-1-7">
<h3 id="sec-1-7">Ejercicio</h3>
<ul>
<li>Explicar los diferentes modos en los que puede ejecutarse <b><b>Apache Hadoop</b></b>.</li>

<li><b>Modo Pseudodistribuido</b>

<ul>
<li>Crea el contendeor de Docker para Hadoop,  vamos a explicar que significa <i>pseudodistribuido</i>.</li>

</ul></li>

</ul>


</section>
</section>
<section>
<section id="slide-sec-2" data-background="#000fff">
<h2 id="sec-2">Apache Hadoop</h2>

</section>
<section id="slide-sec-2-1">
<h3 id="sec-2-1">¿Por qué?</h3>
<ul>
<li>Aunque la capacidad de los discos ha aumentado considerablemente, la velocidad de los mismos no lo ha hecho igual.
<ul>
<li>Los discos actuales de <code>1 Tb</code>, tardan en leerse completos a <code>100 Mb/s</code> cerca de dos a tres horas.</li>
<li>Podemos <i>paralelizar</i> las fuentes en varios discos.
<ul>
<li>Para leerla simultáneamente</li>

</ul></li>
<li>Con varios discos, la <b><b>probabilidad de falla</b></b> aumenta.</li>

</ul></li>
<li>Otro problema es la distribución ¿Cómo combinas varios <code>file systems</code>?</li>

</ul>

</section>
<section id="slide-sec-2-2">
<h3 id="sec-2-2">¿Qué es?</h3>
<ul>
<li>Sistema confiable (<i>realiable</i>) de almacenamiento compartido y de procesamiento de datos.
<ul>
<li><b>Almacenamiento</b>: <i>Hadoop Distributed File System</i>, <code>HDFS</code></li>
<li><b>Procesamiento</b>: Varios <i>frameworks</i> basados en <code>YARN</code>.</li>

</ul></li>

<li>Puede procesar cantidades masivas de datos y escalar conforme crezcan los datos.</li>

<li>Flexibilidad para el procesamiento de datos.
<ul>
<li>No importa la estructura o falta de ella</li>

</ul></li>

<li>Está construido en <code>Java</code>.</li>

</ul>

</section>
<section id="slide-sec-2-3">
<h3 id="sec-2-3">¿Cómo?</h3>
<ul>
<li><code>MapReduce</code> es un sistema de procesamiento <i>batch</i>
<ul>
<li>Permite correr <i>queries</i> contra <b><b>toda</b></b> tu base de datos</li>
<li>Pero el resultado puede tardar minutos, horas, etc&#x2026;</li>
<li>No permite tener a un humano sentado ahí para retroalimentar.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-2-4">
<h3 id="sec-2-4">¿Cómo?</h3>
<ul>
<li>Ahora, gracias a <code>YARN</code> (ver más adelante) tenemos diferentes tipos de procesamiento:
<ul>
<li><i>SQL Interactivo</i>: <code>Impala</code>, <code>Hive</code>, <code>Spark SQL</code>.</li>
<li><i>Iterativos</i>: <code>Spark</code>.</li>
<li><i>Procesamiento de flujos</i>: <code>Storm</code>, <code>Spark Streaming</code>.</li>
<li><i>Búsquedas</i>: <code>Solr</code>.</li>
<li><i>Grafos</i> <code>Spark GraphX</code>.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-2-5">
<h3 id="sec-2-5">¿Por qué no otros sistemas?</h3>
<ul>
<li>¿Por qué no usar un <code>PostgreSQL</code> con muchos discos, muy <i>pimpeado</i>?
<ul>
<li>El problema viene del tiempo que toma mover la cabeza del disco a otro lugar del disco para leer o escribir datos (<i>seek time</i>).
<ul>
<li>¿Cuál es la <i>latencia</i> de la operación?</li>

</ul></li>

</ul></li>

<li>¿Por qué no <i>Grid</i>?
<ul>
<li>Por ejemplo, cosas  de <code>HPC</code> que usan <code>MPI</code>.
<ul>
<li>Son intensivos en <b><b>CPU</b></b>.</li>

</ul></li>
<li>Pero si hay que mover cientos de gigabytes, la transferencia de datos se vuelve un problema.
<ul>
<li>Basicamente, en que <code>Hadoop</code> opera con <i>data locality</i>.</li>

</ul></li>

</ul></li>

</ul>

</section>
<section id="slide-sec-2-6">
<h3 id="sec-2-6">Componentes de Apache Hadoop</h3>
<ul>
<li><b>MapReduce</b> Modelo de procesamiento <i>batch</i> de datos distribuido y paralelo.</li>
<li><b>HDFS</b> Sistema de archivos (<i>file system</i>) distribuido.</li>
<li><b>Pig</b> Capa de abstracción encima de <code>MapReduce</code>. Utiliza <i>Pig Latin</i> un lenguaje de flujo de datos
<ul>
<li>Como <code>dplyr</code></li>

</ul></li>
<li><b>Hive</b> (Hadoop InteractiVE) Es un lenguaje parecido al <code>SQL</code>: <code>HQL</code>, para ejecutar <i>queries</i> sobre el <code>HDFS</code>.</li>
<li><b>HBase</b> Base de datos distribuida orientada a columnas.
<ul>
<li>Depende de <code>Zookeeper</code>.</li>

</ul></li>
<li><b>Impala</b> Lenguaje Interactivo parecido al <code>SQL</code>, pero mucho más rápido de <code>HIVE</code> debido a su arquitectura <b>MPP</b>.</li>

</ul>

</section>
<section id="slide-sec-2-7">
<h3 id="sec-2-7">Componentes de Apache Hadoop</h3>
<ul>
<li><b>Zookeeper</b> Proyecto que proveé un servicio centralizado para facilitar la coordinación de componentes de Hadoop.</li>
<li><b>Sqoop</b> Herramienta para mover datos entre <code>RDBM</code> y <code>HDFS</code>.</li>
<li><b>Flume</b> Servicio para recolectar, agregar y mover grandes cantidades de datos entre máquinas individuales y el <code>HDFS</code>.</li>
<li><b>Oozie</b> Sistema de <i>workflow</i>, se usa para coordinar varios <i>jobs</i> de <b>MapReduce</b>.</li>
<li><b>Mahout</b> Biblioteca de <i>Machine Learning</i>.
<ul>
<li>Ver la carpeta <code>docs</code>.</li>

</ul></li>
<li><b>Ambari</b> Simplifica el aprovisionamiento, gestión y <i>monitoreo</i> de un <i>cluster</i> de Hadoop.</li>
<li><b>Avro</b> Formato de serialización y de persistencia de datos.</li>
<li>Entre otros&#x2026;</li>

</ul>



</section>
</section>
<section>
<section id="slide-sec-3" data-background="#000fff">
<h2 id="sec-3">HDFS : Hadoop File System</h2>


</section>
<section id="slide-sec-3-1">
<h3 id="sec-3-1">HDFS</h3>
<ul>
<li>Sistema de almacenamiento distribuido.
<ul>
<li><i>Namenode</i> <code>-&gt;</code> Master</li>
<li><i>Datanode</i> <code>-&gt;</code>  Slaves</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-3-2">
<h3 id="sec-3-2"><i>Schema on Read</i></h3>
<ul>
<li>Es posible cargar datos sin procesar dentro de Hadoop, la estructura se dará en el tiempo de procesamiento.</li>

<li>Es muy diferente a <i>Schema on Write</i> como el usado en los =RDBM=s
<ul>
<li><i>Schema on Write</i> impone un ciclo de análisis y modelado de datos, así como de su transformación, carga y prueba, antes de los datos puedan ser accesados.</li>
<li>Esto quita mucha flexibilidad: Si se tomaron decisiones incorrectas o los requerimientos cambian, es necesario empezar de nuevo <code>:(</code> .</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-3-3">
<h3 id="sec-3-3">Ventajas</h3>
<ul>
<li>Archivos muy grandes</li>
<li><i>write once, read many times</i>.</li>
<li>Hardware <i>normal</i></li>

</ul>

</section>
<section id="slide-sec-3-4">
<h3 id="sec-3-4">Desventajas</h3>
<ul>
<li>Acceso a los datos de baja latencia.</li>
<li>Muchos archivos pequeños.</li>
<li>Muchas escrituras, modificaciones</li>

</ul>

</section>
<section id="slide-sec-3-5">
<h3 id="sec-3-5">Tamaño del bloque</h3>
<ul>
<li>Cada <i>file system</i> define un tamaño de bloque, el cual es la cantidad mínima de datos que puede escribir o leer.
<ul>
<li>Típicamente son de <code>kb</code>.</li>

</ul></li>
<li>En <code>HDFS</code>, el bloque es de <code>128 Mb</code> por <i>default</i>.
<ul>
<li>Es el concepto fundamental, no el archivo.</li>

</ul></li>

</ul>


</section>
<section id="slide-sec-3-6">
<h3 id="sec-3-6"><i>Namenode</i></h3>
<ul>
<li>Gestiona el <i>filesystem</i>
<ul>
<li>Mantiene el árbol del <i>filesystem</i>.</li>
<li>Mantiene los <code>metadatos</code> de todos los archivos y carpetas del árbol.</li>
<li>Esta información se guarda en disco en dos archivos:
<ul>
<li><code>namespace image</code></li>
<li><code>edit log</code></li>

</ul></li>

</ul></li>
<li>Indica a los <i>datanodes</i> realizar tareas de bajo nivel de <code>I/O</code>.</li>
<li><i>Book Keeper</i>
<ul>
<li>División de archivos en bloques (¿Cómo?)</li>
<li>En qué <i>datanode</i> (¿Quién?)</li>
<li>Monitorea.</li>

</ul></li>
<li>Uso intensivo de <code>RAM</code> y de <code>I/O</code>.</li>
<li>Si se <i>cae</i> el <code>HDFS</code> no puede ser usado
<ul>
<li>Hasta la versión <code>1.x</code> el <i>single point of failure</i>, en Hadoop 2 se incorporó la característica de <i>HIgh Availability</i>.</li>
<li>Su caída puede causar la pérdida total de los datos.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-3-7">
<h3 id="sec-3-7"><i>Namenode</i></h3>
<ul>
<li>Hadoop proveé de dos formas de aliviar esta situación:
<ul>
<li>Respaldos: Se puede configurar al <i>namenode</i> para que escriba su estado a varios <i>filesystems</i>.</li>
<li><i>Secondary Namenode</i></li>

</ul></li>

</ul>

</section>
<section id="slide-sec-3-8">
<h3 id="sec-3-8"><i>Namenode</i></h3>

<div class="figure">
<p><img src="./imagenes/Selección_004.png" alt="Selección_004.png" />
</p>
</div>


</section>
<section id="slide-sec-3-9">
<h3 id="sec-3-9"><i>Datanode</i></h3>
<ul>
<li>Lee y escribe los <code>HDFS</code> <i>blocks</i> y los convierte en archivos del <b>FS</b> local.</li>
<li>Se comunica con otros <i>datanodes</i> para la replicación de los datos.</li>
<li>Pueden realizar <i>caching</i> de bloques.</li>

</ul>

</section>
<section id="slide-sec-3-10">
<h3 id="sec-3-10"><i>Datanode</i></h3>

<div class="figure">
<p><img src="./imagenes/Selección_005.png" alt="Selección_005.png" />
</p>
</div>

</section>
<section id="slide-sec-3-11">
<h3 id="sec-3-11"><i>Secondary Name Node</i></h3>
<ul>
<li>Como el <i>namenode</i> sólo hay uno por <i>cluster</i>.</li>
<li>No es un <i>namenode</i>.</li>
<li>Evita que el <code>edit log</code> crezca mucho.</li>
<li>No recibe ni guarda cambios en tiempo real del <code>HDFS</code>.
<ul>
<li>Va atrás del <i>namenode</i>.</li>

</ul></li>
<li>Sólo toma <i>snapshots</i> de la metadata.</li>

</ul>


</section>
<section id="slide-sec-3-12">
<h3 id="sec-3-12">Línea de comandos</h3>
<ul>
<li>Hay muchas maneras de conectarse y usar el <code>HDFS</code>. La línea de comandos es una de ellas.
<ul>
<li>Y espero que ya sepan que es de las más útiles y eficientes.</li>

</ul></li>

<li>Ayuda: <code>hadoop fs -help</code></li>

</ul>

</section>
<section id="slide-sec-3-13">
<h3 id="sec-3-13">Línea de comandos</h3>
<pre><code data-trim>

hadoop fs -cmd <args>
hadoop fs -ls
hadoop fs -mkdir
hadoop fs -copyFromLocal
hadoop fs -copyToLocal
hadoop fs -put archivo archivo_hdfs
hadoop fs -get archivo_hdfs
hadoop fs -cat archivo_hdfs
hadoop fs -cat archivo_hdfs head
hadoop fs -tail archivo_hdfs
hadoop fs -rm archivo_hdfs
</code></pre>



</section>
</section>
<section>
<section id="slide-sec-4" data-background="#000fff">
<h2 id="sec-4">Arquitectura: Ingesta de datos</h2>

</section>
<section id="slide-sec-4-1">
<h3 id="sec-4-1">Decisiones Arquitectónicas</h3>
<ul>
<li>El hecho de que el <code>HDFS</code> permita <i>Schema on Read</i>, no elimina la necesidad de tomar decisiones arquitectónicas en la ingesta de los datos, entre ellas:

<ul>
<li>¿Cómo se guardarán los datos?
<ul>
<li>Capa de almacenamiento</li>
<li>Formatos de archivos</li>
<li>Formatos de compresión</li>

</ul></li>

<li>¿Diseño de esquema de datos?
<ul>
<li>Directorios donde guardar los datos y donde ponerlos luego del procesamiento y analítica.</li>
<li>También en <code>HBase</code> y en <code>Hive</code> se definen esquemas.</li>

</ul></li>
<li>¿Cómo se gestionarán los metadatos?</li>
<li>¿Cómo se administrará la seguridad?
<ul>
<li>Autenticación, cifrado, acceso controlado.</li>

</ul></li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-2">
<h3 id="sec-4-2">Capa de almacenamiento: <code>HDFS</code> vs <code>HBase</code></h3>
<ul>
<li><code>HDFS</code>
<ul>
<li>Almacena los datos como archivos</li>
<li><i>Scans</i> rápidos.</li>
<li>Malo para acceso aleatorio en escritura y lectura.</li>

</ul></li>

<li><code>HBase</code>
<ul>
<li>Guarda los datos como archivo</li>

</ul></li>

</ul>
<p>
s de HBase en el <code>HDFS</code>.
</p>
<ul>
<li><i>Scans</i> lentos.</li>
<li>Rápido acceso aleatorio a lectura y escritura.</li>

<li>En esta clase nos enfocaremos a <code>HDFS</code>.</li>

</ul>

</section>
<section id="slide-sec-4-3">
<h3 id="sec-4-3">Formatos de archivos</h3>
<ul>
<li>Tipos de archivos de Hadoop
<ul>
<li>Basados en archivos: <code>SequenceFiles</code>.</li>
<li>Formatos serializados: <code>Avro</code>, <code>Thrift</code>.</li>
<li>Formatos columnares: <code>RCFile</code>, <code>ORCFile</code>, <code>Parquet</code>.</li>

</ul></li>

<li>Debido a que la mayoría de formatos de archivos sólo se puede acceder desde <code>Java</code>, nos enfocaremos en sólo dos: <code>Avro</code> y <code>Parquet</code></li>

</ul>

</section>
<section id="slide-sec-4-4">
<h3 id="sec-4-4">Formatos de archivos</h3>
<ul>
<li><code>Avro</code>
<ul>
<li>Independiente del lenguaje.</li>
<li>Almacena el esquema en el encabezado de cada archivo.</li>
<li>Son comprensibles y divisibles.
<ul>
<li>Soporta compresión con <code>snappy</code>.</li>

</ul></li>
<li>Es recomendable usarlo en la ingesta de datos.</li>
<li>Las fallas sólo afectan a una porción del archivo.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-5">
<h3 id="sec-4-5">Formatos de archivos</h3>
<ul>
<li><code>Parquet</code>
<ul>
<li>Diseñado para proveer procesamiento eficiente a través de varios compoentes de hadoop.</li>
<li>Almacena los datos de manera columnar.</li>
<li>Provee excelentes capacidades de compresión.</li>
<li>Soporta estructuras de datos complejas y anidadas.</li>
<li>Los metadatos están guardados al final del archivo.</li>
<li>Puede escribirse y leerse con las APIs de Avro y con esquemas de Avro.</li>
<li>No son tan buenos para recuperarse de errores.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-6">
<h3 id="sec-4-6">Formatos de compresión</h3>
<ul>
<li>Ayuda a reducir los requerimientos de almacenamiento</li>
<li>Mejora el procesamiento de los datos
<ul>
<li>Disminuye ,a cantidad de I/O en disco y red.</li>

</ul></li>
<li>Para aprovechar las capacidades de procesamiento en paralelo de Hadoop es preferible que el formato sea divisible.</li>

</ul>


</section>
<section id="slide-sec-4-7">
<h3 id="sec-4-7">Formatos de compresión</h3>
<ul>
<li><code>bzip2</code>
<ul>
<li>Excelente factor  de compresión</li>
<li>Pero muuuuuy lento en compresión/decompresión</li>
<li>Divisible</li>

</ul></li>

<li><code>snappy</code>
<ul>
<li>Proyecto de Google.</li>
<li>No es divisible, pero muy eficiente en compresión/decompresión.</li>
<li>Se debe de usar con un formato de archivo que provea la capacidad de contenedor (<code>Avro</code>, <code>SequenceFiles</code>).</li>

</ul></li>

<li><code>gzip</code>
<ul>
<li>No es divisible</li>
<li>Buen factor de comrpesión: 2.5x lo de <code>snappy</code>.</li>
<li>Se debe de usar con un formato de archivo que provea la capacidad de contenedor (<code>Avro</code>, <code>SequenceFiles</code>).</li>

</ul></li>

<li><code>lzop</code>
<ul>
<li>Parecido a <code>snappy</code> en eficiencia de compresión/decompresión.</li>
<li>Divisible, pero requiere una etapa de indexado.</li>
<li>Buena elección para guardar archivos de texto planos que no se pondrán dentro de un contenedor.</li>
<li>Licenciamiento raro (No viene incluido con Hadoop).</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-8">
<h3 id="sec-4-8">Esquema</h3>
<ul>
<li><b>Nota</b>: <i>Basado en <a href="http://shop.oreilly.com/product/0636920033196.do">Hadoop Application Architectures.</a></i></li>

<li>¿Por qué?
<ul>
<li>Estructura de archivos estándar facilita la colaboración entre equipos.</li>
<li>Permite la reutilización de código para procesarla.</li>
<li>Permite reforzar las políticas de acceso y evitar así corrupción de los datos.</li>
<li>Permite identificar que datos han sido procesados completamente y cuales no</li>
<li>Muy parecido a los <code>schemas</code> de PostgreSQL.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-9">
<h3 id="sec-4-9">Esquema Propuesto</h3>
<ul>
<li><code>/user/&lt;username&gt;</code>
<ul>
<li>Datos para experimentar (i.e. no son parte del proceso de negocio).</li>
<li><code>JARs</code>, archivos de configuración.</li>
<li>Sólo debe de tener permisos de R/W el usuario en cuestión.</li>

</ul></li>

<li><code>/etl</code>
<ul>
<li>Datos en sus varias etapas de transformación por el ETL.</li>
<li>Subdirectorios reflejan el <i>workflow</i> de los datos.
<ul>
<li>Los ETL son creados por <b>grupos</b> para <b>aplicaciones</b>.</li>
<li>Además cada subdirectorio tendrá a su vez directorios para cada etapa del proceso:
<ul>
<li><code>input</code> para el lugar donde llegan los archivos</li>
<li><code>procesando</code> para los pasos intermedios (puede haber varios)</li>
<li><code>output</code> para el resultado final</li>
<li><code>rechazados</code> para los registros o archivos que no pudieron ser procesados y que deben de verificarse manualmente.</li>

</ul></li>

</ul></li>
<li>La estructura quedaría así:
<ul>
<li><code>/etl/&lt;grupo&gt;/&lt;aplicación&gt;/&lt;proceso&gt;/{input, procesando, output, rechazados}</code></li>

</ul></li>
<li>Sólo el usuario <code>etl</code> y los usuarios del grupo <code>etl</code> pueden R/W.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-10">
<h3 id="sec-4-10">Esquema Propuesto</h3>
<ul>
<li><code>/tmp</code>
<ul>
<li>Datos temporales generados por usuarios o partes de Hadoop.</li>
<li>Se borra su interior regularmente.</li>
<li>Todos tienen permisos de RW en este directorio.</li>

</ul></li>

<li><code>/data</code>
<ul>
<li>Datos procesados y usados por la organización</li>
<li>Existen controles sobre quién puede o no usar los datos</li>
<li>Los usuarios sólo tienen permisos de lectura.</li>
<li>Los procesos automatizados (y auditados) tienen permisos de escritura.</li>

</ul></li>

<li><code>/app</code>
<ul>
<li>Todo lo requerido por la aplicación de Hadoop para funcionar (salvo datos)</li>
<li>Archivos de Oozie (definiciones de <i>workflows</i>),</li>
<li>Archivos de <code>hql</code>, <code>pig</code>, <code>JARs</code>, <code>UDFs</code>, etc.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-11">
<h3 id="sec-4-11">Otras consideraciones</h3>
<ul>
<li><b>Particionado</b>
<ul>
<li>Ayuda a reducir la cantidad de I/O para procesar los datos.</li>
<li>Es una especie de <i>indexado</i> básico.</li>

</ul></li>

</ul>
<pre class="example">
&lt;nombre del dataset&gt;/&lt;columna sobre la cual particionar&gt;=&lt;valor de la columna&gt;/{archivos}
</pre>

<ul>
<li><b>Denormalizar</b>
<ul>
<li>Ahorras <code>Joins</code> (que son lentos)</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-12">
<h3 id="sec-4-12">Ejercicio I</h3>
<ul>
<li>En este ejercicio prepararemos el <b>esquema</b> de nuestra aplicación de gran escala.</li>
<li>Inicializa el contenedor <code>hadoop-pseudo</code>.</li>
<li>Cambia al usuario <code>itam</code>.</li>
<li>Revisa la estructura de directorios con el usuario <code>hdfs</code>.
<ul>
<li>Esto lo puedes hacer con <code>sudo -u hdfs ...</code></li>

</ul></li>
<li>Crea el esquema de directorios propuesta.
<ul>
<li>Esto lo puedes hacer con <code>sudo -u hdfs ...</code></li>
<li><code>/user/&lt;username&gt;</code>, <code>/etl</code> (para la aplicación <code>ufo</code> y <code>gdelt</code>, el grupo es <code>ds</code>), <code>/tmp</code>, <code>/app</code> y <code>/data</code>.</li>
<li>Las últimas tres están vacías.</li>

</ul></li>
<li>Asigna los permisos adecuados.
<ul>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">HDFS Permissions Guide</a>, <a href="http://stackoverflow.com/questions/23095244/add-new-group-to-hdfs">Pregunta de Stackoverflow</a>, <a href="http://spryinc.com/blog/hdfs-permissions-overcoming-permission-denied-accesscontrolexception">Problema del supergroup</a></li>

</ul></li>
<li>Dentro de tu carpeta (siendo el usuario <code>itam</code>), crea la carpeta <code>datasets</code> y adentro <code>ufos</code> y la carpeta <code>gdelt</code>.</li>

</ul>

</section>
<section id="slide-sec-4-13">
<h3 id="sec-4-13">Ejercicio I</h3>
<ul>
<li>Dentro de tu carpeta (siendo el usuario <code>itam</code>), crea la carpeta <code>experimentos</code>.</li>
<li>Carga dos archivos de cada dataset a esta carpeta desde <code>/home/itam/data/</code> usando la línea de comandos.
<ul>
<li>Observa que una de las carpetas es local&#x2026;</li>

</ul></li>

</ul>


<ul>
<li>Verifiquemos que los datos estén bien:</li>

</ul>

<pre><code data-trim>
hadoop fs -cat experimentos/UFO-Dic-2014.tsv | wc -l
hadoop fs -cat experimentos/UFO-Dic-2014.tsv | head
</code></pre>

<ul>
<li>Observa como los datos están en formato de texto, justo como la copia que está en tu disco duro.</li>

</ul>


</section>
<section id="slide-sec-4-14">
<h3 id="sec-4-14">Ejercicio II</h3>
<ul>
<li>En este ejercicio usaremos <code>kite</code>.</li>
<li><a href="http://kitesdk.org/"><code>Kite</code></a> es una herramienta que nos permite cargar y administrar los metadatos de los archivos a Hadoop.
<ul>
<li>Pueden obtener ayuda con <code>kite-dataset help comando</code>.</li>

</ul></li>
<li>Tanto <code>Avro</code>, como <code>Hive Metastore</code> pueden servir para gestionar los metadatos y <code>kite</code> puede trabajar con ambos.</li>
<li>En este ejercicio, nos enfocaremos en el dataset de <code>ufos</code>.</li>
<li>Y a partir de aquí, todos los ejercicios son con el usuario <code>itam</code>.</li>

</ul>

</section>
<section id="slide-sec-4-15">
<h3 id="sec-4-15">Ejercicio II</h3>
<ul>
<li><code>HDFS</code> y <code>Avro</code> para guardar los metadatos.</li>
<li>Infiere el esquema a partir de uno de los archivos:</li>

</ul>

<pre><code data-trim>
kite-dataset csv-schema data/UFO-Nov-2014.tsv --class UFO -o ufos.avsc  --delimiter "\t"
</code></pre>

<ul>
<li>Esto va a marcar un error, arréglalo con <code>sed</code>.
<ul>
<li>Cuando hay <code>/</code> de por medio puedes cambiar el separador de <code>sed</code> por cualquier caracter, ejemplo:</li>

</ul></li>

</ul>

<pre><code data-trim>
sed -e -i 's@cambiar_algo@por_esto@g' archivo
</code></pre>


<ul>
<li>Abre el archivo <code>ufos.avsc</code>, es el esquema en formato <code>avro</code>.</li>
<li>Ahora crearemos el <code>dataset</code> en el <code>hdfs</code>.</li>

</ul>

<pre><code data-trim>
kite-dataset create dataset:hdfs:/user/itam/datasets/ufos --schema ufos.avsc
</code></pre>

<ul>
<li>Observa los cambios ocurridos en la carpeta <code>ufos</code> del <code>hdfs</code>.
<ul>
<li>Recuerda que puedes ver el contenido con el comando <code>hadoop fs -cat</code></li>

</ul></li>

<li>Para verificar que se realizó bien puedes ejecutar:</li>

</ul>

<pre><code data-trim>
kite-dataset schema dataset:hdfs:/user/itam/datasets/ufos
</code></pre>

</section>
<section id="slide-sec-4-16">
<h3 id="sec-4-16">Ejercicio II</h3>
<ul>
<li>Por último, importemos los datos</li>

</ul>

<pre><code data-trim>
kite-dataset csv-import data/UFO-Nov-2014.tsv dataset:hdfs:/user/itam/datasets/ufos --delimiter "\t"
kite-dataset csv-import data/UFO-Dic-2014.tsv dataset:hdfs:/user/itam/datasets/ufos --delimiter "\t"
</code></pre>


<ul>
<li>Veamos que si se copiaron:</li>

</ul>
<pre><code data-trim>
kite-dataset show dataset:hdfs:/user/itam/datasets/ufos
</code></pre>

<ul>
<li>Ahora observa como se ve un conjunto de datos en fornato <code>avro</code>, usando las herramientas de línea de comandos.
<ul>
<li>No lo abras con <code>hadoop fs -cat ...</code> o la consola se dañará&#x2026;</li>

</ul></li>

</ul>


<ul>
<li><b>NOTA</b>: Si algo salió mal, puedes borrar el dataset con</li>

</ul>
<pre><code data-trim>
kite-dataset delete dataset:hdfs:/user/itam/datasets/ufos
</code></pre>

</section>
<section id="slide-sec-4-17">
<h3 id="sec-4-17">Ejercicio II</h3>
<ul>
<li>Ahora guardaremos los datos en  <code>hive metastore</code>.
<ul>
<li>No te preocupes más adelante explicaré que es esto, por el momento piensa en una base de datos para los metadatos.</li>

</ul></li>

<li>Los pasos son casi los mismos que el ejercicio anterior, sólo cambia el destino: ya no es el <code>HDFS</code>, ahora es <code>hive metastore</code>.</li>

<li>Crea el <code>dataset</code></li>

</ul>

<pre><code data-trim>
kite-dataset create ufos --schema ufos.avsc
</code></pre>

<ul>
<li>Para verificar que se realizó bien puedes ejecutar:</li>

</ul>

<pre><code data-trim>
kite-dataset schema ufos
</code></pre>

<ul>
<li>Y para asegurarnos que no son los mismos datos que antes (los guardados en el <code>hdfs</code>), ejecuta</li>

</ul>

<pre><code data-trim>
kite-dataset show ufos
</code></pre>

</section>
<section id="slide-sec-4-18">
<h3 id="sec-4-18">Ejercicio II</h3>
<ul>
<li>Importemos los datos</li>

</ul>

<pre><code data-trim>
kite-dataset csv-import data/UFO-Nov-2014.tsv ufos --delimiter "\t"
kite-dataset csv-import data/UFO-Dic-2014.tsv ufos --delimiter "\t"
</code></pre>


<ul>
<li>Veamos que si se copiaron:</li>

</ul>
<pre><code data-trim>
kite-dataset show ufos
</code></pre>

<ul>
<li><b>NOTA</b>: Si algo salió mal, puedes borrar el dataset con</li>

</ul>
<pre><code data-trim>
kite-dataset delete ufos
</code></pre>


</section>
<section id="slide-sec-4-19">
<h3 id="sec-4-19">Ejercicio III</h3>
<ul>
<li>En este momento, tienes 3 veces los datos en tres formatos diferentes: A
<ol>
<li>Archivo de texto</li>
<li>Archivo <code>avro</code></li>
<li>Guardado como tabla en <code>hive</code> y sus metadatos en el <code>hive metastore</code>.</li>

</ol></li>

<li>Más adelante veremos en detalle las <i>abstracciones</i> y <i>procesadores</i> que tiene <code>Hadoop</code> para manipular y analizar los datos, pero por el momento los usaremos para ver los datos, sin dar mucha explicación.
<ul>
<li>En lo que sigue, observa el código, todo tendrá más sentido cuando expliquemos apropiadamente estas herramientas.</li>

</ul></li>

<li>En este ejercicio, veremos <code>spark</code>, <code>pig</code>, <code>hive</code> e <code>impala</code>.</li>

</ul>

</section>
<section id="slide-sec-4-20">
<h3 id="sec-4-20">Ejercicio III</h3>
<ul>
<li>Empecemos con el archivo de texto (localizados en <code>hdfs://localhost/user/itam/experimentos/</code>)</li>

<li>Usaremos la consola de <code>python</code> de <code>spark</code></li>

</ul>

<pre><code data-trim>
pyspark
</code></pre>

<ul>
<li>La respuesta, luego de varias líneas de texto debe de ser:</li>

</ul>

<pre><code data-trim>
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.3.0
      /_/

Using Python version 2.7.8 (default, Oct 20 2014 15:05:19)
SparkContext available as sc, HiveContext available as sqlCtx.
>>>
</code></pre>

<ul>
<li>Observa que hay dos contextos al final: <code>SparkContext</code> y <code>HiveContext</code>, estos contextos permiten interactuar con el cluster de Hadoop.</li>

</ul>

</section>
<section id="slide-sec-4-21">
<h3 id="sec-4-21">Ejercicio III</h3>
<ul>
<li>Carguemos como <b>RDD</b> el archivo (en este caso, estamos cargando líneas de texto, no como archivo <code>tsv</code>)</li>

</ul>

<pre><code data-trim>
ufos_nov = sc.textFile("hdfs://localhost/user/itam/experimentos/UFO-Nov-2014.tsv")
</code></pre>

<ul>
<li>Contemos las líneas que hemos cargado</li>

</ul>

<pre><code data-trim>
ufos_nov.count()
</code></pre>

<ul>
<li>Veamos los primeros cinco renglones</li>

</ul>

<pre><code data-trim>
ufos_nov.take(5)
</code></pre>

<ul>
<li>O sólo el primero</li>

</ul>


<pre><code data-trim>
ufos_nov.first()
</code></pre>


<ul>
<li>Si queremos contar el número de estados</li>

</ul>

<pre><code data-trim>
ufos_nov.map(lambda line: (line.split('\t')[2]))\
.distinct()\
.count()
</code></pre>


</section>
<section id="slide-sec-4-22">
<h3 id="sec-4-22">Ejercicio III</h3>
<ul>
<li>¿Qué pasa si queremos cargar el archivo e identificar las columnas?
<ul>
<li>¡Definimos una función en <code>python</code>!</li>

</ul></li>

</ul>

<pre><code data-trim>
import csv
from io import StringIO


def load_tsv(archivo):
    return csv.reader(StringIO(archivo[1]), delimiter='\t')
</code></pre>

<ul>
<li>Y leemos el archivo</li>

</ul>
<pre><code data-trim>
ufos_nov = sc.textFile("hdfs://localhost/user/itam/experimentos/UFO-Nov-2014.tsv").flatMap(load_tsv)
ufos_nov.take(3)[2]
</code></pre>

<ul>
<li>Más adelante veremos como explotar esta estructura.</li>

<li>Para salir presiona <code>Ctrl+C</code> ó <code>Ctrl+D</code>.</li>

</ul>

</section>
<section id="slide-sec-4-23">
<h3 id="sec-4-23">Ejercicio III</h3>
<ul>
<li><code>Pig</code> es una abstracción sobre MapReduce</li>

<li><code>Pig</code> tiene un archivo de configuración localizado en <code>~/.pigbootup</code></li>

<li>Más adelante requeriremos algunos <code>JARs</code> para ejecutar cosas en <code>Pig</code>, en lugar de usarlos desde el sistema de archivos local, los leeremos desde el <code>hdfs</code>.
<ul>
<li>Crea una carpeta llamada <code>lib</code> en <code>/user/itam</code></li>
<li>Copia a esta carpeta los siguientes archivos:
<ul>
<li><code>/usr/lib/pig/datafu-1.1.0-cdh5.4.0.jar</code></li>
<li><code>/usr/lib/pig/piggybank.jar</code></li>
<li><code>/usr/lib/pig/lib/avro-1.7.6-cdh5.4.0.jar</code></li>
<li><code>/usr/lib/pig/lib/snappy-java-1.0.5.jar</code></li>
<li><code>/usr/lib/pig/lib/json-simple-1.1.jar</code></li>

</ul></li>

</ul></li>

<li>Crea el archivo <code>.pigbootup</code> en tu carpeta <code>$HOME</code> (i.e. <code>/home/itam</code>)</li>

<li>Agrega lo siguiente:</li>

</ul>
<pre><code data-trim>
REGISTER hdfs://localhost/user/itam/lib/datafu-1.1.0-cdh5.4.0.jar
REGISTER hdfs://localhost/user/itam/lib/piggybank.jar
REGISTER hdfs://localhost/user/itam/lib/avro-1.7.6-cdh5.4.0.jar
REGISTER hdfs://localhost/user/itam/lib/snappy-java-1.0.5.jar
REGISTER hdfs://localhost/user/itam/lib/json-simple-1.1.jar
</code></pre>



<ul>
<li>Para ejecutarlo</li>

</ul>

<pre><code data-trim>
pig -useHCatalog
</code></pre>


</section>
<section id="slide-sec-4-24">
<h3 id="sec-4-24">Ejercicio III</h3>
<ul>
<li>Para replicar lo que hicimos con <code>Spark</code>:</li>

</ul>

<pre><code data-trim>
ufos_dic = LOAD 'experimentos/UFO-Dic-2014.tsv' using PigStorage('\t')  \
           AS (Timestamp:chararray, \
               City:chararray, State:chararray, \
               Shape:chararray, Duration:chararray, \
               Summary:chararray, Posted:chararray);
DESCRIBE ufos_dic;
head = LIMIT ufos_dic 5;
DUMP head;
</code></pre>

<ul>
<li>Puedes seguir la ejecución vía web  <a href="http://0.0.0.0:8088">aquí</a>.</li>

<li>Nota el uso de mayúsculas para las palabras clave de <code>Pig</code>.</li>

</ul>

</section>
<section id="slide-sec-4-25">
<h3 id="sec-4-25">Ejercicio III</h3>
<ul>
<li>Ahora usemos los archivos con formato <code>avro</code> y observemos como, dado que tienen metadatos, es mucho más fácil.
<ul>
<li>Nota lo limpio que va a quedar el código ahora&#x2026;</li>

</ul></li>

</ul>

<pre><code data-trim>
ufos = LOAD 'datasets/ufos' USING org.apache.pig.piggybank.storage.avro.AvroStorage();
DESCRIBE ufos;
ILLUSTRATE ufos;
head = LIMIT ufos 5;
DUMP head;
</code></pre>

<ul>
<li>Observa como no hubo problemas con el header del archivo!
<ul>
<li>¡En el ejercicio anterior (tanto con <code>pig</code> como con <code>spark</code>) era la primera línea!</li>

</ul></li>

<li>Para ver los diferentes estados</li>

</ul>
<pre><code data-trim>
states = DISTINCT (FOREACH ufos GENERATE State);
DUMP states;
</code></pre>

<ul>
<li>Para salir presiona <code>Ctrl+C</code> ó <code>Ctrl+D</code>.</li>

</ul>

</section>
<section id="slide-sec-4-26">
<h3 id="sec-4-26">Ejercicio III</h3>
<ul>
<li>Por último usaremos las herramientas parecidas a <code>SQL</code> que proveé Hadoop: <code>Hive</code> e <code>Impala</code>.</li>

<li>Usaremos el <code>Hive Metastore</code>.
<ul>
<li>Aunque podríamos usar el <code>hdfs</code> o <code>avro</code> en el <code>hdfs</code>.</li>

</ul></li>

<li>Para ejecutar el cliente de <code>Hive</code></li>

</ul>

<pre><code data-trim>
beeline -u jdbc:hive2://localhost:10000
</code></pre>

<ul>
<li>Veámos que tablas hay disponibles</li>

</ul>

<pre><code data-trim>
show tables;
</code></pre>

<ul>
<li>Obtengamos los primeros 5</li>

</ul>

<pre><code data-trim>
select * from ufos limit 5;
</code></pre>

<ul>
<li>Contar los estados diferentes:</li>

</ul>

<pre><code data-trim>
select count(distinct State) from ufos;
</code></pre>

<ul>
<li>Ver el plan de ejecución del <i>query</i></li>

</ul>

<pre><code data-trim>
explain select count(distinct State) from ufos;
</code></pre>


<ul>
<li>Compara con este <i>query</i>
<ul>
<li>¿Cuál es la diferencia?</li>

</ul></li>

</ul>

<pre><code data-trim>
explain select count(*) from (select distinct State from ufos) as t;
</code></pre>

<ul>
<li>Para salir presiona <code>Ctrl+C</code> ó <code>Ctrl+D</code>.</li>

</ul>

</section>
<section id="slide-sec-4-27">
<h3 id="sec-4-27">Ejercicio III</h3>
<ul>
<li>Para iniciar <code>Impala</code></li>

</ul>

<pre><code data-trim>
impala-shell
</code></pre>

<ul>
<li>Debido a que Impala <b>no</b> es una abstracción de <b>MapReduce</b>, sus tiempos son impresionantemente rápidos</li>

</ul>

<pre><code data-trim>
invalidate metadata; # Siempre ejecutarlo cuando se modifiquen las tablas fuera de Impala
show tables;
describe ufos;
select * from ufos limit 5; # Este quizá tarde un poco... (warming up)
select * from ufos limit 15; # Debería de volar
</code></pre>

<ul>
<li>Top 5 de avistamientos por estado</li>

</ul>

<pre><code data-trim>
select state, count(*) as conteo from ufos group by state order by conteo desc limit 5;
</code></pre>

<ul>
<li>Para salir presiona <code>Ctrl+D</code>.</li>

</ul>

</section>
<section id="slide-sec-4-28">
<h3 id="sec-4-28">Ejercicio III: Recapitulando</h3>
<ul>
<li>Vimos diferentes maneras de interactuar con los datos
<ul>
<li>Lo vamos a profundizar luego.</li>

</ul></li>

<li>Es importante notar que aunque usamos diferentes herramientas para cada tipo de archivo (Texto, Avro, Tabla),  <i>todas</i> las herramientas pueden ver <i>todos</i> los formatos.
<ul>
<li>Casi&#x2026;por lo menos los mostrados aquí.</li>
<li>Por ejemplo, podemos usar <code>pig</code> para leer las tablas de <code>hive</code>, cambiando el <code>LOAD</code> como sigue:</li>

</ul></li>

</ul>
<pre><code data-trim>
ufos = load 'ufos' using org.apache.hive.hcatalog.pig.HCatLoader();
describe ufos;
illustrate ufos;
...
</code></pre>

<ul>
<li>Es importante notar también, que cada herramienta es para un diferente proceso (ingeniería, analítica, etc.)</li>

<li>Estamos explorando los datos, aún no establecemos un <i>workflow</i>
<ul>
<li>También lo veremos más adelante.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-29">
<h3 id="sec-4-29">Ejercicio IV</h3>
<ul>
<li>Repite todo lo anterior para cargar dos archivos de <code>gdelt</code> de tu elección.</li>

</ul>


</section>
</section>
<section>
<section id="slide-sec-5" data-background="#000fff">
<h2 id="sec-5">YARN</h2>

</section>
<section id="slide-sec-5-1">
<h3 id="sec-5-1">YARN</h3>
<ul>
<li>La infraestructura de Hadoop <code>0.x</code> y <code>1.x</code> era monolítica, por eso fue rediseñada.</li>
<li><code>YARN</code>: <i>Yet Another Resource Negotiator</i>.</li>
<li>La gestión de recursos es extraída de los paquetes de <code>MapReduce</code> para que puedan ser utilizadas por otros componentes.</li>
<li>Aportaciones
<ul>
<li>Escalabilidad.</li>
<li>Compatibilidad con <code>MapReduce</code>.</li>
<li>Mejoras en la gestión del <i>cluster</i>.</li>
<li>Soporte para otros modelos de programación (además de <code>MapReduce</code>).
<ul>
<li><i>Graph processing</i></li>
<li><i>Message Passing Interface</i> (<b>MPI</b>).</li>
<li>Soporte para procesamiento <i>real-time</i> o <i>near real-time</i>.
<ul>
<li><code>MapReduce</code> es <i>batch-oriented</i>.</li>

</ul></li>

</ul></li>
<li>Agilidad.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-5-2">
<h3 id="sec-5-2">YARN</h3>
<ul>
<li>Se dividieron las dos responsabilidades del <i>JobTracker</i>:
<ul>
<li>Gestión de recursos (<i>Resource Management</i>)</li>
<li>Asignación y vigilancia de trabajos (<i>Job scheduling-monitoring</i>)</li>

</ul></li>

<li>La idea es tener un <i>ResourceManager</i> global y un <i>NodeManager</i> por
nodo esclavo, los cuales forman un sistema para la administración de
aplicaciones distribuidas.</li>

<li>El <i>ResourceManager</i> tiene dos componentes principales:
<ul>
<li><i>Scheduler</i>: Asigna los recursos para las aplicaciones (<i>pluggeable</i>).</li>
<li><i>Application Manager</i>: Responsable de aceptar las solicitudes de
trabajos, negociando al principio para ejecutar el <i>Application
Master</i> específico y provee un servicio de reinicio, por si el
<i>Application Master</i> falla.</li>

</ul></li>

<li>En cada nodo:

<ul>
<li>El <i>Application Master</i>: Negocia sus recursos con el <i>Scheduler</i>,</li>

</ul>
<p>
monitorea sus avances y reporta su estatus.
</p>

<ul>
<li>El <i>NodeManager</i> es el responsable de los contenedores,
monitorear el uso de recursos y reportar todo al
<i>ResourceManager</i>.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-5-3">
<h3 id="sec-5-3">Arquitectura MapReduce Hadoop 1.x</h3>

<div class="figure">
<p><img src="./imagenes/MRArch.png" alt="MRArch.png" />
</p>
</div>

</section>
<section id="slide-sec-5-4">
<h3 id="sec-5-4">Arquitectura Hadoop 2.x</h3>

<div class="figure">
<p><img src="./imagenes/Selección_003.png" alt="Selección_003.png" />
</p>
</div>


</section>
<section id="slide-sec-5-5">
<h3 id="sec-5-5">Cambios 1.x -&gt; 2.x</h3>

<div class="figure">
<p><img src="./imagenes/yarn.png" alt="yarn.png" />
</p>
</div>


</section>
<section id="slide-sec-5-6">
<h3 id="sec-5-6">Multiparadigma en Hadoop 2.x</h3>

<div class="figure">
<p><img src="imagenes/Hadoop-2.0-Intro-Blog2.jpg" alt="Hadoop-2.0-Intro-Blog2.jpg" />
</p>
</div>



<p>
Imagen tomada de <a href="http://www.edureka.co/blog/apache-hadoop-2-0-and-yarn/">edureka!</a>
</p>

</section>
<section id="slide-sec-5-7">
<h3 id="sec-5-7">Procesadores y Abstracciones</h3>

<div class="figure">
<p><img src="imagenes/HDFS.jpg" alt="HDFS.jpg" />
</p>
</div>


<p>
Imagen tomada de <a href="http://radar.oreilly.com/2015/02/processing-frameworks-for-hadoop.html">O'reilly</a>
</p>


</section>
</section>
<section>
<section id="slide-sec-6" data-background="#000fff">
<h2 id="sec-6">Procesamiento</h2>

</section>
<section id="slide-sec-6-1">
<h3 id="sec-6-1">Tipos</h3>
<ul>
<li>MapReduce</li>
<li>Spark</li>
<li>Impala</li>

</ul>


</section>
</section>
<section>
<section id="slide-sec-7" data-background="#000fff">
<h2 id="sec-7">Procesamiento: MapReduce</h2>
</section>
<section id="slide-sec-7-0-1">
<h4 id="sec-7-0-1">MapReduce en Hadoop</h4>
<ul>
<li>Principal <i>framework</i> de ejecución de <code>Apache Hadoop</code>.</li>
<li>Inspirado en las operaciones <b>MAP</b> y <b>REDUCE</b> de los lenguajes funcionales.</li>
<li>Modelo de programación para proceso de datos distribuido  y paralelo.</li>
<li>Divide las tareas (<i>jobs</i>) en fases de <i>mapeo</i> y fases de <i>reducción</i>.</li>
<li>Los desarrolladores crean tareas <i>MapReduce</i> para Hadoop usando datos guardados en el <code>HDFS</code>.</li>

</ul>

</section>
<section id="slide-sec-7-0-2">
<h4 id="sec-7-0-2">MapReduce: Ventajas</h4>
<ul>
<li><i>Fault-tolerant</i>.</li>
<li>Esconde los detalles de implementación a los programadores.</li>
<li>Escala con el tamaño de los datos.</li>

</ul>


</section>
<section id="slide-sec-7-0-3">
<h4 id="sec-7-0-3">MapReduce</h4>
<ul>
<li>Dos fases de procesamiento:
<ul>
<li><i>key-value</i> como Input y Output</li>
<li>El programador especifica:
<ul>
<li>Tipos de <i>key-value</i></li>
<li>Funciones: <code>MAP</code> y <code>REDUCE</code>.</li>

</ul></li>

</ul></li>

</ul>


</section>
<section id="slide-sec-7-0-4">
<h4 id="sec-7-0-4">Una pequeña regresión&#x2026;</h4>

</section>
<section id="slide-sec-7-0-5">
<h4 id="sec-7-0-5">map-reduce: Matemáticamente</h4>
<PRE><CODE DATA-TRIM>
map: (k1, v1) -> list(k2, v2)
</CODE></PRE>

<ul>
<li><code>map</code> Mapea (aplica una función <i>f</i>) un conjunto de entrada de pares <i>key-value</i> a otro conjunto intermedio de <i>key-values</i></li>

</ul>


</section>
<section id="slide-sec-7-0-6">
<h4 id="sec-7-0-6">map-reduce: Matemáticamente</h4>
<PRE><CODE DATA-TRIM>
reduce: (k2, list(v2)) -> list(k3, v3)
</CODE></PRE>

<ul>
<li><code>reduce</code>  Aplica una función <i>g</i> a todos los valores (<i>values</i>) asociados a una llave (<i>key</i>) y acumula el resultado. Emite pares de <i>key-values</i>.</li>

</ul>

</section>
<section id="slide-sec-7-0-7">
<h4 id="sec-7-0-7">Python <code>map</code></h4>
<div class="org-src-container">

<pre  class="src src-python"># Equivalente en for-loop

items = [1,2,3,4,5]
cuadrados = []
for x in items:
    cuadrados.append(x**2)

print cuadrados
</pre>
</div>


<div class="org-src-container">

<pre  class="src src-python"># Usando la funcion map(function, sequence)

items = [1,2,3,4,5]

print list(map((lambda x: x**2), items))
</pre>
</div>


</section>
<section id="slide-sec-7-0-8">
<h4 id="sec-7-0-8">Python <code>reduce</code></h4>
<div class="org-src-container">

<pre  class="src src-python"># Equivalente en for-loop
L = [1,2,3,4]
result = L[0]
for x in L[1:]:
    result = result*x

print result
</pre>
</div>

<div class="org-src-container">

<pre  class="src src-python"># Usando la funcion reduce(funcion, secuencia)
print reduce((lambda x,y: x*y), [1,2,3,4])
</pre>
</div>

</section>
<section id="slide-sec-7-0-9">
<h4 id="sec-7-0-9">Python <code>map</code> y <code>reduce</code></h4>
<div class="org-src-container">

<pre  class="src src-python">a = range(1, 4)
b = range(4, 9)
c = range(9, 15)
print "a -&gt;  %s, b -&gt; %s , c -&gt; %s" % (a, b, c)

L1 = map(lambda x:len(x), [a,b,c])
print "L1 -&gt; %s" % L1

L2 = reduce(lambda x, y: x+y, L1)
print "L2 -&gt; %s" % L2
</pre>
</div>



</section>
<section id="slide-sec-7-0-10">
<h4 id="sec-7-0-10">MapReduce y map-reduce</h4>
<ul>
<li>Básicamente es lo mismo, pero&#x2026;</li>
<li><code>map</code>, <code>reduce</code> (entre otras) son parte de lenguajes funcionales.</li>
<li><code>MapReduce</code> es la aplicación de esta idea aplicada a problemas <i>vergonzosamente</i> <i>paralelos</i>.
<ul>
<li>Ver la carpeta <code>docs</code> para el artículo de <b>Google</b> sobre <code>MapReduce</code>.</li>

</ul></li>

</ul>


</section>
<section id="slide-sec-7-0-11">
<h4 id="sec-7-0-11">GNU Parallel</h4>
<div class="org-src-container">

<pre  class="src src-sh">find ./data/books -type f | parallel -j0 egrep -i  '\[\[:digit:\]\]' {} | awk '{s+=$1} END {print s}'
</pre>
</div>


<ul>
<li><b>¿Puedes identificar las partes <code>map</code> y <code>reduce</code>?</b></li>
<li>Esto ya es un <code>MapReduce</code>.</li>

</ul>


</section>
<section id="slide-sec-7-0-12">
<h4 id="sec-7-0-12">MapReduce en Hadoop</h4>
<ul>
<li>A nivel programático:
<ul>
<li><i>Data</i> de entrada</li>
<li>Programa MapReduce</li>
<li>Configuración</li>
<li>Subtareas: <code>map</code> y <code>reduce</code></li>

</ul></li>

</ul>


</section>
<section id="slide-sec-7-0-13">
<h4 id="sec-7-0-13">MapReduce: <i>Mapper</i></h4>
<ul>
<li>Hadoop divide la entrade de datos al <i>job</i> MapReduce en pedazos de tamaño fijo llamados <i>input splits</i>.</li>
<li>Hadoop crea una tarea <code>map</code> para cada <i>input split</i>.</li>
<li><code>map</code> escribe al <i>file system</i> local.
<ul>
<li>Si el <code>reducer</code> tiene éxito se borra la salida del <i>mapper</i>.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-7-0-14">
<h4 id="sec-7-0-14">Map only</h4>

<div class="figure">
<p><img src="./imagenes/map_only.png" alt="map_only.png" />
</p>
</div>


</section>
<section id="slide-sec-7-0-15">
<h4 id="sec-7-0-15">MapReduce: <i>Reducer</i></h4>
<ul>
<li>La entrada es la salida de (posiblemente) todos los <i>mappers</i>.</li>
<li>Estas se transmiten vía red al nodo donde corre el <i>reducer</i>.</li>
<li>La salida se guarda en el <code>HDFS</code>.</li>

</ul>

</section>
<section id="slide-sec-7-0-16">
<h4 id="sec-7-0-16">Map, One reduce</h4>

<div class="figure">
<p><img src="./imagenes/map_one_reduce.png" alt="map_one_reduce.png" />
</p>
</div>

</section>
<section id="slide-sec-7-0-17">
<h4 id="sec-7-0-17">MapReduce</h4>

<div class="figure">
<p><img src="./imagenes/map_reduce.png" alt="map_reduce.png" />
</p>
</div>


</section>
<section id="slide-sec-7-0-18">
<h4 id="sec-7-0-18">MapReduce: <i>Combiner</i></h4>
<ul>
<li>Es una medida de optimización.</li>
<li>Es para ahorrar ancho de banda.</li>
<li>Una especie de <i>reducer</i> local.</li>
<li>No es parte (estrictamente) del MapReduce
<ul>
<li>Por eso no lo había mencionado.</li>

</ul></li>

</ul>


</section>
<section id="slide-sec-7-0-19">
<h4 id="sec-7-0-19">Word count</h4>
<ul>
<li>Es el ejemplo <i>Hola Mundo</i> de Apache Hadoop.</li>
<li>No sólo eso, es el ejemplo que se utiliza en el trabajo seminal
<ul>
<li><b>MapReduce: Simplified Data Processing on Large Clusters</b> <i>(2006)</i>.</li>
<li>En la carpeta <code>docs</code> como ya había dicho.</li>

</ul></li>
<li>Solamente 1 <code>Map</code> y 1 <code>Reduce</code>.</li>

</ul>

</section>
<section id="slide-sec-7-0-20">
<h4 id="sec-7-0-20">Word count</h4>
<ul>
<li><b>mapper</b>
<ul>
<li><code>k1</code> -&gt; nombre de archivo</li>
<li><code>v1</code> -&gt; texto del archivo</li>
<li><code>k2</code> -&gt; palabra</li>
<li><code>v2</code> -&gt; "1"</li>

</ul></li>

<li><b>reducer</b>
<ul>
<li><code>k2</code> -&gt; palabra</li>
<li>list(v2) -&gt; (1,1,1,1,1,1,&#x2026;, 1)</li>

</ul>
<p>
Suma los "1" y produce una lista de
</p>

<ul>
<li>k3 -&gt; palabra</li>
<li>v3 -&gt; suma</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-7-0-21">
<h4 id="sec-7-0-21">Word count</h4>

<div class="figure">
<p><img src="./imagenes/word_count.png" alt="word_count.png" />
</p>
</div>

</section>
<section id="slide-sec-7-0-22">
<h4 id="sec-7-0-22">Pseudocódigo</h4>
<pre><code data-trim>
map (String key, String value)
   for each word w in value
      Emit(w, 1)

reduce (String key, Iterator values)
   int wordcount = 0
   for each v in values
      wordcount += v
      Emit(key, wordcount)

</code></pre>

</section>
<section id="slide-sec-7-0-23">
<h4 id="sec-7-0-23">Mockup</h4>
<ul>
<li>Ver los archivos <code>word_count.py</code> y <code>mapreduce.py</code> en la carpeta <code>mock</code>.</li>

</ul>

<pre><code data-trim>
chmod +x word_count.py
python word_count.py
</code></pre>

<ul>
<li>Este es un ejemplo de mentiritas, no usa Apache Hadoop.</li>

</ul>


</section>
<section id="slide-sec-7-0-24">
<h4 id="sec-7-0-24">Ejercicio</h4>
<ul>
<li>Organízate en grupos de cuatro personas</li>

<li>Diseñe (en <i>pseudo código</i>, imágen, código, lo que sea más fácil) el <b><b>MapReduce</b></b> para lo siguiente:
<ul>
<li>Encontrar el máximo de un conjunto de datos.</li>
<li>Encontrar el promedio y desviación estándar de unos datos.</li>
<li>Encontrar el top 10 de una cantidad.</li>
<li>Contar por grupo</li>

</ul></li>

<li>Subir el <i>diseño</i> a <code>github</code> en la carpeta <code>grupos</code>, dentro de una carpeta de equipo (agregar un <code>README</code> para saber los participantes)</li>

<li><b>NOTA</b> Esta carpeta será la usada para el proyecto final.</li>

</ul>


</section>
</section>
<section>
<section id="slide-sec-8">
<h2 id="sec-8">Abstracciones de MapReduce</h2>
<div class="outline-text-2" id="text-8">
</div></section>
<section id="slide-sec-8-1" data-state="soothe">
<h3 id="sec-8-1">Abstracciones: Pig</h3>

</section>
<section id="slide-sec-8-1-1">
<h4 id="sec-8-1-1">Pig</h4>
<ul>
<li>Proyecto de Apache</li>
<li>Abstracción encima de Hadoop
<ul>
<li><i>Pig Latin</i> compila a <code>MapReduce</code></li>
<li>En cierta forma <i>Pig Latin</i> es para analistas, <i>data scientist</i> y estadísticos.</li>
<li><code>MapReduce</code>  es para programadores (aunque los <i>data scientist</i> deberían de poder hacerlo también)</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-8-1-2">
<h4 id="sec-8-1-2">Pig</h4>
<ul>
<li>Pig es un <i>data flow programming language</i></li>
<li>Es decir,
<ul>
<li>Ejecuta paso a paso</li>
<li>Cada paso es una transformación de datos</li>

</ul></li>
<li>En cambio <code>SQL</code> es un conjunto de <i>constraints</i> que en conjunto definen el resultado buscado.</li>

</ul>

</section>
<section id="slide-sec-8-1-3">
<h4 id="sec-8-1-3">Pig</h4>
<ul>
<li>¿Qué cosas puede hacer?
<ul>
<li><code>joins</code></li>
<li><code>sorts</code></li>
<li><code>filters</code></li>
<li><code>group by</code></li>
<li><i>User defined functions</i> <code>UDF</code>'s</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-8-1-4">
<h4 id="sec-8-1-4">Pig</h4>
<ul>
<li>¿Qué cosas <b>puedo</b> hacer?

<ul>
<li><code>ETLs</code>
<ul>
<li>Limpiar.</li>
<li><i>Joins</i> gigantes.</li>

</ul></li>

<li>Búsqueda en <i>Raw</i>.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-8-1-5">
<h4 id="sec-8-1-5">Pig</h4>
<ul>
<li>Componentes
<ul>
<li><i>Pig Latin</i>
<ul>
<li>Los <code>keywords</code> no son <i>case-sensitive</i>, pero las relaciones y los <code>UDFs</code> si lo son.</li>

</ul></li>
<li><code>Grunt</code>
<ul>
<li>Local</li>
<li>MapReduce</li>

</ul></li>
<li><code>Pig compiler</code></li>

</ul></li>

</ul>

</section>
<section id="slide-sec-8-1-6">
<h4 id="sec-8-1-6">Pig</h4>
<ul>
<li>Es posible ejecutar también <i>scripts</i> de <i>Pig Latin</i> (terminación <code>.pig</code>) sin entrar a <code>grunt</code>.</li>

</ul>

<pre><code data-trim>
pig script_file.pig
</code></pre>

<ul>
<li>Si quieren pasar parámetros</li>

</ul>
<pre><code data-trim>
pig -p var=bla/bla var2=bla/bla/bla script_file.pig
</code></pre>

<ul>
<li>Y usarse desde programas en <code>Java</code> con la clase <code>PigServer</code>.
<ul>
<li>Como una especie de <code>JDBC</code>, pero para <i>Pig Latin</i>.</li>

</ul></li>

</ul>


</section>
<section id="slide-sec-8-1-7">
<h4 id="sec-8-1-7">Pig latin: <i>Building blocks</i></h4>
<ul>
<li>Escalares
<ul>
<li>Son interfaces a clases <code>java.lang</code>
<ul>
<li><code>int</code>, , <code>long</code>, <code>float</code>, <code>double</code></li>

</ul></li>
<li>Por ejemplo:</li>

</ul></li>

</ul>
<pre><code data-trim>
'Adolfo'
</code></pre>

<ul>
<li>Tuplas
<ul>
<li>Colleción ordenada de tamaño fijo de datos.</li>
<li>Están divididos en <i>fields</i>, cada uno conteniendo un elemento.</li>
<li>Como son ordenados, se pueden referir por posición.</li>

</ul></li>

</ul>
<pre><code data-trim>
('Adolfo', 3, 8.17, 23)
</code></pre>

<ul>
<li><i>Bags</i>
<ul>
<li>Colección sin ordenar de tuplas.</li>

</ul></li>

</ul>
<pre><code data-trim>
{('Adolfo', 3, 8.17, 23), ('Paty', 3.14, 9, 'A')}
</code></pre>


</section>
<section id="slide-sec-8-1-8">
<h4 id="sec-8-1-8">Pig latin: <i>Operaciones</i></h4>
<ul>
<li><code>load</code>, <code>store</code>, <code>dump</code></li>

</ul>
<pre><code data-trim>
store procesados into 'output/procesados'; -- Guarda la relación en el  HDFS
dump procesados; -- Imprime en pantalla la relación
</code></pre>

<ul>
<li><code>foreach</code>
<ul>
<li>Aplica un conjunto de expresiones a cada elemento del <i>data pipeline</i>.</li>
<li>Es el operador de proyección de <code>Pig latin</code>.</li>

</ul></li>

<li><code>filter</code>
<ul>
<li>Seleccionar que registros se mantendrán en el <i>data pipeline</i>.</li>

</ul></li>

<li><code>group</code>
<ul>
<li>Agrupa registros con la misma llave en un <i>bag</i>.</li>
<li>La sintaxis es parecida a la de <code>sql</code>, pero son muy diferentes.
<ul>
<li>No hay relación entre el agrupamiento y las funciones de agregación (recuerden sus clases de <code>sql</code>).</li>

</ul></li>

</ul></li>

<li><code>order by</code>
<ul>
<li>Ordena los datos.</li>

</ul></li>

<li><code>distinct</code>
<ul>
<li>Remueve duplicados.</li>

</ul></li>

<li><code>limit</code>, <code>sample</code>
<ul>
<li>Limita la cantidad de información que se ve.</li>

</ul></li>

<li><code>parallel</code>
<ul>
<li>Afecta la cantidad de <code>reducers</code> que hay.</li>

</ul></li>

</ul>


</section>
<section id="slide-sec-8-1-9">
<h4 id="sec-8-1-9">¿Qué funciones hay?</h4>
<p>
<a href="http://www.qubole.com/resources/cheatsheet/pig-function-cheat-sheet/">Pig Cheatsheet</a>
</p>


</section>
<section id="slide-sec-8-1-10">
<h4 id="sec-8-1-10">Expresiones Regulares de Java</h4>
<p>
En los siguientes ejercicios llegaremos a usar expresiones regulares <code>Pig</code> y <code>hive</code> soportan las <code>regex</code> de <code>Java</code>,
<a href="http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html">aquí</a> pueden obtener más información sobre el tema.
</p>

</section>
<section id="slide-sec-8-1-11">
<h4 id="sec-8-1-11">Ejemplo: Wordcount</h4>
<ul>
<li>Para comprender bien lo que está pasando te recomiendo usar <code>illustrate</code> o <code>describe</code> en cada paso.</li>

</ul>

<pre><code data-trim>
shakespeare = load 'books/pg100.txt' using TextLoader as (line:chararray);
-- Usando UDFs y expresiones regulares de Java
palabras = foreach shakespeare generate flatten(TOKENIZE(REPLACE(LOWER(TRIM(line)), '[\\p{Punct}, \\p{Cntrl}]', ' '))) as palabra;
grupo = group palabras by palabra;
conteo = foreach grupo generate $0 as palabra, count($1) as cantidad;
ordenados = order conteo by cantidad desc;
top10 = limit ordenados 10;
dump top10;
</code></pre>

</section>
<section id="slide-sec-8-1-12">
<h4 id="sec-8-1-12">Ejercicio</h4>
<ul>
<li>Describe cada línea con comentarios y agrega los esquemas.
<ul>
<li>i.e. elimina los <code>$0</code>, etc</li>

</ul></li>
<li>Guarda la salida de cada uno de estos en una carpeta <code>output/wordcount/pig</code> en tu carpeta <code>hdfs</code>.</li>
<li>Copia la salida al sistema de archivos local y súbelo a <code>github</code>.</li>

</ul>

</section>
<section id="slide-sec-8-1-13">
<h4 id="sec-8-1-13">Pig: JOINS</h4>
<ul>
<li>Cargamos fuente 1</li>
<li>Cargamos fuente 2</li>
<li>Unimos las fuentes (<i>bags</i>) mediante una llave</li>
<li>Súper simple</li>

</ul>

<p>
Pig soporta <i>inner joins</i> (valor por omisión), <i>left outer joins</i> (y
<i>right</i> también) y <i>full outer</i> joins.
</p>


<pre><code data-trim>
fuentes_unidas = join fuente1 by (keys) [left|right|full outer] fuente2 by (keys);
</code></pre>

<p>
Además <code>Pig</code> soporta <code>cogroup</code> además de los <code>joins</code> (el <code>cogroup</code>
preserva la estructura de las fuentes y crea tuplas por cada llave)
</p>

<pre><code data-trim>
fuentes_unidas = cogroup fuente1 by (keys) fuente2 by (keys);
</code></pre>


</section>
<section id="slide-sec-8-1-14">
<h4 id="sec-8-1-14">Pig: Ejemplo de JOINs y COGROUPs</h4>
<ul>
<li>Fuente de datos: <code>mascotas (dueño, mascotas)</code></li>

</ul>
<pre><code data-trim>
(Adolfo, tortuga)
(Adolfo, pez)
(Adolfo, gato)
(Paty, perro)
(Paty, gato)
</code></pre>

<ul>
<li>Fuente de datos: <code>amigos(amigo1, amigo2)</code></li>

</ul>
<pre><code data-trim>
(Diana, Adolfo)
(Gabriel, Adolfo)
(Shanti, Paty)
</code></pre>

<ul>
<li><code>COGROUP mascotas by dueño, amigos por amigo2;</code></li>

</ul>
<pre><code data-trim>
(Adolfo, {(Adolfo, tortuga), (Adolfo, pez), (Adolfo, gato)}, {(Diana, Adolfo), (Gabriel, Adolfo)})
(Paty, {(Paty, perro), (Paty, gato)}, {(Shanti, Paty)})
</code></pre>

<ul>
<li><code>JOIN mascotas by dueño, amigos por amigo2;</code></li>

</ul>
<pre><code data-trim>
(Adolfo, tortuga, Diana)
(Adolfo, tortuga, Gabriel)
(Adolfo, pez, Diana)
(Adolfo, pez, Gabriel)
(Adolfo, gato, Diana)
(Adolfo, gato, Gabriel)
(Paty, perro, Shanti)
(Paty, gato, Shanti)
</code></pre>

</section>
<section id="slide-sec-8-1-15">
<h4 id="sec-8-1-15">Aclaraciones sobre GROUP y FLATTEN</h4>
<ul>
<li><code>FLATTEN</code> elimina un nivel anidamiento
<ul>
<li>Ejemplo</li>

</ul></li>

</ul>
<pre><code data-trim>
(Adolfo, (tortuga, pez, gato))
(Paty, (perro, gato))
</code></pre>
<ul>
<li>FLATTEN eliminaría los bags internos</li>

</ul>
<pre><code data-trim>
(Adolfo, tortuga)
(Adolfo, pez)
(Adolfo, gato)
(Paty, perro)
(Paty, gato)
</code></pre>

<ul>
<li><code>GROUP .. BY</code> organiza los <i>bags</i> en <i>bags</i>
<ul>
<li>Siguiendo con los datos anteriores de mascotas:
<ul>
<li>GROUP mascotas BY dueño;</li>

</ul></li>

</ul></li>

</ul>
<pre><code data-trim>
( Adolfo, {(Adolfo, tortuga), (Adolfo, pez), (Adolfo, gato)} )
( Paty, {(Paty, perro), (Paty, gato)} )
</code></pre>

<ul>
<li>En cierto sentido <code>FLATTEN</code> y <code>GROUP .. BY</code> son operaciones inversas
entre sí.</li>

</ul>

</section>
<section id="slide-sec-8-1-16">
<h4 id="sec-8-1-16">Ejercicio</h4>
<ul>
<li>Describe cada línea con comentarios y agrega los esquemas.</li>
<li>Guarda la salida de cada uno de estos en una carpeta <code>output/ufos/pig</code> en tu carpeta <code>hdfs</code>.</li>
<li>Copia la salida al sistema de archivos local y súbelo a <code>github</code>.</li>

</ul>

<pre><code data-trim>
ufos = load 'ufos' using org.apache.hive.hcatalog.pig.HCatLoader();
a_imprimir = limit ufos 5;
por_estado = group ufos by State;
describe por_estado;
explain por_estado;
illustrate por_estado;
-- itera sobre cada elemento del bag
conteo = foreach por_estado generate group count_star(ufos);
ordenados = order conteo by $1 desc;
top_five = limit ordenado 5;
unicos = distinct conteos;
muestreo = sample por_estado 0.1;
filtrados = filter conteos by substring(group, 0, 2) == 'W';
mayores = filter conteos by $1 > 50;
</code></pre>


</section>
<section id="slide-sec-8-2" data-state="soothe">
<h3 id="sec-8-2">Abstracciones: Hive</h3>

</section>
<section id="slide-sec-8-2-1">
<h4 id="sec-8-2-1">Hive</h4>
<ul>
<li>Proyecto de Apache.</li>
<li><span class="underline">Abstracción</span> pra modelar y procesar datos en Hadoop.</li>
<li>Proveé de una manera de estructurar datos guardados en el <code>HDFS</code>.</li>
<li>Permite crear <span class="underline">queries</span> muy similares a <code>SQL</code> (llamado <code>HQL</code>) y correrlos contra los datos.</li>
<li>Contiene un almacén de metadatos (<code>HCatalog</code>), que además puede ser compartido con otras interfaces como <code>Pig</code>, <code>MapReduce</code>, <code>Impala</code>, <code>Spark</code>, etc.</li>
<li>Da Acceso al <code>HDFS</code> y <code>HBase</code>.</li>

</ul>

</section>
<section id="slide-sec-8-2-2">
<h4 id="sec-8-2-2">Bibliografía recomendada</h4>
<ul>
<li>Sitio web de Hive</li>
<li>Hadoop: The Definitive Guide</li>
<li>Programming Hive</li>

</ul>


</section>
<section id="slide-sec-8-2-3">
<h4 id="sec-8-2-3">Arquitectura de Apache Hive</h4>

<div class="figure">
<p><img src="./imagenes/hive-remote.jpeg" alt="hive-remote.jpeg" />
</p>
</div>


</section>
<section id="slide-sec-8-2-4">
<h4 id="sec-8-2-4">¿Qué funciones hay?</h4>
<p>
<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF">Apache Hive Docs</a>
</p>

</section>
<section id="slide-sec-8-2-5">
<h4 id="sec-8-2-5">Ejemplo: Wordcount</h4>
<ul>
<li>Describe cada línea con comentarios y agrega los esquemas.</li>
<li>Averigua (usando la documentación) como guardar la tabla a archivo y compara el <code>top10</code> con el resultado de <code>Pig</code>.</li>
<li>¿Por qué da diferente?</li>
<li>Modifica el código para arreglar la diferencia.</li>
<li>Guarda la salida de cada uno de estos en una carpeta <code>output/wordcount/hive</code> en tu carpeta <code>hdfs</code>.</li>
<li>Copia la salida al sistema de archivos local y súbelo a <code>github</code>.</li>

</ul>

<pre><code data-trim>
-- Limpiamos el ambiente
delete table shakespeare;
delete table wordcount;

-- Creamos la tabla que contendrá las obras de Shakespeare
create table shakespeare(linea string);

-- Verifiamos
show tables;

-- Cargamos los datos a la tabla
load data inpath '/user/itam/books/pg100.txt' overwrite into table shakespeare;

-- Quereamos y guardamos en una tabla
-- Los símbolos raros '\\p{Punct}' y similares son expresiones regulares de Java
create table wordcount as
select palabra, count(*) as conteo from
(
select
explode(split(lcase(regexp_replace(trim(linea),'[\\p{Punct}, \\p{Cntrl}]', ' ')), ' ')) as palabra
from shakespeare
) palabras
group by palabra
order by conteo desc limit 10;
-- Este código  se podría hacer más pequeño con LATERAL (¿Recuerdan la clase de PostgreSQL?)
</code></pre>

</section>
<section id="slide-sec-8-2-6">
<h4 id="sec-8-2-6">Ejercicio</h4>
<ul>
<li>Repite el ejercicio de <code>Pig</code> sobre <code>ufos</code>, pero ahora en <code>Hive</code>.</li>

</ul>



</section>
</section>
<section>
<section id="slide-sec-9" data-background="#000fff">
<h2 id="sec-9">Procesamiento: Spark</h2>
</section>
<section id="slide-sec-9-1">
<h3 id="sec-9-1">Spark</h3>
<ul>
<li><i>Framework</i> de cómputo general para <i>clusters</i></li>
<li>Ejecuta en <code>YARN</code>
<ul>
<li>Aunque también puede hacer <i>standalone</i>, o ejecutar sobre <code>EC2</code> o <code>Mesos</code>.</li>

</ul></li>
<li>Soporta varios lenguajes
<ul>
<li><code>Python</code>, <code>Java</code> o <code>Scala</code></li>
<li>Hay soporte experimental para <code>R</code>: <a href="https://spark-summit.org/2014/wp-content/uploads/2014/07/SparkR-SparkSummit.pdf"><code>SparkR</code></a>
<ul>
<li>Esta presentación tiene un excelente chiste sobre los <b>RDDs</b>.</li>

</ul></li>
<li>En esta clase usaremos <code>Python</code>, pero es necesario mencionar que para explotar esta herramienta al máximo habría que aprender <code>Scala</code>.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-9-2">
<h3 id="sec-9-2">Resilient Distributed Datasets (RDDs)</h3>
<ul>
<li>Es una de las ideas principales de Spark.</li>
<li><code>RDDs</code> es una abstracción que representa una colleción de objetos de sólo lectura que está particionada a lo largo de varias máquinas.</li>
<li>Sus ventajas:
<ul>
<li>Pueden ser reconstruidas a partir de su <i>lineage</i>. (Soportan fallos&#x2026;)</li>
<li>Pueden ser accesadas vía operaciones en paralelo, parecidas a MapReduce.</li>
<li>Son <i>cached</i> en memoria para su uso inmediato.</li>
<li>Fueron construidas para ser almacenadas de manera distribuida.</li>
<li>Contienen cualquier tipo de dato (ya sea de <code>Python</code>, <code>Java</code> o <code>Scala</code>) incluidos tipos definidos por el programador.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-9-3">
<h3 id="sec-9-3">Resilient Distributed Datasets (RDDs)</h3>
<ul>
<li>Soportan dos tipos de operaciones
<ul>
<li><b>Transformaciones</b></li>
<li><b>Acciones</b>.</li>

</ul></li>

<li>Las <i>transformaciones</i> construyen un <code>RDD</code> nuevo a partir del anterior.
<ul>
<li>Muy similar a lo que hace cada paso de <code>Pig</code>.</li>
<li>Cada transformación queda guardada por <code>Spark</code> en el <i>lineage graph</i> un <b>DAG</b>.</li>

</ul></li>

<li>Las <i>acciones</i> calculan un resultado basado en el <code>RDD</code>.</li>

<li>La diferencia es que las <code>RDD</code> son computadas en forma <i>lazy</i>, sólo son ejecutadas hasta la acción.</li>

<li>Si quieres usarlo una <code>RDD</code> varias veces debes de persistirla (con <code>persist()</code>).</li>

</ul>

</section>
<section id="slide-sec-9-4">
<h3 id="sec-9-4">Flujo típico</h3>
<ol>
<li>Crear un <code>RDD</code> a partir de datos externos.</li>
<li>Transformarlo a nuevos <code>RDDs</code>.</li>
<li>Persistir algunos <code>RDDs</code> para su uso posterior.</li>
<li>Lanzar acciones.</li>

</ol>

</section>
<section id="slide-sec-9-5">
<h3 id="sec-9-5">Transformaciones</h3>
<ul>
<li><code>map</code>
<ul>
<li>Usa una función y la aplica a cada elemento del <code>RDD</code>, el resultado se guarda en un nuevo <code>RDD</code>.</li>

</ul></li>
<li><code>filter</code>
<ul>
<li>Usa una función y devuelve sólo los elementos que pasan la función (que devuelven verdadero) en el nuevo <code>RDD</code>.</li>

</ul></li>
<li><code>flatMap</code>
<ul>
<li>Como el <code>map</code> pero regresa un iterador por cada elemento
<ul>
<li>Por ejemplo una función que divide una cadena.</li>

</ul></li>

</ul></li>
<li><code>distinct</code>, <code>sample</code></li>
<li><code>union</code>, <code>intersection</code>, <code>substract</code>, <code>cartesian</code></li>

</ul>

</section>
<section id="slide-sec-9-6">
<h3 id="sec-9-6">Acciones</h3>
<ul>
<li><code>reduce</code>
<ul>
<li>Opera en dos elementos del mismo tipo del <code>RDD</code> y regresa un elemento del mismo tipo.</li>

</ul></li>
<li><code>aggregate</code>
<ul>
<li>Nos permite implementar acumuladores.</li>

</ul></li>
<li><code>collect</code>
<ul>
<li>Regresa el <code>RDD</code> completo.</li>

</ul></li>
<li><code>take</code>
<ul>
<li>Regresa un número <code>n</code> de elementos del  <code>RDD</code>.</li>

</ul></li>
<li><code>count</code>, <code>countByValue</code>, <code>top</code>, <code>foreach</code>.</li>

</ul>

</section>
<section id="slide-sec-9-7">
<h3 id="sec-9-7">Ejemplo: WordCount</h3>
<pre><code data-trim>

def tokenize(texto):
    texto.split()

shakespeare = sc.textFile("hdfs://localhost/user/itam/books/pg100.txt")

wordcount = shakespeare.flatMap(tokenize).\
                        map(lambda x: (x,1)).\
                        reduceByKey(add).\
                        map(lambda x: (x[1], x[0])).\
                        sortByKey(False)

wordcount.take(10)
</code></pre>

</section>
<section id="slide-sec-9-8">
<h3 id="sec-9-8">Ejercicio</h3>
<ul>
<li>Explica el código anterior</li>
<li>Modifica el código de <code>Spark</code> y <code>Python</code> para que reproduzca el resultado de <code>Pig</code> y <code>Hive</code>.</li>
<li>Súbelo a github</li>

</ul>

</section>
<section id="slide-sec-9-9">
<h3 id="sec-9-9">Ejercicio</h3>
<ul>
<li>Repite el ejercicio de <code>Pig</code> sobre <code>ufos</code>, pero ahora en <code>Spark</code>.</li>

</ul>


</section>
</section>
<section>
<section id="slide-sec-10" data-background="#000fff">
<h2 id="sec-10">Procesamiento: Impala</h2>
</section>
<section id="slide-sec-10-1">
<h3 id="sec-10-1">Impala</h3>
<ul>
<li>Cloudera basó este desarrollo en dos <i>white papers</i> de Google describiendo <i>baja latencia</i> en consultas con tecnologías llamadas <b>F1</b> y <b>Dremel</b>.</li>
<li>No está basado en el motor de procesamiento <code>MapReduce</code>.</li>
<li>Optimizado en <i>latencia</i>.</li>
<li>Usa <code>SQL</code> y utiliza <code>Hive Metastore</code>.</li>
<li>Soporta el <code>hdfs</code> y <code>HBase</code>.</li>

</ul>

</section>
<section id="slide-sec-10-2">
<h3 id="sec-10-2">Impala</h3>
<ul>
<li>Todo está en memoria
<ul>
<li>No escribe a disco como <code>MapReduce</code>.</li>

</ul></li>
<li>Tiene los demonios siempre corriendo.
<ul>
<li>No levanta procesos para cada tarea, como <code>MapReduce</code>.</li>

</ul></li>
<li>Escrito en <code>C++</code> no en <code>Java</code>.</li>

<li><b>Nota</b>: Con la aparición de <a href="https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark:+Getting+Started?cmp=ex-data-na-na-na_architectural_considerations_for_hadoop_applications_2"><code>Hive on Spark</code></a>, habrá que ver cual <i>framework</i> gana.</li>

</ul>

</section>
<section id="slide-sec-10-3">
<h3 id="sec-10-3">¿Qué funciones hay?</h3>
<p>
<a href="http://www.cloudera.com/content/cloudera/en/documentation/cloudera-impala/latest/topics/impala_functions.html">Built-in Functions de Impala</a>
</p>

</section>
<section id="slide-sec-10-4">
<h3 id="sec-10-4">Ejercicio</h3>
<ul>
<li>Escribe el wordcount en Impala.
<ul>
<li>Deberás averiguar que funciones utilizar para reproducir la salida de <code>Pig</code> y <code>Hive</code>.</li>

</ul></li>

<li>Súbelo a github el código y el resultado, así como la comparación con las otras salidas.</li>

</ul>

</section>
<section id="slide-sec-10-4-1">
<h4 id="sec-10-4-1">Ejercicio</h4>
<ul>
<li>Repite el ejercicio de <code>Pig</code> sobre <code>ufos</code>, pero ahora en <code>Impala</code>.</li>

</ul>


</section>
</section>
<section>
<section id="slide-sec-11" data-background="#000fff">
<h2 id="sec-11">Arquitectura: Orquestación</h2>

</section>
<section id="slide-sec-11-1">
<h3 id="sec-11-1">Orquestación</h3>
<ul>
<li>Regularmente existen varios pasos de procesamiento para preparar los datos.
<ul>
<li>Extraer los datosw (desde una carpeta, el internet, una base de
datos) e importarlos al <code>hdfs</code>.</li>
<li>Validar los datos.</li>
<li>Trasnformarlos a un formato más adecuado.</li>
<li>Ejecutar agregaciones y generación de variables.</li>

</ul></li>

<li>Además estos pasos se empiezan a ejecutar cuando:
<ul>
<li>A un tiempo dado
<ul>
<li>e.g. Cada medianoche</li>

</ul></li>
<li>Un evento ocurre
<ul>
<li>e.g. Se agregó un nuevo archivo</li>

</ul></li>

</ul></li>

<li>Coordinar los pasos
<ul>
<li>Un paso se sigue al otro, sólo si el anterior terminó
exitosamente.</li>
<li>Repetir el paso</li>

</ul></li>

<li>Tomar acciones de gestión
<ul>
<li>Mandar correos</li>
<li>Tomar tiempos de ejecución</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-11-2">
<h3 id="sec-11-2">Orquestación</h3>
<ul>
<li>Al concepto de coordinación, gestión, programación se les conoce
como <i>orquestación</i>.</li>

<li>La <i>orquestación</i> (como muchas cosas en este curso) se representa
por un grafo dirigido acíclico (<code>DAG</code>).</li>

<li>A un <code>DAG</code> se le conoce como <i>workflow</i> y a la administración de los
<i>worflows</i> se le conoce commo <b>orquestación de workflows</b>.</li>

<li>En esta clase veremos a <code>Luigi</code> como <i>orquestador</i>.</li>

</ul>

</section>
<section id="slide-sec-11-3">
<h3 id="sec-11-3">Ejercicio</h3>
<ul>
<li>Reúnanse con su equipo y diseñen los pasos que tendría que tener un
<i>workflow</i> para <code>ufos</code> y para <code>gdelt</code>.</li>

<li>el objetivo del <i>workflow</i> es dual, dejarlos preparados para nueva
explotación analítica y prepararlos para mostrarlos en un <code>shiny</code>.</li>

<li>Recuerde usar la estructura de directorios discutida.</li>

<li>Indica las posibles excepciones.</li>

<li>Describa los pasos en un documento y súbalo a <code>github</code> en la carpeta
del equipo.</li>

</ul>




</section>
<section id="slide-sec-11-4">
<h3 id="sec-11-4">"Orquestando" la  Ingesta</h3>
<ul>
<li>¿Cada cuándo se ingestan?
<ul>
<li><i>super batch</i>, <i>batch</i>, toma de decisiones <i>near real time</i>, proceso de eventos <i>near real time</i>, <i>real time</i>.</li>

</ul></li>

<li>¿Incremental o <i>full</i>?
<ul>
<li>Recuerda que <code>hdfs</code> es casi <i>read-only</i>&#x2026;</li>

</ul></li>

<li>Consideraciones
<ul>
<li>Del sistema de entrada
<ul>
<li>Velocidad de los discos</li>
<li>Tipo del archivo de entrada</li>
<li>Base de datos relacional</li>
<li><i>Streaming</i></li>

</ul></li>

<li>Transformaciones, Particionado, División</li>
<li>¿ <i>On-the-fly</i> ?</li>
<li>Cuellos de botella</li>
<li>¿Push o Pull?</li>
<li>Casos de excepción.</li>

</ul></li>

</ul>



</section>
<section id="slide-sec-11-5">
<h3 id="sec-11-5">Apache Flume</h3>
<div class="outline-text-3" id="text-11-5">
</div></section>
<section id="slide-sec-11-5-1">
<h4 id="sec-11-5-1">¿Por qué?</h4>
<ul>
<li>Hasta este momento hemos movido los datos hacia dentro de <code>HDFS</code> de
manera manual usando <code>hadoop fs ...</code>
<ul>
<li>Esto no va a escalar&#x2026;</li>

</ul></li>

<li>Si han puesto atención a la clase, quizá quieran automatizar esto
usando <code>bash</code>, y una combinación de <code>parallel</code> con <code>scp</code> y
obviamente <code>hadoop fs ...</code></li>

<li>Si logran armar el <i>script</i> el siguiente paso es ejecutarlo
"solito", y para eso quizá haya que poner un <code>cron</code></li>

</ul>

</section>
<section id="slide-sec-11-5-2">
<h4 id="sec-11-5-2">¿Por qué?</h4>
<ul>
<li>Esto tampoco va a escalar&#x2026;
<ul>
<li><i>hardcodeado</i></li>
<li>Poca o nula configuración</li>
<li>Mantenimiento</li>
<li>Manejo de cargas</li>
<li>Manejo de datos en masa</li>
<li>Manejo de fallos y excepciones</li>
<li>Agreguen su pesadilla favorita</li>

</ul></li>

</ul>


</section>
<section id="slide-sec-11-5-3">
<h4 id="sec-11-5-3">Apache Flume</h4>
<ul>
<li>Componente para la captura (<i>ingesta</i>) de datos basados en eventos a Hadoop.</li>
<li>Apache Flume es un sistema de alto desempeño para la ingesta de datos.</li>
<li>Es escalable horizontalmente y muy configurable.</li>

<li>Una posible alternativa es <b>Apache Kafka</b>.</li>

</ul>

</section>
<section id="slide-sec-11-5-4">
<h4 id="sec-11-5-4">Apache Flume</h4>
<ul>
<li>Distribuido
<ul>
<li>Agentes en varias máquinas</li>

</ul></li>

<li>Escalable
<ul>
<li>Más máquinas, más eventos</li>

</ul></li>

<li>Confiable
<ul>
<li>Almacenamiento confiable, <i>failover</i>, replicación, distribución, etc.</li>

</ul></li>

<li>Fácil instalación y configuración.</li>

</ul>

</section>
<section id="slide-sec-11-5-5">
<h4 id="sec-11-5-5">¿Cómo funciona?</h4>

<div class="figure">
<p><img src="./imagenes/Diagramas_1(1).jpg" alt="Diagramas_1(1).jpg" />
</p>
</div>

</section>
<section id="slide-sec-11-5-6">
<h4 id="sec-11-5-6">¿Cómo funciona?</h4>

<div class="figure">
<p><img src="./imagenes/Diagramas_2(1).jpg" alt="Diagramas_2(1).jpg" />
</p>
</div>

</section>
<section id="slide-sec-11-5-7">
<h4 id="sec-11-5-7">¿Cómo funciona?</h4>

<div class="figure">
<p><img src="./imagenes/Diagramas_3(1).jpg" alt="Diagramas_3(1).jpg" />
</p>
</div>


</section>
<section id="slide-sec-11-5-8">
<h4 id="sec-11-5-8">¿Cómo funciona?</h4>

<div class="figure">
<p><img src="./imagenes/Diagramas_4(1).jpg" alt="Diagramas_4(1).jpg" />
</p>
</div>


</section>
<section id="slide-sec-11-5-9">
<h4 id="sec-11-5-9">¿Qué es un evento?</h4>
<ul>
<li>Unidad de datos transportada por <code>Flume</code>.</li>
<li>Compuesta por <code>Header</code> (un mapa: <code>Map &lt;java.lang.String
  java.lang.String&gt;</code>) y por <code>payload</code> (<code>byte[]</code>)</li>

</ul>

</section>
<section id="slide-sec-11-5-10">
<h4 id="sec-11-5-10">Agente de Flume</h4>
<ul>
<li>Responsable de transmitir los <i>eventos</i>.</li>
<li>Consiste en <i>Fuente</i> (<code>source</code>), <i>Canal</i> (<code>channel</code>) y <i>Sumidero</i> (<code>sink</code>).</li>

</ul>


<div class="figure">
<p><img src="./imagenes/Diagramas_5(1).jpg" alt="Diagramas_5(1).jpg" />
</p>
</div>

</section>
<section id="slide-sec-11-5-11">
<h4 id="sec-11-5-11">Agente de Flume</h4>
<ul>
<li><i>Fuente</i>
<ul>
<li><code>http</code>, <code>jms</code>, <code>netcat</code>, etc.</li>
<li><code>Exec</code>
<ul>
<li>Observa un comando de <code>unix</code> y procesa el <code>stdout</code> de este.</li>

</ul></li>
<li><i>Spooling directory</i>
<ul>
<li>Observa un directorio para la aparición de nuevos archivos.</li>
<li>Procesa los <i>eventos</i> de los nuevos archivos.</li>
<li>Después de procesarlo, lo borra o lo renombra.</li>

</ul></li>
<li><code>AvroSource</code></li>

</ul></li>

<li><i>Canal</i>
<ul>
<li>Sirve de <i>buffer</i> de los eventos mientras se van extrayendo de la
fuente.</li>
<li>Tipos: <code>Memory</code> (no es durable, muy rápido), <code>File</code> (Durable,
soporta cifrado, Es el canal de mayor capacidad). <code>JDBC</code></li>

</ul></li>

<li><i>Sumidero</i>
<ul>
<li>Remueve los eventos del <i>canal</i> y los envía a su siguiente
destino.</li>

<li><code>hdfs</code>, <code>hbase</code>, <code>solr</code>, <code>elasticsearch</code></li>
<li><code>AvroSink</code></li>

</ul></li>

</ul>

</section>
<section id="slide-sec-11-5-12">
<h4 id="sec-11-5-12">Interceptores</h4>
<ul>
<li>Proveen capacidad de inspeccionar y modificar <i>al vuelo</i> eventos.</li>
<li>Está pegado a la <i>fuente</i>.</li>
<li>Es invocado cada vez que un evento se mueve de la <i>fuente</i> al
<i>canal</i>.</li>
<li>Se pueden concatenar varios <i>interceptores</i>.</li>

</ul>


</section>
<section id="slide-sec-11-5-13">
<h4 id="sec-11-5-13">Ejemplo: Hola Mundo</h4>
<ul>
<li>Crea la carpeta <code>ingesta</code> en el <i>file system</i> local.</li>
<li>En ella crea un archivo llamado <code>hola_mundo.conf</code>. Este será el
archivo de configuración de nuestro primer ejemplo de <code>Flume</code>.</li>

</ul>

<pre><code data-trim>
agent.sources=s1
agent.channels=c1
agent.sinks=k1

agent.sources.s1.type=netcat
agent.sources.s1.channels=c1
agent.sources.s1.bind=0.0.0.0
agent.sources.s1.port=12345

agent.channels.c1.type=memory

agent.sinks.k1.type=logger
agent.sinks.k1.channel=c1
</code></pre>

<ul>
<li>Ejecuta la siguiente línea de comandos para levantar un agente de
<code>Flume</code>:</li>

</ul>

<pre><code data-trim>
flume-ng agent -n agent -c . -f ingesta/hola_mundo.conf -Dflume.root.logger=INFO,console
</code></pre>


</section>
<section id="slide-sec-11-5-14">
<h4 id="sec-11-5-14">Ejemplo: Hola Mundo</h4>
<ul>
<li>Ahora abre otra terminal y conéctate a <code>docker</code>
<ul>
<li>Recuerda que es con <code>docker exec...</code></li>

</ul></li>

<li>Cámbiate al usuario <code>itam</code>.</li>

<li>Ejecuta <code>nc localhost 12345</code></li>

<li>Y manda mensajes, observa como <code>Flume</code> reacciona.</li>

</ul>


</section>
<section id="slide-sec-11-5-15">
<h4 id="sec-11-5-15">Ejercicio I</h4>
<ul>
<li>En este ejercicio, veremos como mover datos desde una carpeta hacia
<code>hadoop</code> usando <code>flume</code>.
<ul>
<li>Lo haremos de manera incremental.</li>

</ul></li>

</ul>



<ul>
<li>Primero, Conectaremos el archivo al log en pantalla.</li>

<li>Crea la carpeta <code>/opt/ufos</code></li>

<li>Guarda lo siguiente en <code>ingestion/ufo-agent1.conf</code>.</li>

</ul>

<pre><code data-trim>
# Componentes
UFOAgent.sources = UFODir
UFOAgent.channels = c1
UFOAgent.sinks = UFOLogger

# Canal
UFOAgent.channels.c1.type = memory

# Fuente e Interceptores
UFOAgent.sources.UFODir.type = spooldir
UFOAgent.sources.UFODir.channels = c1
UFOAgent.sources.UFODir.spoolDir = /opt/ufos
UFOAgent.sources.UFODir.fileHeader = true
UFOAgent.sources.UFODir.channels = c1

# Sumidero
UFOAgent.sinks.UFOLogger.type=logger
UFOAgent.sinks.UFOLogger.channel=c1
</code></pre>

<ul>
<li>Para ejecutar el agente:</li>

</ul>

<pre><code data-trim>
flume-ng agent -n UFOAgent --conf ingestion -f ingestion/ufo_agent1.conf
</code></pre>

<ul>
<li>Para terminar <code>Ctrl+C</code>.</li>

</ul>

</section>
<section id="slide-sec-11-5-16">
<h4 id="sec-11-5-16">Ejercicio I</h4>
<ul>
<li>Antes de iniciar el siguiente ejercicio:
<ul>
<li>Limpia la carpeta  <code>/opt/ufos</code>.</li>
<li>Borra la carpeta en el <code>hdfs</code> <code>/ufos_flume/</code></li>

</ul></li>

<li>Haz esto mismo entre cada ejercicio de esta sección.</li>

</ul>


</section>
<section id="slide-sec-11-5-17">
<h4 id="sec-11-5-17">Ejercicio II</h4>
<ul>
<li>Ahora lo mandaremos al HDFS, como no queremos perder datos (y no es
tan rápido la generación de datos) usaremos un canal de archivo.</li>

</ul>


<ul>
<li>Guarda la configuración siguiente en <code>ingestion/ufo-agent2.conf</code>.</li>

</ul>

<pre><code data-trim>
# Componentes
UFOAgent.sources = UFODir
UFOAgent.channels = archivo
UFOAgent.sinks = UFOHDFS

# Canal
UFOAgent.channels.archivo.type = file
UFOAgent.channels.archivo.checkpointDir = /opt/ufos/log/checkpoint/
UFOAgent.channels.archivo.dataDirs = /opt/ufos/log/data/

# Fuente e Interceptores
UFOAgent.sources.UFODir.type = spooldir
UFOAgent.sources.UFODir.channels = archivo
UFOAgent.sources.UFODir.spoolDir = /opt/ufos
UFOAgent.sources.UFODir.fileHeader = true

# Sumidero
UFOAgent.sinks.UFOHDFS.type=hdfs
UFOAgent.sinks.UFOHDFS.channel=archivo
UFOAgent.sinks.UFOHDFS.hdfs.path = /user/itam/ufos_flume/
UFOAgent.sinks.UFOHDFS.hdfs.fileType = DataStream
UFOAgent.sinks.UFOHDFS.hdfs.filePrefix = UFOData
UFOAgent.sinks.UFOHDFS.hdfs.writeFormat = Text
UFOAgent.sinks.UFOHDFS.hdfs.batchSize = 200000
UFOAgent.sinks.UFOHDFS.hdfs.rollSize = 0
UFOAgent.sinks.UFOHDFS.hdfs.rollCount = 2000000
</code></pre>


<ul>
<li>Para ejecutar el agente:</li>

</ul>

<pre><code data-trim>
flume-ng agent -n UFOAgent --conf ingestion -f ingestion/ufo_agent2.conf
</code></pre>

</section>
<section id="slide-sec-11-5-18">
<h4 id="sec-11-5-18">Ejercicio II</h4>
<ul>
<li>Espera a que salga algo parecido a esto:</li>

</ul>

<pre><code data-trim>
...
15/05/20 06:01:12 INFO file.EventQueueBackingStoreFile: Start checkpoint for /opt/ufos/log/checkpoint/checkpoint, elements to sync = 530
15/05/20 06:01:12 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1432101643755, queueSize: 0, queueHead: 528
15/05/20 06:01:12 INFO file.Log: Updated checkpoint for file: /opt/ufos/log/data/log-1 position: 141088 logWriteOrderID: 1432101643755
15/05/20 06:01:20 INFO hdfs.BucketWriter: Closing /user/itam/ufos_flume//UFOData.1432101649049.tmp
15/05/20 06:01:20 INFO hdfs.BucketWriter: Renaming /user/itam/ufos_flume/UFOData.1432101649049.tmp to /user/itam/ufos_flume/UFOData.1432101649049
15/05/20 06:01:20 INFO hdfs.HDFSEventSink: Writer callback called
</code></pre>


<p>
<i>NOTA:</i> Otra manera de saber que ya terminó, es esperando a que
desaparezca el archivo con terminación <code>.tmp</code> en el <code>hdfs</code>.
</p>

</section>
<section id="slide-sec-11-5-19">
<h4 id="sec-11-5-19">Ejercicio III</h4>
<ul>
<li>Incorporaremos Avro y el esquema generado por <code>kite</code>.</li>

<li>Antes es necesario hacer lo siguiente
<ul>
<li>Esto es necesario debido a como está instalado el modo
pseudodistribuido de <b>Cloudera</b>.</li>

</ul></li>

</ul>


<pre><code data-trim>
sudo ln -s ../../kite/kite-morphlines-* /usr/lib/flume-ng/lib
sudo ln -s ../../kite/lib/metrics* /usr/lib/flume-ng/lib
sudo ln -s ../../kite/lib/config-1.0.2.jar /usr/lib/flume-ng/lib
sudo ln -s ../../kite/lib/Saxon-HE-9.5.1-5.jar /usr/lib/flume-ng/lib
sudo ln -s ../../kite/lib/tika-* /usr/lib/flume-ng/lib
</code></pre>



</section>
<section id="slide-sec-11-5-20">
<h4 id="sec-11-5-20">Ejercicio III</h4>
<ul>
<li>Guarda lo que sigue en <code>ingestion/ufo_agent3.conf</code></li>

</ul>

<pre><code data-trim>
# Componentes
UFOAgent.sources = UFODir
UFOAgent.channels = archivo
UFOAgent.sinks = UFOKiteDS

# Canal
UFOAgent.channels.archivo.type = file
UFOAgent.channels.archivo.checkpointDir = /opt/ufos/log/checkpoint/
UFOAgent.channels.archivo.dataDirs = /opt/ufos/log/data/

# Fuente
UFOAgent.sources.UFODir.type = spooldir
UFOAgent.sources.UFODir.channels = archivo
UFOAgent.sources.UFODir.spoolDir = /opt/ufos
UFOAgent.sources.UFODir.fileHeader = true
UFOAgent.sources.UFODir.deletePolicy = immediate

# Interceptor
UFOAgent.sources.UFODir.interceptors = attach-schema morphline

UFOAgent.sources.UFODir.interceptors.attach-schema.type = static
UFOAgent.sources.UFODir.interceptors.attach-schema.key = flume.avro.schema.url
UFOAgent.sources.UFODir.interceptors.attach-schema.value = file:/home/itam/schemas/ufos.avsc

UFOAgent.sources.UFODir.interceptors.morphline.type = org.apache.flume.sink.solr.morphline.MorphlineInterceptor$Builder
UFOAgent.sources.UFODir.interceptors.morphline.morphlineFile = /home/itam/ingestion/morphline.conf
UFOAgent.sources.UFODir.interceptors.morphline.morphlineId = convertUFOFileToAvro


# Sumidero
UFOAgent.sinks.UFOKiteDS.type = org.apache.flume.sink.kite.DatasetSink
UFOAgent.sinks.UFOKiteDS.channel = archivo
UFOAgent.sinks.UFOKiteDS.kite.repo.uri = dataset:hive
UFOAgent.sinks.UFOKiteDS.kite.dataset.name = ufos
UFOAgent.sinks.UFOKiteDS.kite.batchSize = 10
</code></pre>

</section>
<section id="slide-sec-11-5-21">
<h4 id="sec-11-5-21">Ejercicio III</h4>
<p>
Esta configuración consume más memoria de la que está asignada por
<i>default</i>, observa el parámetro <code>-Xmx100m</code>
</p>
<ul>
<li>Quítalo, para que veas el error que sale (te puede salir en tu
proyecto final)</li>

</ul>
<pre><code data-trim>
 flume-ng agent -n UFOAgent -Xmx100m --conf ingestion -f ingestion/spooldir_example.conf
</code></pre>

<p>
<i>Nota:</i> Esto está marcando un error que no aparecía en la versión
anterior de <code>flume</code>, ya la reporté <a href="https://github.com/kite-sdk/kite-examples/issues/27">aquí</a>.
</p>


</section>
<section id="slide-sec-11-5-22">
<h4 id="sec-11-5-22">Bibliografía</h4>
<ul>
<li>Documentación de Flume</li>
<li>Using Flume</li>

</ul>

</section>
<section id="slide-sec-11-6">
<h3 id="sec-11-6">Apache Sqoop</h3>
<div class="outline-text-3" id="text-11-6">
</div></section>
<section id="slide-sec-11-6-1">
<h4 id="sec-11-6-1">Apache Sqoop</h4>
<ul>
<li>Herramienta para importar eficientemente <i>data</i> desde <code>RDBMS</code> a Hadoop (<code>HDFS,
  Hive, Hbase</code>) y viceversa.</li>

<li>Soporta cualquier <code>RDBMS</code> que tenga conexión <code>JDBC</code> (<code>PostgreSQL, MySQL, Oracle, Teradata</code>, etc.).</li>

<li>Tiene soporte nativo para <code>MySQL</code> y <code>PostgreSQL</code>.</li>

<li><code>Sqoop</code> genera varios trabajos <code>MapReduce</code>, en los cuales, en cada
<code>Mapper</code> se conecta a la base de datos usando <code>JDBC</code>, selecciona un
pedazo de la tabla a copiar y escribe al <code>hdfs</code>.</li>

</ul>



</section>
<section id="slide-sec-11-6-2">
<h4 id="sec-11-6-2">Apache Sqoop</h4>
<ul>
<li>Puede importar una tabla o un esquema de la base de datos.</li>

<li>Puede ejecutar <i>queries</i> (y así filtrar la tabla, o hacer <code>joins</code>).</li>

<li>Soporta carga <i>incremental</i>.</li>

<li>Es importante notar que <code>sqoop</code> no maneja los datos en el formato
esperado, por lo que  será necesario tener procesamiento posterior
para transformar los datos.</li>

</ul>


</section>
<section id="slide-sec-11-6-3">
<h4 id="sec-11-6-3">Apache Sqoop</h4>

<div class="figure">
<p><img src="./imagenes/sqoop.png" alt="sqoop.png" />
</p>
</div>


</section>
<section id="slide-sec-11-6-4">
<h4 id="sec-11-6-4">Importar datos desde el RDBM a HDFS</h4>
<ul>
<li>Una tabla</li>

</ul>
<pre><code data-trim>
sqoop import --connect jdbc:postgresql://host/database \
--username usuario --password contraseña  \
--input-fields-terminated-by "\t"
--table <tabla> --target-dir <directorio>
</code></pre>

<p>
<i>NOTA:</i> El parámetro de <code>--input-fiedls-terminated-by</code> indica como
separar los campos en el <code>hdfs</code>.
</p>

<p>
<i>NOTA:</i> Si no quieres escribir la contraseña puedes usar la bandera <code>-P</code>.
</p>

</section>
<section id="slide-sec-11-6-5">
<h4 id="sec-11-6-5">Importar datos desde el RDBM a HDFS</h4>
<ul>
<li><i>Query</i>: Join</li>

</ul>
<pre><code data-trim>
sqoop import --connect jdbc:postgresql://host/database \
--username usuario --password contraseña  \
--query 'select tabla_1.*, tabla_2.* from tabla_1 join tabla_2 on (tabla_1.id = tabla_2.id) where $CONDITIONS' \
--split-by tabla_1.id --target-dir <directorio>
</code></pre>

<p>
<i>NOTA:</i> El parámetro de <code>--split-by</code> indica que columna se usará para
dividir los datos en varias tareas paralelas. (Por <i>default</i>, <code>sqoop</code>
usa cuatro <code>mappers</code> y usará la columna de <code>id</code> para dividir el trabajo).
</p>

<p>
<i>NOTA:</i> <code>$CONDITIONS</code> será reemplazado por <code>sqoop</code> para particionar
los <i>queries</i>.
</p>

</section>
<section id="slide-sec-11-6-6">
<h4 id="sec-11-6-6">Importar datos desde el RDBM a Hive</h4>
<ul>
<li>Tabla de <code>RDBMS</code> a Tabla de <code>Hive</code></li>

</ul>

<pre><code data-trim>
sqoop import --connect jdbc:postgresql://host/database \
--username usuario --password contraseña  \
--table <tabla> --hive-import  \
--hive-overwrite --split-by <columna>
</code></pre>

<p>
<i>NOTA:</i> Si no usas <code>--hive-overwrite</code> se agregarán los datos a la
tabla de <code>hive</code>.
</p>

<p>
Ahora puedes checarlo con <code>beeline</code>.
</p>

<pre><code data-trim>
show tables;
</code></pre>

<p>
<i>NOTA:</i> También soporta importar hacia <code>Hbase</code>, ver la documentación.
</p>

</section>
<section id="slide-sec-11-6-7">
<h4 id="sec-11-6-7">Exportar</h4>
<ul>
<li>Muchas veces deseamos mover datos a un <code>RDBMS</code> luego de procesarlos
o analizarlos, quizá para conectarlos con un sistema de <i>BI</i>, o una
aplicación web.</li>

<li>Se debe de crear la tabla primero donde depositaremos los datos</li>

</ul>
<p>
(se puede hacer con <code>sqoop</code> o con <code>psql</code>)
</p>

<pre><code data-trim>
sqoop eval --connect jdbc:postgresql://host/database \
--username usuario --password contraseña  \
--query "CREATE TABLE ..."
</code></pre>

<ul>
<li>Luego exportar</li>

</ul>

<pre><code data-trim>
sqoop eval --connect jdbc:postgresql://host/database \
--username usuario --password contraseña  \
--export-dir directorio_hdfs  --table tabla \
--columns lista_de_columnas \
--input-fields-terminated-by "\t" \
</code></pre>


</section>
<section id="slide-sec-11-6-8">
<h4 id="sec-11-6-8">Jobs</h4>
<ul>
<li>Algunas importaciones/exportaciones deberán de repetirse, en lugar
de escribirlas una y otra vez, es posible guardarlas como <code>jobs</code></li>

<li>Listar los <code>jobs</code></li>

</ul>
<pre><code data-trim>
sqoop job --list
</code></pre>

<ul>
<li>Crear un <code>job</code></li>

</ul>
<pre><code data-trim>
sqoop job --create nombre_job -- ...
</code></pre>

<ul>
<li>Ejecutarlo</li>

</ul>

<pre><code data-trim>
sqoop job --exec nombre_job
</code></pre>

</section>
<section id="slide-sec-11-6-9">
<h4 id="sec-11-6-9">Carga incremental</h4>
<ul>
<li>Si la tabla es más o menos pequeña y tarda muy poco en cargarse a
<code>Hadoop</code>, no vale la pena molestarse, con reescribir todo es más que
suficiente.</li>
<li>Si es necesario hacerlo incremental, hay dos opciones por <code>ID</code> y por
<code>Timestamp</code>.
<ul>
<li>Cuando cada renglón tiene su propio <code>id</code> y nuevos renglones tienen
<code>ids</code> más grandes que el último guardado en <code>hadoop</code>, se debe de
crear un <code>job</code></li>

</ul></li>

</ul>

<pre><code data-trim>
sqoop job --create <nombre_job> --import \
--connect jdbc:postgresql://host/database \
--username usuario --password contraseña  \
--table tabla --check-column columna-regularmente-id \
--incremental append
</code></pre>

<p>
y luego ejecutarlo cuando se necesite:
</p>

<pre><code data-trim>
sqoop job --exec <nombre_job>
</code></pre>

</section>
<section id="slide-sec-11-6-10">
<h4 id="sec-11-6-10">Ejercicio</h4>
<ul>
<li>Es posible <i>linkear</i> dos contenedores de <code>docker</code>. En este
ejercicio, veremos como.</li>

<li>Inicializa el <code>container</code> de <code>hadoop-pseudo</code></li>

</ul>

<pre><code data-trim>
docker start -ai hadoop-pseudo
</code></pre>

<ul>
<li>Instala (como el usuario <code>itam</code>) el cliente de <code>postgresql</code>, <code>psql</code></li>

</ul>

<pre><code data-trim>
sudo apt-get install postgresql-client
</code></pre>

<ul>
<li>Descarga, si no lo has hecho,  la imagen de <code>docker-postgresql</code></li>

</ul>

<pre><code data-trim>
docker pull nanounanue/docker-postgresql
</code></pre>

<ul>
<li>Crea un contenedor de <code>postgresql</code> y <i>linkealo</i> a <code>hadoop-pseudo</code></li>

</ul>

<pre><code data-trim>
docker run --link hadoop-pseudo:hadoop-pseudo --name pg_server nanounanue/docker-postgresql
</code></pre>


<ul>
<li>Averigua la dirección IP de cada contenedor</li>

</ul>

<pre><code data-trim>
# Visto en https://github.com/docker/docker/issues/8786
docker ps -q | xargs docker inspect --format '{{ .Id }} - {{ .Name }} - {{ .NetworkSettings.IPAddress }}'
</code></pre>


<ul>
<li>Desde el contenedor de <code>hadoop-pseudo</code> conéctate con <code>psql</code></li>

</ul>

<pre><code data-trim>
psql -h direccion_ip_de_pg-server -U itam
</code></pre>

<p>
<i>NOTA:</i> La base de datos es <code>itam</code> y la contraseña es <code>itam</code>.
</p>


</section>
<section id="slide-sec-11-6-11">
<h4 id="sec-11-6-11">Ejercicio</h4>
<ul>
<li>Ahora realiza lo siguiente:

<ul>
<li>¿Cuál serían los comandos para la copia la tabla de transacciones
de tarjeta bancaria (que hiciste en la sección de <code>postgresql</code>)
hacia Hadoop, colócalo en una tabla de <code>Hive</code> y en una carpeta
dentro de tu usuario.</li>

<li>¿Y para extraer las tablas de tus análisis recientes con <code>Impala</code>? ¿Cómo los
copias a una base de datos?</li>

</ul></li>

</ul>


</section>
<section id="slide-sec-11-6-12">
<h4 id="sec-11-6-12">Bibliografía</h4>
<ul>
<li>Documentación de Apache Sqoop</li>
<li>Apache Sqoop Cookbook</li>

</ul>


</section>
<section id="slide-hive-metastore">
<h3 id="hive-metastore"><a id="sec-11-7" name="sec-11-7"></a>Hive Metastore: HCatalog</h3>

</section>
<section id="slide-sec-11-7-1">
<h4 id="sec-11-7-1">¿Por qué?</h4>
<ul>
<li>Como vimos hay varias herramientas para analizar los datos:
<code>MapReduce</code>, <code>Hive</code>, <code>Pig</code>, <code>Impala</code> y los varios sabores de
<code>Spark</code>.</li>

<li>Luego de procesar los datos, se generan o archivos en el <i>file
system</i> local, o archivos en el <code>hdfs</code> o tablas en <code>hive</code>.</li>

<li>Todo suena muy bien, hasta que nos enfrentamos a que para seguir
procesándolas necesitamos:
<ul>
<li>Recordar y/o comunicar <i>dónde</i> está el <code>dataset</code>.</li>
<li>En qué <i>formato</i> está.</li>
<li>El <i>esquema</i>.</li>
<li>Entre otras cosas.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-11-7-2">
<h4 id="sec-11-7-2">¿Por qué?</h4>
<ul>
<li>Estos problemas no los tiene <code>hive</code>, ya que el <i>lee</i> el lugar,
formato y esquema del <code>hive metastore</code>.</li>

<li>Pero <code>MapReduce</code>, <code>Pig</code> y <code>Spark</code> deciden eso en los <i>Scripts</i>
<ul>
<li>Por ejemplo, <code>Pig</code> lo hace en la función <code>LOAD</code>.</li>

</ul></li>

<li>Si hay cambios, <code>Pig</code>, <code>MapReduce</code> y <code>Spark</code> sufrirán&#x2026;</li>

<li><code>HCatalog</code> trata de resolver estos problemas, ya que conoce la
locación, formato y esquema de nuestros <code>datasets</code>.</li>

</ul>

</section>
<section id="slide-sec-11-7-3">
<h4 id="sec-11-7-3">HCatalog</h4>
<ul>
<li>Está incorporado a <code>Hive</code> desde la versión <code>0.11</code>.</li>

<li>Es una capa administrativa de tablas y almacenamiento que permite
que diferentes herramientas de procesamiento de datos (<code>Pig</code>,
<code>MapReduce</code>, <code>Spark</code>) puedan leer y escribir más fácilmente del <code>HDFS</code>.</li>
<li>Contiene una abstracción que presenta una vista relacional de los
datos contenidos en el <code>HDFS</code>, asegurando que los usuarios no se
preocupen dónde o en que formato están almacenados los datos.</li>

</ul>

</section>
<section id="slide-sec-11-7-4">
<h4 id="sec-11-7-4">HCatalog</h4>
<ul>
<li>Provee una abstracción sobre los <code>datasets</code> guardados en el <code>hdfs</code>.</li>
<li>Permite realizar <i>data discovery</i>.</li>
<li>Provee notificaciones sobre disponibilidad de los datos.
<ul>
<li>Creación de base de datos, tabla o partición.</li>
<li>Destrucción de base de datos, tabla o partición.</li>

</ul></li>
<li><code>HCatalog</code> permite que proyectos diferentes a <code>hive</code> consulten
tablas de <code>Hive</code>.</li>

</ul>

</section>
<section id="slide-sec-11-7-5">
<h4 id="sec-11-7-5">HCatalog</h4>
<ul>
<li>Utiliza el <code>DDL</code> de <code>Hive</code>.</li>
<li>Provee interfaces de escritura y lectura para <code>Pig</code>, <code>MapReduce</code> y
<code>Hive</code>.</li>
<li>Usa la línea de comandos para manejar la definición de los datos y
metadatos.
<ul>
<li>Llamada imaginativamente <code>hcat</code>.</li>
<li>Por ejemplo, para ver el esquema de una tabla:
<pre><code data-trim>
hcat -e "desc tabla"
</code></pre></li>

</ul></li>

<li><code>HCatalog</code> presenta los datos de manera relacional.</li>
<li>Los datos son guardados en tablas y las tablas en bases de datos.</li>

<li><code>WebHCat</code> es la interfaz API <code>REST</code> de <code>HCatalog</code>.</li>

</ul>

</section>
<section id="slide-sec-11-7-6">
<h4 id="sec-11-7-6">HCatalog: Flujo de datos</h4>
<ul>
<li>Usuario 1 copia datos al HDFS</li>

</ul>
<pre><code data-trim>
hadoop distcp file:///data/books/pg2047.txt hdfs://data/20140430/books
hcat "alter table books add partition (ds='20140430') location 'hdfs://data/20140430/books'"
</code></pre>

<ul>
<li>Usuario 2 usa <code>Pig</code> para limpiar y preparar los datos.
<ul>
<li><code>HCatalog</code> mandará al <code>JMS</code> un mensaje de que la información está disponible.</li>
<li><i>NOTA:</i> Revisa la sección de <code>pig</code> para poder ver los <code>classpath</code> completos.</li>

</ul></li>

</ul>

<pre><code data-trim>
A = load 'books' using HCatLoader();
B = filter A by date = '20150430';
...
store Z into 'procesados' using HCatStorer("date=20140430");
</code></pre>

<ul>
<li>Usuario 3 realiza cierta analítica</li>

</ul>

<pre><code data-trim>
select col1, count(col3)
from procesados
where date  = '20150430'
group by col1;
</code></pre>


</section>
<section id="slide-sec-11-8">
<h3 id="sec-11-8">Luigi</h3>
<div class="outline-text-3" id="text-11-8">
</div></section>
<section id="slide-sec-11-8-1">
<h4 id="sec-11-8-1">Luigi</h4>
<ul>
<li>Orquestador de <b>Spotify</b></li>

<li>Escrito en <code>python</code>.
<ul>
<li>Cualquier cosa funcionará entonces: <code>scikit</code>, <code>pyspark</code>, etc.</li>

</ul></li>

<li>Integrado con <code>Hdfs</code>.</li>

<li>Soporta <i>out-of-the-box</i> <code>postgresql</code>
<ul>
<li>Interesante: ¿Dónde queda <code>sqoop</code>?</li>

</ul></li>

<li>Tiene algunas decisiones raras de diseño &#x2026;
<ul>
<li>Ver los <a href="http://luigi.readthedocs.org/en/latest/api/luigi.contrib.html">contribs</a> sobre todo los de <code>hive</code>, <code>sqla</code>, etc.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-11-8-2">
<h4 id="sec-11-8-2">Ejemplo</h4>
<ul>
<li>Lo mejor es ver un ejemplo sencillo</li>

</ul>


<ul>
<li>Este es el <code>workflow</code></li>

</ul>

<pre><code data-trim>
import luigi

class SimpleTask(luigi.Task):
    def output(self):
        return MockFile("SimpleTask", mirror_on_stderr=True)

    def run(self):
        _out = self.output().open('w')
        _out.write(u"Hello World!")
        _out.close()


class DecoratedTask(luigi.Task):
    def output(self):
        return MockFile("DecoratedTask", mirror_on_stderr=True)

    def requires(self):
        return SimpleTask()

    def run(self):
        _in = self.input().open("r")
        _out = self.output().open("w")
        for line in _in:
            outval = u"Decorated " + line + u"\n"
            _out.write(outval)

        _out.close()
        _in.close()


if __name__ == '__main__':
    from luigi.mock import MockFile
    luigi.run(main_task_cls=DecoratedTask)

</code></pre>

</section>
<section id="slide-sec-11-8-3">
<h4 id="sec-11-8-3">Ejemplo</h4>
<ul>
<li>Levanta el <code>scheduler</code>
<ul>
<li>(Ajusta acordemente)</li>

</ul></li>

</ul>

<pre><code data-trim>
luigid --port 8000 --background  --logdir=/home/itam/workflows/log
</code></pre>

<ul>
<li><i>Abre tu navegador </i></li>

<li>Ejecuta el =workflow</li>

</ul>

<pre><code data-trim>
python linked_luigi.py --scheduler-port 8000
</code></pre>


</section>
<section id="slide-sec-11-8-4">
<h4 id="sec-11-8-4">Luigi</h4>
<ul>
<li>Y ahora ver las siguientes presentaciones:</li>

<li><a href="https://vimeo.com/63435580">Luigi - Batch Data Processing in Python</a>
<ul>
<li>El <a href="http://cdn.oreillystatic.com/en/assets/1/event/95/Luigi%20-%20Batch%20Data%20Processing%20at%20Large%20Scale%20Presentation.pdf">pdf</a></li>

</ul></li>

<li>Los <a href="http://luigi-td.readthedocs.org/en/latest/gettingstarted.html">docs</a></li>

</ul>




</section>
</section>
<section>
<section id="slide-sec-12">
<h2 id="sec-12">Modo distribuido</h2>
<div class="outline-text-2" id="text-12">
</div></section>
<section id="slide-sec-12-1">
<h3 id="sec-12-1">Modo distribuido</h3>
<ul>
<li>Sigue las instrucciones de <a href="http://www.cloudera.com/content/cloudera/en/documentation/cloudera-manager/v5-0-0/Cloudera-Manager-Installation-Guide/cm5ig_install_on_ec2.html">aquí</a> para levantar un cluster.</li>

</ul>

<p>
<i>NOTA:</i> Requerirás una cuenta en <code>AWS</code>.
</p>


</section>
</section>
<section>
<section id="slide-sec-13" data-state="alert">
<h2 id="sec-13">Proyecto Final</h2>

</section>
<section id="slide-sec-13-1">
<h3 id="sec-13-1">Proyecto Final</h3>
<ul>
<li>Implementar el flujo propuesto por el equipo.</li>

<li>Utilizar al menos:
<ul>
<li><code>Sqoop</code></li>
<li><code>Flume</code></li>
<li><code>Luigi</code></li>
<li><code>Hive</code> o <code>Impala</code> para hacer un ejemplo de analítica
<ul>
<li>Probablemente fuera del <b>ETL</b>.</li>

</ul></li>
<li><code>Pig</code> o <code>PySpark</code> para hacer al menos un paso del <b>ETL</b></li>
<li>El sistema de carpetas.</li>

</ul></li>

<li>Para la base de datos de <code>gdelt</code>.</li>

<li>Reporte de trabajo escrito, más presentación ejecutiva.
<ul>
<li><i>NOTA:</i> Recuerda que son para dos fines diferentes.</li>
<li>Describa todo lo que querían hacer, junto con el caso de
negocio.</li>
<li>Y luego lo implementado.</li>

</ul></li>

<li><b>Fecha de entrega:</b> Viernes 5 de junio, a las 23:59.</li>

</ul>

</section>
</section>
<section>
<section id="slide-sec-14" data-background="#e95d3c">
<h2 id="sec-14">Disclaimer</h2>
<ul>
<li>Algunas imágenes se tomaron de los libros <i>Professional Hadoop Solutions</i> de <b>Wrox</b> y de la página de <a href="http://hortonworks.com/hadoop/yarn/"><b>Hortonworks</b></a>. Las otras son mías.</li>
<li>Debería de ser claro cuales son cuales. <code>(^_^)</code></li>
<li>Para otras imágenes, en la lámina se indica de dónde fueron tomadas.</li>

</ul>
</section>
</section>
</div>
</div>
<p> Creado por Adolfo De Unánue Tiscareño. </p>

<script src="http://cdn.jsdelivr.net/reveal.js/2.5.0/lib/js/head.min.js"></script>
<script src="http://cdn.jsdelivr.net/reveal.js/2.5.0/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: true,
center: true,
slideNumber: true,
rollingLinks: false,
keyboard: true,
overview: true,
width: 1200,
height: 800,
margin: 0.10,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'fade', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: 'default',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'http://cdn.jsdelivr.net/reveal.js/2.5.0/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: 'http://cdn.jsdelivr.net/reveal.js/2.5.0/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'http://cdn.jsdelivr.net/reveal.js/2.5.0/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'http://cdn.jsdelivr.net/reveal.js/2.5.0/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
]
});
</script>
</body>
</html>
