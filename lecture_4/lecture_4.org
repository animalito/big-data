#+Title: Lecture 4
#+Author: Adolfo De Unánue
#+Email: adolfo.deunanue@itam.mx

#+OPTIONS: toc:nil reveal_mathjax:t
#+REVEAL_TRANS: fade
#+REVEAL_THEME: night
#+REVEAL_MARGIN: 0.1
#+REVEAL_MIN_SCALE: 0.5
#+REVEAL_MAX_SCALE: 2.5
#+REVEAL_HEAD_PREAMBLE: <meta name="description" content="Big data - Lecture 6.">
#+REVEAL_POSTAMBLE: <p> Creado por Adolfo De Unánue Tiscareño. </p>
#+REVEAL_PLUGINS: (highlight markdown notes)
#+REVEAL_ROOT: http://cdn.jsdelivr.net/reveal.js/2.5.0/
#+OPTIONS: tex:t
#+OPTIONS: reveal_center:t reveal_progress:t reveal_history:nil reveal_control:t
#+OPTIONS: reveal_mathjax:t reveal_rolling_links:t reveal_keyboard:t reveal_overview:t num:nil
#+OPTIONS: reveal_width:1200 reveal_height:800


# tema: default, beige, sky, night, serif, simple, moon
# trans: default, cube, page, concave, zoom, linear, fade, none

* Docker

** Obtener la imagen

#+begin_example
docker pull nanounanue/docker-hadoop
#+end_example

** Ejecutar un contenedor

#+begin_example
docker run -ti --rm \
  -e "AUTHORIZED_SSH_PUBLIC_KEY=$(cat ~/.ssh/id_rsa.pub)" \
  -v /home/nano/tmp/docker-hadoop-data/:/home/hduser/hdfs-data/ \
  -v /home/nano/tmp/docker-hadoop-logs/:/srv/hadoop/logs/ \
  -p 2122:2122 -p 2181:2181 -p 39534:39534 -p 9000:9000 \
  -p 50070:50070 -p 50010:50010 -p 50020:50020 -p 50075:50075 \
  -p 50090:50090 -p 8030:8030 -p 8031:8031 -p 8032:8032 \
  -p 8033:8033 -p 8088:8088 -p 8040:8040 -p 8042:8042 \
  -p 13562:13562 -p 47784:47784 -p 10020:10020 -p 19888:19888 \
nanounanue/docker-hadoop /bin/bash
#+end_example

** Contenedor con Hadoop

#+begin_example
docker run -ti --name hadoop_pseudodistribuido \
  -e "AUTHORIZED_SSH_PUBLIC_KEY=$(cat ~/.ssh/id_rsa.pub)" \
  -v /home/nano/tmp/docker-hadoop-data/:/home/hduser/hdfs-data/ \
  -v /home/nano/tmp/docker-hadoop-logs/:/srv/hadoop/logs/ \
  -p 2122:2122 -p 2181:2181 -p 39534:39534 -p 9000:9000 \
  -p 50070:50070 -p 50010:50010 -p 50020:50020 -p 50075:50075 \
  -p 50090:50090 -p 8030:8030 -p 8031:8031 -p 8032:8032 \
  -p 8033:8033 -p 8088:8088 -p 8040:8040 -p 8042:8042 \
  -p 13562:13562 -p 47784:47784 -p 10020:10020 -p 19888:19888 \
nanounanue/docker-hadoop
#+end_example

** Conectarse al contenedor
#+begin_example
ssh hduser@localhost -p 2122
#+end_example

** Navegador Web


- [[http://127.0.0.1:50090][Consola de Yarn]]

- [[http://127.0.0.1:50070][Consola de HDFS]]


* Apache Hadoop

** ¿Por qué?

- Aunque la capacidad de los discos ha aumentado considerablemente, la velocidad de los mismos no lo ha hecho igual.
  - Los discos actuales de =1 Tb=, tardan en leerse completos a =100 Mb/s= cerca de dos a tres horas.
  - Podemos /paralelizar/ las fuentes en varios discos.
    - Para leerla simultáneamente
  - Con varios discos, la **probabilidad de falla** aumenta.
- Otro problema es la distribución ¿Cómo combinas varios =file systems=?

** ¿Qué es?

- Sistema confiable (/realiable/) de almacenamiento compartido y de análisis.
  - *Almacenamiento*: HDFS
  - *Análisis*: MapReduce

** ¿Cómo?

- =MapReduce= es un sistema de procesamiento /batch/
  - Permite correr /queries/ contra **toda** tu base de datos
  - Pero el resultado puede tardar minutos, horas, etc...
  - No permite tener a un humano sentado ahí para retroalimentar.

** ¿Cómo?

- Ahora, gracias a =YARN= (ver más adelante) tenemos diferentes tipos de procesamiento:
  - /SQL Interactivo/: =Impala=, =Hive=, =Tez=.
  - /Iterativos/: =Spark=.
  - /Procesamiento de flujos/: =Storm=, =Spark Streaming=.
  - /Búsquedas/: =Solr=.

** ¿Por qué no otros sistemas?

- ¿Por qué no usar un =PostgreSQL= con muchos discos, muy /pimpeado/?
  - El problema viene del tiempo que toma mover la cabeza del disco a otro lugar del disco para leer o escribir datos (/seek time/).
    - ¿Cuál es la /latencia/ de la operación?

- ¿Por qué no /Grid/?
  - Por ejemplo, cosas  de =HPC= que usan =MPI=.
    - Son intensivos en **CPU**.
  - Pero si hay que mover cientos de gigabytes, la transferencia de datos se vuelve un problema.
    - Basicamente, en que =Hadoop= opera con /data locality/.

** Componentes de Apache Hadoop

- *MapReduce* Modelo de procesamiento /batch/ de datos distribuido y paralelo.
- *HDFS* Sistema de archivos (/file system/) distribuido.
- *Pig* Capa de abstracción encima de =MapReduce=. Utiliza /Pig Latin/ un lenguaje de flujo de datos
  - Como =dplyr=
- *Hive* (Hadoop InteractiVE) Es un lenguaje parecido al =SQL=: =HQL=, para ejecutar /queries/ sobre el =HDFS=.
- *HBase* Base de datos distribuida orientada a columnas.
  - Depende de =Zookeeper=.


** Componentes de Apache Hadoop

- *Zookeeper* Sistema distribuido de coordinación.
- *Sqoop* Herramienta para mover datos entre =RDBM= y =HDFS=.
- *Flume* Servicio para recolectar, agregar y mover grandes cantidades de datos entre máquinas individuales y el =HDFS=.
- *Oozie* Sistema de /workflow/, se usa para coordinar varios /jobs/ de *MapReduce*.
- *Mahout* Biblioteca de /Machine Learning/.
  - Ver la carpeta =docs=.
- *Ambari* Simplifica el aprovisionamiento, gestión y /monitoreo/ de un /cluster/ de Hadoop.
- *Avro* Formato de serialización y de persistencia de datos.
- Entre otros...



* HDFS : Hadoop File System


** HDFS

- Sistema de almacenamiento distribuido.
  - /Namenode/ =->= Master
  - /Datanode/ =->=  Slaves

** Ventajas

- Archivos muy grandes
- /write once, read many times/.
- Hardware _normal_

** Desventajas

- Acceso a los datos de baja latencia.
- Muchos archivos pequeños.
- Muchas escrituras, modificaciones

** Tamaño del bloque

- Cada /file system/ define un tamaño de bloque, el cual es la cantidad mínima de datos que puede escribir o leer.
  - Típicamente son de =kb=.
- En =HDFS=, el bloque es de =128 Mb= por /default/.
  - Es el concepto fundamental, no el archivo.


** /Namenode/
  - Gestiona el /filesystem/
    - Mantiene el árbol del /filesystem/.
    - Mantiene los =metadatos= de todos los archivos y carpetas del árbol.
    - Esta información se guarda en disco en dos archivos:
      - =namespace image=
      - =edit log=
  - Indica a los /datanodes/ realizar tareas de bajo nivel de =I/O=.
  - /Book Keeper/
    - División de archivos en bloques (¿Cómo?)
    - En qué /datanode/ (¿Quién?)
    - Monitorea.
  - Uso intensivo de =RAM= y de =I/O=.
  - Si se /cae/ el =HDFS= no puede ser usado
    - Hasta la versión =1.x= el /single point of failure/, en Hadoop 2 se incorporó la característica de /HIgh Availability/.
    - Su caída puede causar la pérdida total de los datos.

** /Namenode/

- Hadoop proveé de dos formas de aliviar esta situación:
  - Respaldos: Se puede configurar al /namenode/ para que escriba su estado a varios /filesystems/.
  - /Secondary Namenode/

** /Namenode/

[[file:./imagenes/Selección_004.png]]


** /Datanode/
  - Lee y escribe los =HDFS= /blocks/ y los convierte en archivos del *FS* local.
  - Se comunica con otros /datanodes/ para la replicación de los datos.
  - Pueden realizar /caching/ de bloques.

** /Datanode/

[[file:./imagenes/Selección_005.png]]

** /Secondary Name Node/
  - Como el /namenode/ sólo hay uno por /cluster/.
  - No es un /namenode/.
  - Evita que el =edit log= crezca mucho.
  - No recibe ni guarda cambios en tiempo real del =HDFS=.
    - Va atrás del /namenode/.
  - Sólo toma /snapshots/ de la metadata.


** Línea de comandos

- Hay muchas maneras de conectarse y usar el =HDFS=. La línea de comandos es una de ellas.
  - Y espero que ya sepan que es de las más útiles y eficientes.

- Ayuda: =hadoop fs -help=

** Línea de comandos

#+begin_example
hadoop fs -cmd <args>
hadoop fs -ls
hadoop fs -mkdir
hadoop fs -copyFromLocal
hadoop fs -copyToLocal
hadoop fs -put archivo archivo_hdfs
hadoop fs -get archivo_hdfs
hadoop fs -cat archivo_hdfs
hadoop fs -cat archivo_hdfs head
hadoop fs -tail archivo_hdfs
hadoop fs -rm archivo_hdfs
#+end_example

** Copia en paralelo

#+begin_example
hadoop distcp
#+end_example

** Modo Pseudodistribuido

- Crea una imagen sin Hadoop corriendo, vamos a explicar que significa /pseudodistribuido/.

** Ejercicio
- Crea una imagen con Hadoop corriendo.
- Conéctate con el usuario =hduser=.
- Verifique que =alias= tiene definido el usuario =hduser=.
  - Usa el comando =alias=.
- Crear una carpeta =ufo= en el =HDFS= y suba  los archivos de =ufo= a la carpeta recién creada.
  - Descomprime los archivos antes de subirlos
  - Crea un /script/ para esta tarea, llámalo =ufo_hdfs.sh=.
- Crear una carpeta =gdelt= en el =HDFS= y suba los archivos de =gdelt= a esta carpeta.
  - Descomprime los archivos antes de subirlos
  - Crea un /script/ para esta tarea, llámalo =gdelt_hdfs.sh=.
- Muestra las carpetas en la línea de comandos.
  - Modifica los usuarios y permisos del HDFS ¿Cómo crees que se haga?
- Muestra las carpetas en la vista web.

* MapReduce

** MapReduce en Hadoop
- Principal /framework/ de ejecución de =Apache Hadoop=.
- Inspirado en las operaciones *MAP* y *REDUCE* de los lenguajes funcionales.
- Modelo de programación para proceso de datos distribuido  y paralelo.
- Divide las tareas (/jobs/) en fases de /mapeo/ y fases de /reducción/.
- Los desarrolladores crean tareas /MapReduce/ para Hadoop usando datos guardados en el =HDFS=.

** MapReduce: Ventajas

   - /Fault-tolerant/.
   - Esconde los detalles de implementación a los programadores.
   - Escala con el tamaño de los datos.


** MapReduce

- Dos fases de procesamiento:
  - /key-value/ como Input y Output
  - El programador especifica:
    - Tipos de /key-value/
    - Funciones: =MAP= y =REDUCE=.


** Una pequeña regresión...

** map-reduce: Matemáticamente

#+BEGIN_EXAMPLE
map: (k1, v1) -> list(k2, v2)
#+END_EXAMPLE

- =map= Mapea (aplica una función /f/) un conjunto de entrada de pares /key-value/ a otro conjunto intermedio de /key-values/


** map-reduce: Matemáticamente

#+BEGIN_EXAMPLE
reduce: (k2, list(v2)) -> list(k3, v3)
#+END_EXAMPLE

- =reduce=  Aplica una función /g/ a todos los valores (/values/) asociados a una llave (/key/) y acumula el resultado. Emite pares de /key-values/.

** Python =map=

#+begin_src python :results output :export both
# Equivalente en for-loop

items = [1,2,3,4,5]
cuadrados = []
for x in items:
    cuadrados.append(x**2)

print cuadrados
#+end_src

#+RESULTS:
: [1, 4, 9, 16, 25]


#+begin_src python :results output :export both
# Usando la funcion map(function, sequence)

items = [1,2,3,4,5]

print list(map((lambda x: x**2), items))
#+end_src

#+RESULTS:
: [1, 4, 9, 16, 25]


** Python =reduce=

#+begin_src python :results output :export both
# Equivalente en for-loop
L = [1,2,3,4]
result = L[0]
for x in L[1:]:
    result = result*x

print result

#+end_src

#+RESULTS:
: 24

#+begin_src python :results output :export both
# Usando la funcion reduce(funcion, secuencia)
print reduce((lambda x,y: x*y), [1,2,3,4])
#+end_src

#+RESULTS:
: 24

** Python =map= y =reduce=

#+begin_src python :results output :export both
a = range(1, 4)
b = range(4, 9)
c = range(9, 15)
print "a ->  %s, b -> %s , c -> %s" % (a, b, c)

L1 = map(lambda x:len(x), [a,b,c])
print "L1 -> %s" % L1

L2 = reduce(lambda x, y: x+y, L1)
print "L2 -> %s" % L2
#+end_src

#+RESULTS:
: a ->  [1, 2, 3], b -> [4, 5, 6, 7, 8] , c -> [9, 10, 11, 12, 13, 14]
: L1 -> [3, 5, 6]
: L2 -> 14



** MapReduce y map-reduce

- Básicamente es lo mismo, pero...
- =map=, =reduce= (entre otras) son parte de lenguajes funcionales.
- =MapReduce= es la aplicación de esta idea aplicada a problemas /vergonzosamente/ /paralelos/.
  - Ver la carpeta =docs= para el artículo de *Google* sobre =MapReduce=.


** GNU Parallel

#+begin_src sh
find ./data/books -type f | parallel -j0 egrep -i  '\[\[:digit:\]\]' {} | awk '{s+=$1} END {print s}'
#+end_src

#+RESULTS:
: 1480715


- *¿Puedes identificar las partes =map= y =reduce=?*
- Esto ya es un =MapReduce=.


** MapReduce en Hadoop

- A nivel programático:
  - /Data/ de entrada
  - Programa MapReduce
  - Configuración
  - Subtareas: =map= y =reduce=


** MapReduce: /Mapper/

- Hadoop divide la entrade de datos al /job/ MapReduce en pedazos de tamaño fijo llamados /input splits/.
- Hadoop crea una tarea =map= para cada /input split/.
- =map= escribe al /file system/ local.
  - Si el =reducer= tiene éxito se borra la salida del /mapper/.

** Map only

[[file:./imagenes/map_only.png]]


** MapReduce: /Reducer/

- La entrada es la salida de (posiblemente) todos los /mappers/.
- Estas se transmiten vía red al nodo donde corre el /reducer/.
- La salida se guarda en el =HDFS=.

** Map, One reduce

[[file:./imagenes/map_one_reduce.png]]

** MapReduce

[[file:./imagenes/map_reduce.png]]


** MapReduce: /Combiner/

- Es una medida de optimización.
- Es para ahorrar ancho de banda.
- Una especie de /reducer/ local.
- No es parte (estrictamente) del MapReduce
  - Por eso no lo había mencionado.

** MapReduce: Componentes

/Job Tracker/

- Uno por /cluster/.
- Contacto entre Hadoop y la aplicación cliente.
- Determina el plan de ejecución:
  -  ¿Qué archivos?
  -  ¿Quién hace las tareas?
  -  Monitoriza.
- Si una tarea falla, relanza la tarea.

** MapReduce: Componentes

/Task tracker/

- Esclavo
- Ejecuta la tarea en cada nodo
- Uno por nodo
  - Pero se pueden crear varios /Java Virtual Machines/ (JVM) para tener varios /Mappers/ o /Reducers/ en paralelo.
- =PING= al /JobTracker/, si no, se supone que el nodo a muerto.


** Arquitectura: MapReduce

[[file:./imagenes/arquitectura_MR.png]]



* Apache Haoop 2.x: YARN


** YARN

- La infraestructura de Hadoop =0.x= y =1.x= era monolítica, por eso fue rediseñada.
- =YARN=: /Yet Another Resource Negotiator/.
- La gestión de recursos es extraída de los paquetes de =MapReduce= para que puedan ser utilizadas por otros componentes.
- Aportaciones
  - Escalabilidad.
  - Compatibilidad con =MapReduce=.
  - Mejoras en la gestión del /cluster/.
  - Soporte para otros modelos de programación (además de =MapReduce=).
    - /Graph processing/
    - /Message Passing Interface/ (*MPI*).
    - Soporte para procesamiento /real-time/ o /near real-time/.
      - =MapReduce= es /batch-oriented/.
  - Agilidad.

** YARN

- Se dividieron las dos responsabilidades del /JobTracker/:
  - Gestión de recursos (/Resource Management/)
  - Asignación y vigilancia de trabajos (/Job scheduling-monitoring/)

- La idea es tener un /ResourceManager/ global y un /NodeManager/ por
  nodo esclavo, los cuales forman un sistema para la administración de
  aplicaciones distribuidas.

- El /ResourceManager/ tiene dos componentes principales:
  - /Scheduler/: Asigna los recursos para las aplicaciones (/pluggeable/).
  - /Application Manager/: Responsable de aceptar las solicitudes de
    trabajos, negociando al principio para ejecutar el /Application
    Master/ específico y provee un servicio de reinicio, por si el
    /Application Master/ falla.

- En cada nodo:

   - El /Application Master/: Negocia sus recursos con el /Scheduler/,
  monitorea sus avances y reporta su estatus.

   - El /NodeManager/ es el responsable de los contenedores,
     monitorear el uso de recursos y reportar todo al
     /ResourceManager/.

** Arquitectura MapReduce Hadoop 1.x

[[file:./imagenes/MRArch.png]]

** Arquitectura Hadoop 2.x

[[file:./imagenes/Selección_003.png]]


** Cambios 1.x -> 2.x

[[file:./imagenes/yarn.png]]


** Multiparadigma en Hadoop 2.x

 [[file:./imagenes/YARN.png]]


* Hello World!: Word count

** Word count

- Es el ejemplo /Hola Mundo/ de Apache Hadoop.
- No sólo eso, es el ejemplo que se utiliza en el trabajo seminal
  - *MapReduce: Simplified Data Processing on Large Clusters* /(2006)/.
  - En la carpeta =docs= como ya había dicho.
- Solamente 1 =Map= y 1 =Reduce=.


** Word count

- *mapper*
  - =k1= -> nombre de archivo
  - =v1= -> texto del archivo
  - =k2= -> palabra
  - =v2= -> "1"

- *reducer*
  - =k2= -> palabra
  - list(v2) -> (1,1,1,1,1,1,..., 1)

  Suma los "1" y produce una lista de

  - k3 -> palabra
  - v3 -> suma

** Word count

[[file:./imagenes/word_count.png]]

** Pseudocódigo

#+begin_example
map (String key, String value)
   for each word w in value
      Emit(w, 1)

reduce (String key, Iterator values)
   int wordcount = 0
   for each v in values
      wordcount += v
      Emit(key, wordcount)

#+end_example

** Mockup

- Ver los archivos =word_count.py= y =mapreduce.py= en la carpeta =mock=.

#+begin_example
chmod +x word_count.py
python word_count.py
#+end_example

- Este es un ejemplo de mentiritas, no usa Apache Hadoop.

** =Hadoop Streaming=

- Es un paquete (=jar file=) de Java incluido en Hadoop.
- Cualquier ejecutable como /mapper/ o /reducer/.
- Lee del /stdin/, escribe al /stout/.
- Protocolo =k\t v\n=
  - ¡Súper importante!
  - No recibes =k list(v,v,v...)=

- En pseudocódigo

#+begin_example
cat input_file | mapper | sort | reducer > output_file
#+end_example

** Hadoop Streaming: Ejecución

#+begin_example
hadoop jar
$HADOOP_INSTALL/contrib/streaming/*streaming*.jar
-input input_dir
-output output_dir
-mapper my_mapper
-reducer my_reducer
-file my_mapper
-file my_reducer
-jobconf mapred.map.tasks=2
-jobconf mapred.reduce.tasks=2
#+end_example

** Ejercicios
- Obtener una lista de formas, usando =ufo= con =Hadoop Streming= y =bash=.
  - /Tip/: Recuerden el comando =cut= y =uniq=.

- Obtener una lista de ciudades, estado y su número de avistamientos, usando =ufo= con =Hadoop Streming= y =bash=.

** Hadoop Streaming: Python

- Flujo recomendado:
- Probamos el /mapper/

#+begin_src sh
echo "foo foo ernesto david angelica ernesto" | src/wordcount/python/mapper.py
#+end_src

#+RESULTS:
| foo      | 1 |
| foo      | 1 |
| ernesto  | 1 |
| david    | 1 |
| angelica | 1 |
| ernesto  | 1 |

- Ahora probamos /mapper/ y /reducer/
  - Nota el =sort=

#+begin_src sh
echo "foo foo ernesto david angelica ernesto" | src/wordcount/python/mapper.py | sort -k1.1  | src/wordcount/python/reducer.py
#+end_src

#+RESULTS:
| angelica | 1 |
| david    | 1 |
| ernesto  | 2 |
| foo      | 2 |

- Ahora probamos con un archivo

#+begin_example sh
cat data/books/pg5000.txt | src/wordcount/python/mapper.py
#+end_example


** Hadoop Streaming: Python

#+begin_example
$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-*streaming*.jar
-file mapper.py -mapper mapper.py
-file reducer.py -reducer reducer.py
-input books -output books-py-output
#+end_example




* Pig

** Pig

- Proyecto de Apache
- Abstracción encima de Hadoop
  - /Pig Latin/ compila a =MapReduce=
  - En cierta forma /Pig Latin/ es para analistas, /data scientist/ y estadísticos.
  - =MapReduce=  es para programadores (aunque los /data scientist/ deberían de poder hacerlo también)

** Pig

- Pig es un /data flow programming language/
- Es decir,
  - Ejecuta paso a paso
  - Cada paso es una transformación de datos
- En cambio =SQL= es un conjunto de /constraints/ que en conjunto definen el resultado buscado.

** Pig

- ¿Qué cosas puede hacer?
  - =joins=
  - =sorts=
  - =filters=
  - =group by=
  - /User defined functions/ =UDF='s

** Pig

- ¿Qué cosas *puedo* hacer?

  - =ETLs=
    - Limpiar.
    - /Joins/ gigantes.

  - Búsqueda en /Raw/.

** Pig

- Componentes
  - /Pig Latin/
  - =Grunt=
    - Local
    - MapReduce
  - =Pig compiler=

** Pig

- Es posible ejecutar también /scripts/ de /Pig Latin/ (terminación =.pig=) sin entrar a =grunt=.

#+begin_example
pig script_file.pig

# Si quieren pasar parámetros
pig -p var=bla/bla var2=bla/bla/bla script_file.pig
#+end_example

- Y usarse desde programas en =Java= con la clase =PigServer=.
  - Como una especie de =JDBC=, pero para /Pig Latin/.


** Pig: /Building blocks/

Fields
#+begin_example
'Adolfo'
#+end_example

Tuplas
#+begin_example
('Adolfo', 3, 8.17, 23)
#+end_example

/Bags/
#+begin_example
{('Adolfo', 3, 8.17, 23), ('Paty', 3.14, 9, 'A')}
#+end_example

** Ejercicio

1. Crear una carpeta =rita= en el =HDFS=.
2. Agregar los siguientes archivos:
   - =airports.csv=
   - =plane_data.csv=
   - =carriers.csv=
3. Ejecutar =grunt=.

#+begin_example pig
# Pig latin puede ejecutar comandos del hdfs
cat rita/airports
# Especificando el separador (,) y el esquema (no es necesario)
airports = load 'rita/airports' using PigStorage(',') as (iata:chararray, ..., latitude:float, ...);
# Hasta este momento se ejecuta todo...
dump airports;
# El comando store guarda al HDFS y también ejecuta todo.
#+end_example

** Ejercicio

#+begin_example pig
a_imprimir = limit airports 5;
por_estado = group airports by state;
describe por_estado;
explain por_estado;
illustrate por_estado;
# itera sobre cada elemento del bag
conteo = foreach por_estado generate group count_star(airports);
ordenados = order conteo by $1 desc;
top_five = limit ordenado 5;
unicos = distinct conteos;
muestreo = sample por_estado 0.1;
filtrados = filter conteos by substring(group, 0, 2) == 'W';
mayores = filter conteos by $1 > 50;
#+end_example

** Ejercicio: Trucos del =foreach=
#+begin_example pig
# Proyectar
foreach airports generate iata, airport, country;

# Expresiones posicionales
# $1 -> iata
# $3 -> city
# $5 -> country

# Rangos
# ..country, iata..country, latitude..

# Tokenizar
tokens = foreach lineas generate tokenize(linea);
# Cada fila obtenida es un bag de palabras.
#+end_example

** Pig: JOINS

1. Cargamos fuente 1
2. Cargamos fuente 2
3. Unimos las fuentes (/bags/) mediante una llave
4. Súper simple

Pig soporta /inner joins/ (valor por omisión), /left outer joins/ (y
/right/ también) y /full outer/ joins.


#+begin_example pig
fuentes_unidas = join fuente1 by (keys) [left|right|full outer] fuente2 by (keys);
#+end_example

Además =Pig= soporta =cogroup= además de los =joins= (el =cogroup=
preserva la estructura de las fuentes y crea tuplas por cada llave)

#+begin_example pig
fuentes_unidas = cogroup fuente1 by (keys) fuente2 by (keys);
#+end_example


** Pig: Ejemplo de JOINs y COGROUPs

#+begin_example

# Fuentes de datos

mascotas: (dueño, mascotas)
----------------------
(Adolfo, tortuga)
(Adolfo, pez)
(Adolfo, gato)
(Paty, perro)
(Paty, gato)

amigos: (amigo1, amigo2)
----------------------
(Diana, Adolfo)
(Gabriel, Adolfo)
(Shanti, Paty)


COGROUP mascotas by dueño, amigos por amigo2;
---------------------------------------------
(Adolfo, {(Adolfo, tortuga), (Adolfo, pez), (Adolfo, gato)}, {(Diana, Adolfo), (Gabriel, Adolfo)})
(Paty, {(Paty, perro), (Paty, gato)}, {(Shanti, Paty)})

JOIN mascotas by dueño, amigos por amigo2;
-------------------------------------------
(Adolfo, tortuga, Diana)
(Adolfo, tortuga, Gabriel)
(Adolfo, pez, Diana)
(Adolfo, pez, Gabriel)
(Adolfo, gato, Diana)
(Adolfo, gato, Gabriel)
(Paty, perro, Shanti)
(Paty, gato, Shanti)
#+end_example

** Aclaraciones sobre GROUP y FLATTEN

- =FLATTEN= elimina un nivel anidamiento

#+begin_example pig
# Datos:
# (Adolfo, (tortuga, pez, gato))
# (Paty, (perro, gato))
# FLATTEN eliminaría los bags internos
(Adolfo, tortuga)
(Adolfo, pez)
(Adolfo, gato)
(Paty, perro)
(Paty, gato)
#+end_example

- =GROUP .. BY= organiza los /bags/ en /bags/
#+begin_example pig
# Siguiendo con los datos anteriores de mascotas
GROUP mascotas BY dueño;

# ( Adolfo, {(Adolfo, tortuga), (Adolfo, pez), (Adolfo, gato)} )
# ( Paty, {(Paty, perro), (Paty, gato)} )

#+end_example

- En cierto sentido =FLATTEN= y =GROUP .. BY= son operaciones inversas
  entre sí.

** Tarea

Crear un =wordcount= para los archivos en =data= usando =Pig=


* Hive

** Hive

- /Datawarehouse/.
- =HQL= es casi idéntico a =SQL=.
- Proyecto de Apache.
- Estructura a diversos formatos.
- /Queries/.
- Acceso al =HDFS= y =HBASE=.
- /Queries/ en tiempo real.
- Facilidad de uso.

** Arquitectura de Apache Hive

[[file:./imagenes/hive-remote.jpeg]]

** Ejercicio: Crear RITA en Hive

#+begin_example sql
CREATE EXTERNAL TABLE carriers(
code STRING,
description STRING
)
COMMENT 'Códigos de carriers'
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' STORED AS TEXTFILE;

CREATE EXTERNAL TABLE airports(
iata STRING,
airport STRING,
city STRING,
state STRING,
country STRING,
latitude FLOAT,
longitude FLOAT
)
COMMENT 'Códigos y localización de aeropuertos'
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' STORED AS TEXTFILE;

CREATE EXTERNAL TABLE planes_data(
tailnum STRING,
type STRING,
manufacturer STRING,
issue_date STRING,
model STRING,
status STRING,
aircraft_type STRING,
engine_type STRING,
year STRING
)
COMMENT 'Datos de algunos aviones mencionados en RITA'
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' STORED AS TEXTFILE;
#+end_example


** Ejercicio: Crear RITA en Hive

#+begin_example sql
CREATE EXTERNAL TABLE rita(
Year STRING,
Month STRING,
DayofMonth STRING,
DayOfWeek STRING,
DepTime STRING,
CRSDepTime STRING,
ArrTime STRING,
CRSArrTime STRING,
UniqueCarrier STRING,
FlightNum STRING,
TailNum STRING,
ActualElapsedTime INT,
CRSElapsedTime INT,
AirTime INT,
ArrDelay INT,
DepDelay INT,
Origin STRING,
Dest STRING,
Distance FLOAT,
TaxiIn INT,
TaxiOut INT,
Cancelled INT,
CancellationCode STRING,
Diverted INT,
CarrierDelay INT,
WeatherDelay INT,
NASDelay INT,
SecurityDelay INT,
LateAircraftDelay INT
)
COMMENT 'Base de datos conteniendo los vuelos de 1987 a 2008'
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' STORED AS TEXTFILE LOCATION '/user/hive/rita';
#+end_example

** Ejercicio: RITA y HIVE

#+begin_example sql
-- ¿Se crearon bien las tablas?
show tables;

-- ¿Se cargó bien rita?
select * from rita limit 5;

-- Cargamos airports
load data inpath 'hive/datawarehouse/rita/catalogs/airports.csv'
overwrite into table airports;

-- Probamos
select * from airports where iata='SAN';

-- ¿Y si hacemos un JOIN?

select * from rita join airports on (rita.Origin = airports.iata) limit 10;
-- ¿Qué pasó?
#+end_example

** Tarea

Crear un =wordcount= para los archivos en =data= usando =Hive=



* Ejercicios

** RITA, HIVE y PIG
#+begin_example pig
register /home/hduser/hadoop-src/pig-0.12.0/contrib/piggybank/java/piggybank.jar;
define replace org.apache.pig.piggybank.evaluation.string.REPLACE;
define substring org.apache.pig.piggybank.evaluation.string.SUBSTRING;
define s_split org.apache.pig.piggybank.evaluation.string.Split;
define reverse org.apache.pig.piggybank.evaluation.string.Reverse;

airports = LOAD '/user/nano/rita_catalogs/airports.csv'
USING PigStorage(',')
AS
(iata:chararray,airport:chararray,city:chararray,
state:chararray,country:chararray,latitude:float,longitude:float);

fixed_airports = foreach airports
                 generate replace(iata, '"', ''),
                          replace(airport, '"', ''),
                          replace(city, '"', ''),
                          replace(state, '"', ''),
                          replace(country, '"', ''),
                          latitude, longitude;

store fixed_airports into '/user/pig/airports-fixed' using PigStorage(',');
#+end_example

** RITA y HIVE: Joins

#+begin_example sql
load data inpath 'pig_fixed/airports/part-m-00000'
overwrite into table airports;

-- ¿Y ahora?
select * from airports where iata='SAN';

select * from rita join airports on (rita.Origin = airports.iata) limit 10;
#+end_example



** Tarea: Pig y Hive

- Crear una tabla de RITA limpia (usando =PIG= y =HIVE=)
- Ejecutar dos exploraciones de las tareas de analítica de PostgreSQL,
  uno usando =PIG= y otro usando =HIVE=.

** ¡Más ejercicios!


- Usando =RITA= (lo que tengan cargado en su nodo), calcule:
  - Con =Pig=:
    - El número de vuelos por aeropuerto.
    - ¿Cuál es el más activo?
  - Con =Hive=:
    - Número de =km= por avión.
    - ¿Cuál es el /top/ 5?
    - ¿Sería más fácil en =Pig=?

** ¡Más ejercicios!

- Lo que sigue, son de los pocos algoritmos de /grafos/ que se pueden ejecutar en Hadoop...

- Similitud de *Jaccard*
  - También conocida como /similitud estructural/.
  - =RITA= se puede armar como un /grafo/  y queremos obtener los nodos (aeropuertos) que son similares.
  -  J(A,B) = \frac{|A \cap B|}{|A \cup B|} = \frac{|A \cap B|}{|A|+|B|-|A \cap B|}

** ¡Más ejercicios!

[[file:./imagenes/ejercicio_grafos.png]]


** Tarea

- Bajar la fuente de datos de  [[http://www.infochimps.com/datasets/marvel-universe-social-graph][aquí]].

- Aplicar la /similitud de Jaccard/ a este /set/.

- Obtener los más parecidos al *Capitán América*.

- Artículos:
  -  [[http://arxiv.org/abs/cond-mat/0202174][Marvel Universe looks almost like a real social network]]
  -  [[http://arxiv.org/abs/0708.2410][How to become a superhero]]


** ¡Más /grafos/!

- *degree of a node* = Número de conexiones a otros nodos
- *degree distribution* = Distribución de probabilidad en los /degrees/ sobre todo el /grafo/.

- =P(k)= = Fracción de nodos del /grafo/ con grado =k=,
  - Es decir: =P(k) = n_k/n=

** ¡Más /grafos/!

Entonces, en =RITA= podemos definir como grado el número siguiente:
- Por cada aeropuerto, el número de vuelos que llegan más el número de vuelos que salen
  - Para hacerlo temporal, incluiremos el mes
  - ¿Se les ocurre otra forma?


** ¡Más /grafos/!

#+begin_example
grunt> vuelos =
# Si no están en este formato hacer un ETL a:
# origen destino vuelo mes||año

# Transformamos:
grunt> aeropuertos_salidas =
# Sólo aeropuerto, mes, vuelos_entrada
grunt> aeropuertos_entrada =
# idem

# Unimos
grunt> agrupados = COGROUP aeropuertos_entradas BY (aeropuerto, mes), aeropuertos_salidas BY (aeropuerto, mes)
# ¿Por qué COGROUP y no JOIN?

# Sumamos
grunt> degree_dist = FOREACH agrupados {
               vuelos_degree = SUM(aeropuertos_entrada.vuelos_entrada) + SUM(aeropuertos_salida.vuelos_salida)
               GENERATE
                FLATTEN(group) AS (aeropuerto, mes),
                vuelos_degree AS vuelos_degree
              ;
};

# Guardamos
grunt> STORE degree_dist INTO ... ;

#+end_example

** ¡Más /grafos/!

- ¿Qué distribución tiene?

- *Ejercicio*: Graficarlo en R.
  - Obtener el resultado con =hdfs= y luego usarlo en =R= mostrar la temporalidad con =ggplot2=.

- *Tarea* ¿Alguna medida de grado para el /dataset/ de superhéroes?
  - ¿Qué distribución tiene?

- Más artículos:
  - [[http://arxiv.org/abs/cond-mat/0106096][Statistical mechanics of complex networks]]
  - [[http://arxiv.org/abs/cond-mat/0106144][Evolution of networks]]
  - [[http://arxiv.org/abs/cond-mat/0303516][The structure and function of complex networks]]


** Otro truco en =Hive=

- Si no saben =PigLatin= aún es posible limpiar en =Hive=.

- Creen una tabla con una sola columna y carguen un año de =RITA=

#+begin_example
create table temp_rita (columnota STRING);
LOAD DATA INPATH '/user/rita/2000.csv' OVERWRITE INTO TABLE temp_rita;
#+end_example

- Ahora creen una tabla con la estructura de las columnas que quieran
  extraer o limpiar

#+begin_example
create table rita (col_1 STRING, col_2 INT, ....);
# Y para extraerlas usen
insert overwrite table rita
select
regexp_extract(columnota, 'expresión_regular', 1) col_1,
regexp_extract(columnota, 'expresión_regular', 1) col_2,
...
from temp_rita
#+end_example

- Un ejemplo de expresión regular sería
#+begin_example
^(?:([^,]*)\,?)'
#+end_example
 y luego usen un cuantificador para extraer la columna que deseen.




* HCatalog

** HCatalog

- Está incorporado a =Hive= desde la versión =0.11=.

- Es una capa administrativa de tablas y almacenamiento que permite
  que diferentes herramientas de procesamiento de datos (=Pig=,
  =MapReduce=) puedan leer y escribir más fácilmente del =HDFS=.
- Contiene una abstracción que presenta una vista relacional de los
  datos contenidos en el =HDFS=, asegurando que los usuarios no se
  preocupen dónde o en que formato están almacenados los datos.

** HCatalog

- Utiliza el =DDL= de =Hive=.
- Provee interfaces de escritura y lectura para =Pig=, =MapReduce= y
  =Hive=.
- Usa la línea de comandos para manejar la definición de los datos y
  metadatos.
- =HCatalog= presenta los datos de manera relacional.
- Los datos son guardados en tablas y las tablas en bases de datos.

- =WebHCat= es la interfaz API =REST= de =HCatalog=.

** HCatalog: Flujo de datos

- Usuario 1 copia datos al HDFS
#+begin_example
hadoop distcp file:///data/books/pg2047.txt hdfs://data/20140430/books
hcat "alter table books add partition (ds='20140430') location 'hdfs://data/20140430/books'"
#+end_example

- Usuario 2 usa =Pig= para limpiar y preparar los datos.
  - =HCatalog= mandará al =JMS= un mensaje de que la información está disponible.

#+begin_example
A = load 'books' using HCatLoader();
B = filter A by date = '20140430';
...
store Z into 'procesados' using HCatStorer("date=20140430");
#+end_example

- Usuario 3 realiza cierta analítica
#+begin_example
select col1, count(col3)
from procesados
where date  = '201340430'
group by col1;
#+end_example

* HBase

** CAP Theorem

[[file:./imagenes/cap.png]]

* Apache Sqoop

** Apache Sqoop

- Herramienta para importar eficientemente /data/ desde =RDBMS= a Hadoop (=HDFS,
  Hive, Hbase=) y viceversa.
- Soporta cualquier =RDBMS= que tenga conexión =JDBC= (=PostgreSQL, MySQL, Oracle, Teradata=, etc.).
- Tiene soporte nativo para =MySQL= y =PostgreSQL=.

** Apache Sqoop

[[file:./imagenes/sqoop.png]]

** Ejercicio: RITA del tingo al tango

* Apache Flume

* Apache Spark

* Oozie

* Hue

* /Cluster/ de Hadoop

** Preliminares: NTP

- Los relojes de todos los nodos del /cluster/ deben de poder
  sincronizarse, por lo que es necesario instalar *NTP* (/Network
  Time Protocol/) en todos los nodos y un servidor *NTP* en el nodo maestro.

#+begin_example
sudo apt-get install ntp # en cada nodo
# Habilitar el servicio
chkconfig ntpd on # en cada nodo
# Iniciar el servicio
/etc/init.d/ntpd start # en cada nodo
# Configurar los clientes para usar el servidor NTP local
# en el archivo /etc/ntp.conf
server $IP_SERVIDOR_LOCAL
#+end_example

** Preliminares: DNS

- Todos los nodos deben de estar configurados para =DNS= y =Reverse
  DNS=

Primero verifiquemos que funcione el /forward lookup/:

#+begin_example
nslookup host01
# Debería de devolver:
# Name: host01.dominio
# Address: ip_address
#+end_example

Y el /reverse lookup/

#+begin_example
nslookup ip_address
# Debería de devolver:
x.x.x.x.in-addr.arpa  name=host01.dominio
#+end_example

Si esto falla hay dos opciones

1. Modificar  en /cada nodo/ el =/etc/hosts= y agregar todos los nodos
   (incluido el mismo)

#+begin_example
ip_address name
#+end_example

2. Configurar un servidor de =DNS= con =bind= (No se verá aquí).

** Preliminares: SELinux

- Si tienen habilitado *SELinux* /Security Enhaced Linux/ , hay que
  deshabilitarlo
  - Es muy probable que si están en una =distro= basada en =Debian= no
    esté instalado por /default/.
  - Pueden verificarlo tecleando =getenforce= en la línea de comandos.
  - Si dice que no está instalado, o responde =permissive= o
    =disabled=, ya no hay que hacer nada.
  - En caso contrario, seguir las instrucciones para desactivarlo (No
    se ve aquí, no usamos =distros= basadas en =Red Hat=).

** Preliminares: Firewall

- Deshabiliten el =firewall=

#+begin_example
service ufw stop
#+end_example

- Recuerden que están en una red privada, no hay manera de acceder a
  ella desde afuera.

** Preliminares: Usuarios

-Hay que definir los siguientes usuarios y grupos:

=HDFS_USER=, =YARN_USER=, =ZOOKEEPER_USER=,
=HIVE_USER=, =MAPREDUCE_USER=, =OOZIE_USER=
=WEBHCAT_USER=, =HBASE_USER=, =PIG_USER= y =HADOOP_GROUP=.

Por ejemplo:

#+begin_example
| Servicio             | User      | Grupo  |
|----------------------+-----------+--------|
| HDFS                 | hdfs      | hadoop |
| YARN                 | yarn      | hadoop |
| MapReduce            | mapred    | hadoop |
| Hive                 | hive      | hadoop |
| Pig                  | pig       | hadoop |
| HCatalog/WebHCatalog | hcat      | hadoop |
| HBase                | hbase     | hadoop |
| ZooKeeper            | zookeeper | hadoop |
| Oozie                | oozie     | hadoop |
#+end_example

cada uno de ellos ejecutará y poseerá los servicios que hay que levantar.

** Preliminares: SSH

- Los usuarios deben de poder conectarse en todos los nodos (*todos*).
- Hay que crear las llaves con =ssh-keygen= y distribuirlas con
  =ssh-copy-id=

#+begin_example
ssh-keygen -t rsa -C "your_email@example.com"
ssh-copy-id user@ipaddress
#+end_example

** Preliminares: Directorios

- Núcleares


#+begin_example
| Servicio  | Parámetro         | Definición |
|-----------+-------------------+------------|
| HDFS      | DFS_NAME_DIR      |            |
| HDFS      | DFS_DATA_DIR      |            |
| HDFS      | FS_CHECKPOINT_DIR |            |
| HDFS      | HDFS_LOG_DIR      |            |
| HDFS      | HDFS_PID_DIR      |            |
| HDFS      | HADOOP_CONF_DIR   |            |
| YARN      | YARN_LOCAL_DIR    |            |
| YARN      | YARN_LOG_DIR      |            |
| YARN      | YARN_PID_DIR      |            |
| MapReduce | MAPRED_LOG_DIR    |            |
#+end_example

** Preliminares: Directorios

- Ecosistema

#+begin_example
| Servicio  | Parámetro          | Definición |
|-----------+--------------------+------------|
| Pig       | PIG_CONF_DIR       |            |
| Pig       | PIG_LOG_DIR        |            |
| Pig       | PIG_PID_DIR        |            |
| Oozie     | OOZIE_CONF_DIR     |            |
| Oozie     | OOZIE_DATA         |            |
| Oozie     | OOZIE_LOG_DIR      |            |
| Oozie     | OOZIE_PID_DIR      |            |
| Oozie     | OOZIE_TMP_DIR      |            |
| Hive      | HIVE_CONF_DIR      |            |
| Hive      | HIVE_LOG_DIR       |            |
| Hive      | HIVE_PID_DIR       |            |
| WebHcat   | WEBHCAT_CONF_DIR   |            |
| WebHcat   | WEBHCAT_LOG_DIR    |            |
| WebHcat   | WEBHCAT_PID_DIR    |            |
| Hbase     | HBASE_CONF_DIR     |            |
| Hbase     | HBASE_LOG_DIR      |            |
| Hbase     | HBASE_PID_DIR      |            |
| Zookeeper | ZOOKEEPER_DATA_DIR |            |
| Zookeeper | ZOOKEEPER_CONF_DIR |            |
| Zookeeper | ZOOKEEPER_LOG_DIR  |            |
| Zookeeper | ZOOKEEPER_PID_DIR  |            |
| Sqoop     | SQOOP_CONF_DIR     |            |
#+end_example


** Preliminares: Base de datos

Algunos componentes de =Apache Hadoop=, (como =HIVE=, =Zookeeper=)
requieren de una base de datos relacional para funcionar.

Se recomienda instalar =PostgreSQL=, instale y configure como se vió
en la lección 4 de este curso. Será necesario tener el =driver JDBC=,
no se te olvide.


** ¿Qué Hardware necesito?

- ¿Cuántos datos va a contener el /cluster/?
- ¿Cuáles son las proyecciones de crecimiento de los datos?
- ¿El /cluster/ será usado para tareas programadas en lote?
- ¿Será usado para análisis exploratorio de datos?
- ¿Ambos?



** Hardware: /DataNode/

- Cumplen dos funciones: Almacenan piezas de los datos del =HDFS= y
  ejecutan tareas =MapReduce=.

- Deben de ser lo suficientemente rápidos para
  ejecutar bien la tarea, pero baratos para ser remplazados
  rápidamente si fallan (Son desechables).

- Debido a la redundancia que provee el =HDFS= no es necesario pensar en
  arreglos =RAID= para estos componentes, la opción preferida es
  =JBOD= /Just a Bunch Of Disks/.

- El =DataNode= debe de poseer almacenamiento y poder de cómputo.

** Hardware: /DataNode/

1. Identificar las fuentes de datos.
   - Piensa en 20% a 30% más de la capacidad del /cluster/ por si hay
     nuevos datos.
2. Estimar la tasa de crecimiento de los datos.
3. Multiplica los requerimientos de almacenamiento por el factor de
   replicación (por /default/ es *3*).
   - Ejemplo: Si estimaste =3 TB= en un año, serían =9 TB= la
     capacidad requerida del /cluster/.
4. Archivos temporales de =MapReduce=.
   - =MapReduce= produce archivos que son pasados de la fase de mapeo
     a la de reducción, estos no residen en el =HDFS=, se recomienda
     apartar del 25% al 30% de la capacidad de disco para estos
     archivos.

** Hardware: /DataNode/

- No vale la pena comprar discos de 15000 rpm para el
  /cluster/. Haddop realiza (principalmente) lectura/escritura
  /secuencial/, más que aleatoria, por lo que discos de *7200 rpm* son
  suficientes.

- /Cluster/ de baja densidad
  - Objetivo: Nodos de bajo costo.
  - 6 /slots/ para =HDD=, con discos de 2 TB, nos dan 12 TB por
    servidor.
  - 2 CPUs de 4 /cores/ (cada tarea de map o reduce utiliza un sólo
    /core/, si abusamos un poco podemos tener hasta 12 /slots/ de
    map/reduce por nodo)
  - Cada tarea requiere de 2 a 4 GB de RAM, 36 GB serían razonables,
    pero 48GB por nodo es una opción mejor.
  - Interfaces de 1GbE, utilizando la mayor cantidad de tarjetas de red
  - *Ejercicio:* ¿Cuántos servidores se requerirían para =500TB=?

** Hardware: /DataNode/

- /Cluster/ de alta densidad
  - Objetivo: Un /cluster/ más compacto, con mayor poder por nodo.
    - No se requiere tener cantidades enormes de datos.
    - /Machine learning/, /EDA/, etc.
  - Cada nodo con 16x2 TB en =HDD= o 24x1TB.
  - 16 CPUs y 96 GB de RAM.
  - Interfaz de 10 GbE.

** Hardware: /Namenode/

- El /Namenode/ es crítico en la disponibilidad del =HDFS=.
- Almacena todo el metadata del /filesystem/: que bloques con que
  archivos, en que /datanodes/ están esos bloques, cuántos bloques
  están disponibles y que servidores pueden contenerlos. Toda esta
  información debe de estar en memoria.

- Cada bloque ocupa 250 bytes de RAM y 250 bytes de RAM por cada
  archivo y directorio.
  - ¿Cuánto sería necesario por 5000 archivos de 20 GB cada uno?


** Hardware: =YARN=, =MapReduce=

- =YARN= toma en cuenta los recursos completos del nodo, para negociar
  las peticiones de recursos de las aplicaciones corriendo en el /cluster/ (un ejemplo sería =MapReduce=).

- =YARN= provee esos recursos a cada aplicación en la forma de un
  *Contenedor*.

- Es una buena práctica, permitir 2 contenedores por disco por /core/.

** Hardware: =YARN=, =MapReduce=

- En cada nodo, hay que tomar en cuenta la cantidad reservada para los
  procesos del sistema que *no son* Hadoop.


#+begin_example
| Total Memory per Node | Recommended Reserved System Memory |
|-----------------------+------------------------------------|
| 4 GB                  | 1 GB                               |
| 8 GB                  | 2 GB                               |
| 16 GB                 | 2 GB                               |
| 24 GB                 | 4 GB                               |
| 48 GB                 | 6 GB                               |
| 64 GB                 | 8 GB                               |
| 72 GB                 | 8 GB                               |
| 96 GB                 | 12 GB                              |
| 128 GB                | 24 GB                              |
| 256 GB                | 32 GB                              |
| 512 GB                | 64 GB                              |
#+end_example

- Luego hay que calcular el número máximo de contenedores por nodo

#+begin_example
# de contenedores = min(2*cores, 1.8*discos, (RAM total disponible)/MIN_CONTAINER_SIZE)
#+end_example

** Hardware: =YARN=, =MapReduce=

Donde el =MIN_CONTAINER_SIZE= se toma de la siguiente tabla

#+begin_example
| RAM total por nodo     | Tamaño mínimo del contenedor       |
|------------------------+------------------------------------|
| Less than 4 GB         | 256 MB                             |
| Between 4 GB and 8 GB  | 512 MB                             |
| Between 8 GB and 24 GB | 1024 MB                            |
| Above 24 GB            | 2048 MB                            |
#+end_example


Luego, hay que calcular

#+begin_example
RAM_por_contenedor = max(MIN_CONTAINER_SIZE, (RAM total disponible)/Contenedores)
#+end_example

** Hardware: =YARN=, =MapReduce=

Con estos cálculos se obtiene la siguiente tabla:


#+begin_example
| Configuration File    | Configuration Setting                | Value Calculation                |
|-----------------------+--------------------------------------+----------------------------------|
| yarn-site.xml         | yarn.nodemanager.resource.memory-mb  |  Containers * RAM-per-Container |
| yarn-site.xml         | yarn.scheduler.minimum-allocation-mb |  RAM-per-Container              |
| yarn-site.xml         | yarn.scheduler.maximum-allocation-mb |  containers * RAM-per-Container |
| mapred-site.xml       | mapreduce.map.memory.mb              |  RAM-per-Container              |
| mapred-site.xml       | mapreduce.reduce.memory.mb           |  2 * RAM-per-Container          |
| mapred-site.xml       | mapreduce.map.java.opts              |  0.8 * RAM-per-Container        |
| mapred-site.xml       | mapreduce.reduce.java.opts           |  0.8 * 2 * RAM-per-Container    |
| yarn-site.xml (check) | yarn.app.mapreduce.am.resource.mb    |  2 * RAM-per-Container          |
| yarn-site.xml (check) | yarn.app.mapreduce.am.command-opts   |  0.8 * 2 * RAM-per-Container    |
#+end_example

** Filesystem

- Usemos =ext4=.
- Formatea con el siguiente comando (¡Cuidado fíjate en la partición!)
  - =-m 0= reduce el espacio para =root= a 0% en lugar del 5%.
  - =-O extent, sparse_super, flex_bg= Mejora la lectura secuencial
    (=extent=), y mejora las opciones de espacio (=sparse_super=) al no
    almacenar tantos superbloques y por último empaca juntos los
    metadatos (=flex_bg=).

#+begin_example
mkfs -t ext4 -m 0 -O extent, sparse_super, flex_bg /dev/sdb1
#+end_example


- En el =/etc/fstab=, activa las opciones =noatime=, =noadirtime=

#+begin_example
...
/dev/sda1 /disk1  ext4 noatime, noadirtime 1 2
...
#+end_example


** Ejercicio: Armar un /cluster/

- El objetivo es reproducir el siguiente diagrama arquitectónico (por
  lo menos).

[[file:./imagenes/layout.png]]


- Use =Vagrant=, =chef= y =berkshelf=.


* Compresores

** Instalación

#+begin_example

# Snappy
sudo apt-get install libsnappy1 libsnappy-dev

#LZO
sudo apt-get install liblzo2-2 liblzo2-dev#+end_example
#+end_example

** Tipos

[[file:./imagenes/compresores.png]]

* Misceláneos

** Tips

- =Reduce= es regularmente más intensivo en cuanto consumo de recursos que =Map=
  - Usa =Combiners=.
  - Explora tus datos antes
    - Como están distribuidos es muy importante.
    - Quizá Hadoop no sea lo correcto.

- En la vida real, instala desde una distribución: *BigTop*, *Horton* o *Cloudera*.
  - Y =Vagrant=

** /Small File Problem/

* Disclaimer

Algunas imágenes se tomaron de los libros /Professional Hadoop Solutions/
de *Wrox* y de la página de [[http://hortonworks.com/hadoop/yarn/][Hortonworks]]. Las otras son mías.

Las tablas de la sección /cluster/ de Hadoop, se tomaron de [[http://hortonworks.com/][Hortonworks.]]
