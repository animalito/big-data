#+Title: Lecture 4
#+Author: Adolfo De Unánue
#+Email: adolfo.deunanue@itam.mx

#+OPTIONS: toc:nil reveal_mathjax:t
#+REVEAL_TRANS: fade
#+REVEAL_THEME: night
#+REVEAL_MARGIN: 0.1
#+REVEAL_MIN_SCALE: 0.5
#+REVEAL_MAX_SCALE: 2.5
#+REVEAL_HEAD_PREAMBLE: <meta name="description" content="Big data - Lecture 4.">
#+REVEAL_POSTAMBLE: <p> Creado por Adolfo De Unánue Tiscareño. </p>
#+REVEAL_PLUGINS: (highlight markdown notes)
#+REVEAL_ROOT: http://cdn.jsdelivr.net/reveal.js/2.5.0/
#+OPTIONS: tex:t
#+OPTIONS: reveal_center:t reveal_progress:t reveal_history:nil reveal_control:t
#+OPTIONS: reveal_mathjax:t reveal_rolling_links:t reveal_keyboard:t reveal_overview:t num:nil
#+OPTIONS: reveal_width:1200 reveal_height:800


# tema: default, beige, sky, night, serif, simple, moon
# trans: default, cube, page, concave, zoom, linear, fade, none

* Docker

** Obtener la imagen

#+begin_example
docker pull nanounanue/docker-hadoop
#+end_example

** Ejecutar un contenedor

#+begin_example
docker run -ti --rm \
  -e "AUTHORIZED_SSH_PUBLIC_KEY=$(cat ~/.ssh/id_rsa.pub)" \
  -v /home/nano/tmp/docker-hadoop-data/:/home/hduser/hdfs-data/ \
  -v /home/nano/tmp/docker-hadoop-logs/:/srv/hadoop/logs/ \
  -p 2122:2122 -p 2181:2181 -p 39534:39534 -p 9000:9000 \
  -p 50070:50070 -p 50010:50010 -p 50020:50020 -p 50075:50075 \
  -p 50090:50090 -p 8030:8030 -p 8031:8031 -p 8032:8032 \
  -p 8033:8033 -p 8088:8088 -p 8040:8040 -p 8042:8042 \
  -p 13562:13562 -p 47784:47784 -p 10020:10020 -p 19888:19888
  -p 8000:8000 \
nanounanue/docker-hadoop /bin/zsh
#+end_example

** Contenedor con Hadoop

#+begin_example
docker run -ti --name hadoop_pseudodistribuido \
  -e "AUTHORIZED_SSH_PUBLIC_KEY=$(cat ~/.ssh/id_rsa.pub)" \
  -v /home/nano/tmp/docker-hadoop-data/:/home/hduser/hdfs-data/ \
  -v /home/nano/tmp/docker-hadoop-logs/:/srv/hadoop/logs/ \
  -p 2122:2122 -p 2181:2181 -p 39534:39534 -p 9000:9000 \
  -p 50070:50070 -p 50010:50010 -p 50020:50020 -p 50075:50075 \
  -p 50090:50090 -p 8030:8030 -p 8031:8031 -p 8032:8032 \
  -p 8033:8033 -p 8088:8088 -p 8040:8040 -p 8042:8042 \
  -p 13562:13562 -p 47784:47784 -p 10020:10020 -p 19888:19888 \
  -p 8000:8000 \
nanounanue/docker-hadoop
#+end_example

** Conectarse al contenedor

#+begin_example
ssh hduser@localhost -p 2122
#+end_example

** Navegador Web

- [[http://127.0.0.1:50090][Consola de Yarn]]

- [[http://127.0.0.1:50070][Consola de HDFS]]

- [[http://0.0.0.0:8000][HUE - Hadoop User Experience]]



** Ejercicio

- Explicar los diferentes modos en los que puede ejecutarse **Apache Hadoop**.

-  *Modo Pseudodistribuido*

   - Crea el contendeor de Docker para Hadoop,  vamos a explicar que significa /pseudodistribuido/.


* Apache Hadoop

** ¿Por qué?

- Aunque la capacidad de los discos ha aumentado considerablemente, la velocidad de los mismos no lo ha hecho igual.
  - Los discos actuales de =1 Tb=, tardan en leerse completos a =100 Mb/s= cerca de dos a tres horas.
  - Podemos /paralelizar/ las fuentes en varios discos.
    - Para leerla simultáneamente
  - Con varios discos, la **probabilidad de falla** aumenta.
- Otro problema es la distribución ¿Cómo combinas varios =file systems=?

** ¿Qué es?

- Sistema confiable (/realiable/) de almacenamiento compartido y de procesamiento de datos.
  - *Almacenamiento*: _Hadoop Distributed File System_, =HDFS=
  - *Procesamiento*: Varios _frameworks_ basados en =YARN=.

- Puede procesar cantidades masivas de datos y escalar conforme crezcan los datos.

- Flexibilidad para el procesamiento de datos.
  - No importa la estructura o falta de ella

- Está construido en =Java=.

** ¿Cómo?

- =MapReduce= es un sistema de procesamiento /batch/
  - Permite correr /queries/ contra **toda** tu base de datos
  - Pero el resultado puede tardar minutos, horas, etc...
  - No permite tener a un humano sentado ahí para retroalimentar.

** ¿Cómo?

- Ahora, gracias a =YARN= (ver más adelante) tenemos diferentes tipos de procesamiento:
  - /SQL Interactivo/: =Impala=, =Hive=, =Tez=.
  - /Iterativos/: =Spark=.
  - /Procesamiento de flujos/: =Storm=, =Spark Streaming=.
  - /Búsquedas/: =Solr=.

** ¿Por qué no otros sistemas?

- ¿Por qué no usar un =PostgreSQL= con muchos discos, muy /pimpeado/?
  - El problema viene del tiempo que toma mover la cabeza del disco a otro lugar del disco para leer o escribir datos (/seek time/).
    - ¿Cuál es la /latencia/ de la operación?

- ¿Por qué no /Grid/?
  - Por ejemplo, cosas  de =HPC= que usan =MPI=.
    - Son intensivos en **CPU**.
  - Pero si hay que mover cientos de gigabytes, la transferencia de datos se vuelve un problema.
    - Basicamente, en que =Hadoop= opera con /data locality/.

** Componentes de Apache Hadoop

- *MapReduce* Modelo de procesamiento /batch/ de datos distribuido y paralelo.
- *HDFS* Sistema de archivos (/file system/) distribuido.
- *Pig* Capa de abstracción encima de =MapReduce=. Utiliza /Pig Latin/ un lenguaje de flujo de datos
  - Como =dplyr=
- *Hive* (Hadoop InteractiVE) Es un lenguaje parecido al =SQL=: =HQL=, para ejecutar /queries/ sobre el =HDFS=.
- *HBase* Base de datos distribuida orientada a columnas.
  - Depende de =Zookeeper=.
- *Impala* Lenguaje Interactivo parecido al =SQL=, pero mucho más rápido de =HIVE= debido a su arquitectura *MPP*.

** Componentes de Apache Hadoop

- *Zookeeper* Proyecto que proveé un servicio centralizado para facilitar la coordinación de componentes de Hadoop.
- *Sqoop* Herramienta para mover datos entre =RDBM= y =HDFS=.
- *Flume* Servicio para recolectar, agregar y mover grandes cantidades de datos entre máquinas individuales y el =HDFS=.
- *Oozie* Sistema de /workflow/, se usa para coordinar varios /jobs/ de *MapReduce*.
- *Mahout* Biblioteca de /Machine Learning/.
  - Ver la carpeta =docs=.
- *Ambari* Simplifica el aprovisionamiento, gestión y /monitoreo/ de un /cluster/ de Hadoop.
- *Avro* Formato de serialización y de persistencia de datos.
- Entre otros...



* HDFS : Hadoop File System


** HDFS

- Sistema de almacenamiento distribuido.
  - /Namenode/ =->= Master
  - /Datanode/ =->=  Slaves

** _Schema on Read_

- Es posible cargar datos sin procesar dentro de Hadoop, la estructura se dará en el tiempo de procesamiento.

- Es muy diferente a _Schema on Write_ como el usado en los =RDBM=s
  - _Schema on Write_ impone un ciclo de análisis y modelado de datos, así como de su transformación, carga y prueba, antes de los datos puedan ser accesados.
  - Esto quita mucha flexibilidad: Si se tomaron decisiones incorrectas o los requerimientos cambian, es necesario empezar de nuevo =:(= .

** Ventajas

- Archivos muy grandes
- /write once, read many times/.
- Hardware _normal_

** Desventajas

- Acceso a los datos de baja latencia.
- Muchos archivos pequeños.
- Muchas escrituras, modificaciones

** Tamaño del bloque

- Cada /file system/ define un tamaño de bloque, el cual es la cantidad mínima de datos que puede escribir o leer.
  - Típicamente son de =kb=.
- En =HDFS=, el bloque es de =128 Mb= por /default/.
  - Es el concepto fundamental, no el archivo.


** /Namenode/
  - Gestiona el /filesystem/
    - Mantiene el árbol del /filesystem/.
    - Mantiene los =metadatos= de todos los archivos y carpetas del árbol.
    - Esta información se guarda en disco en dos archivos:
      - =namespace image=
      - =edit log=
  - Indica a los /datanodes/ realizar tareas de bajo nivel de =I/O=.
  - /Book Keeper/
    - División de archivos en bloques (¿Cómo?)
    - En qué /datanode/ (¿Quién?)
    - Monitorea.
  - Uso intensivo de =RAM= y de =I/O=.
  - Si se /cae/ el =HDFS= no puede ser usado
    - Hasta la versión =1.x= el /single point of failure/, en Hadoop 2 se incorporó la característica de /HIgh Availability/.
    - Su caída puede causar la pérdida total de los datos.

** /Namenode/

- Hadoop proveé de dos formas de aliviar esta situación:
  - Respaldos: Se puede configurar al /namenode/ para que escriba su estado a varios /filesystems/.
  - /Secondary Namenode/

** /Namenode/

[[file:./imagenes/Selección_004.png]]


** /Datanode/
  - Lee y escribe los =HDFS= /blocks/ y los convierte en archivos del *FS* local.
  - Se comunica con otros /datanodes/ para la replicación de los datos.
  - Pueden realizar /caching/ de bloques.

** /Datanode/

[[file:./imagenes/Selección_005.png]]

** /Secondary Name Node/
  - Como el /namenode/ sólo hay uno por /cluster/.
  - No es un /namenode/.
  - Evita que el =edit log= crezca mucho.
  - No recibe ni guarda cambios en tiempo real del =HDFS=.
    - Va atrás del /namenode/.
  - Sólo toma /snapshots/ de la metadata.


** Línea de comandos

- Hay muchas maneras de conectarse y usar el =HDFS=. La línea de comandos es una de ellas.
  - Y espero que ya sepan que es de las más útiles y eficientes.

- Ayuda: =hadoop fs -help=

** Línea de comandos

#+begin_example
hadoop fs -cmd <args>
hadoop fs -ls
hadoop fs -mkdir
hadoop fs -copyFromLocal
hadoop fs -copyToLocal
hadoop fs -put archivo archivo_hdfs
hadoop fs -get archivo_hdfs
hadoop fs -cat archivo_hdfs
hadoop fs -cat archivo_hdfs head
hadoop fs -tail archivo_hdfs
hadoop fs -rm archivo_hdfs
#+end_example

* Decisiones Arquitectónicas: Ingesta de datos

** Decisiones Arquitectónicas

- El hecho de que el =HDFS= permita _Schema on Read_, no elimina la necesidad de tomar decisiones arquitectónicas en la ingesta de los datos, entre ellos:

  - ¿Cómo se guardarán los datos?
    - Capa de almacenamiento
    - Formatos de archivos
    - Formatos de compresión

  - ¿Diseño de esquema de datos?
    - Directorios donde guardar los datos y donde ponerlos luego del procesamiento y analítica.
    - También en =HBase= y en =Hive= se definen esquemas.
  - ¿Cómo se gestionarán los metadatos?
  - ¿Cómo se administrará la seguridad?
    - Autenticación, cifrado, acceso controlado.

** Capa de almacenamiento: =HDFS= vs =HBase=

- =HDFS=
  - ALmacena los datos como archivos
  - _Scans_ rápidos.
  - Malo para acceso aleatorio en escritura y lectura.

- =HBase=
  -  Guarda los datos como archivos de HBase en el =HDFS=.
  - _Scans_ lentos.
  - Rápido acceso aleatorio a lectura y escritura.

En esta clase nos enfocaremos a =HDFS=.

** Formatos de archivos

- Tipos de archivos de Hadoop
  - Basados en archivos: =SequenceFiles=.
  - Formatos serializados: =Avro=, =Thrift=.
  - Formatos columnares: =RCFile=, =ORCFile=, =Parquet=.

- Debido a que la mayoría de formatos de archivos sólo se puede acceder desde =Java=, nos enfocaremos en sólo dos: =Avro= y =Parquet=

** Formatos de archivos

- =Avro=
  - Independiente del lenguaje.
  - Almacena el esquema en el encabezado de cada archivo.
  - Son comprensibles y divisibles.
    - Soporta compresión con =snappy=.
  - Es recomendable usarlo en la ingesta de datos.
  - Las fallas sólo afectan a una porción del archivo.

** Formatos de archivos

- =Parquet=
  - Diseñado para proveer procesamiento eficiente a través de varios compoentes de hadoop.
  - Almacena los datos de manera columnar.
  - Provee excelentes capacidades de compresión.
  - Soporta estructuras de datos complejas y anidadas.
  - Los metadatos están guardados al final del archivo.
  - Puede escribirse y leerse con las APIs de Avro y con esquemas de Avro.
  - No son tan buenos para recuperarse de errores.

** Formatos de compresión

- Ayuda a reducir los requerimientos de almacenamiento
- Mejora el procesamiento de los datos
  - Disminuye ,a cantidad de I/O en disco y red.
- Para aprovechar las capacidades de procesamiento en paralelo de Hadoop es preferible que el formato sea divisible.


** Formatos de compresión

- =bzip2=
  - Excelente factor  de compresión
  - Pero muuuuuy lento en compresión/decompresión
  - Divisible

- =snappy=
  - Proyecto de Google.
  - No es divisible, pero muy eficiente en compresión/decompresión.
  - Se debe de usar con un formato de archivo que provea la capacidad de contenedor (=Avro=, =SequenceFiles=).

- =gzip=
  - No es divisible
  - Buen factor de comrpesión: 2.5x lo de =snappy=.
  - Se debe de usar con un formato de archivo que provea la capacidad de contenedor (=Avro=, =SequenceFiles=).

- =lzop=
  - Parecido a =snappy= en eficiencia de compresión/decompresión.
  - Divisible, pero requiere una etapa de indexado.
  - Buena elección para guardar archivos de texto planos que no se pondrán dentro de un contenedor.
  - Licenciamiento raro (No viene incluido con Hadoop).

** Esquema

- Basado en Hadoop Application Architectures.

-  ¿Por qué?
   - Estructura de archivos estándar facilita la colaboración entre equipos.
   - Permite la reutilización de código para procesarla.
   - Permite reforzar las políticas de acceso y evitar así corrupción de los datos.
   - Permite identificar que datos han sido procesados completamente y cuales no
   - Muy parecido a los =schemas= de PostgreSQL.

** Esquema Propuesto

- =/user/<username>=
   - Datos para experimentar (i.e. no son parte del proceso de negocio).
   - =JAR=s, archivos de configuración.
   - Sólo debe de tener permisos de R/W el usuario en cuestión.

- =/etl=
   - Datos en sus varias etapas de transformación por el ETL.
   - Subdirectorios reflejan el _workflow_ de los datos.
     - Los ETL son creados por *grupos* para *aplicaciones*.
     - Además cada subdirectorio tendrá a su vez directorios para cada etapa del proceso:
       - =input= para el lugar donde llegan los archivos
       - =procesando= para los pasos intermedios (puede haber varios)
       - =output= para el resultado final
       - =rechazados= para los registros o archivos que no pudieron ser procesados y que deben de verificarse manualmente.
   - La estructura quedaría así:
     - =/etl/<grupo>/<aplicación>/<proceso>/{input, procesando, output, rechazados}=
   - Sólo el usuario =etl= y los usuarios del grupo =etl= pueden R/W.

- =/tmp=
   - Datos temporales generados por usuarios o partes de Hadoop.
   - Se borra su interior regularmente.
   - Todos tienen permisos de RW en este directorio.

- =/data=
   - Datos procesados y usados por la organización
   - Existen controles sobre quién puede o no usar los datos
   - Los usuarios sólo tienen permisos de lectura.
   - Los procesos automatizados (y auditados) tienen permisos de escritura.

- =/app=
   - Todo lo requerido por la aplicación de Hadoop para funcionar (salvo datos)
   - Archivos de Oozie (definiciones de _workflows_),
   - Archivos de =hql=, =pig=, =JAR=s, =UDF=s, etc.

** Otras consideraciones

- *Particionado*
  - Ayuda a reducir la cantidad de I/O para procesar los datos.
  - Es una especie de _indexado_ básico.
  - =<nombre del dataset>/<columna sobre la cual particionar>=<valor de la columna>/{archivos}=

- *Denormalizar*
  - Ahorras =Joins= (que son lentos)

** Ejercicio
- Cambia al usuario =itam=.
- Revisa la estructura de directorios con el usuario =hdfs=.
  - Esto lo puedes hacer con =sudo -u hdfs ...=
- Crea el esquema de directorios propuesta.
  - Esto lo puedes hacer con =sudo -u hdfs ...=
  - =/user/<username>=, =/etl= (para la aplicación =ufo= y =gdelt=, el grupo es =ds=), =/tmp=, =/app= y =/data=.
  - Las últimas tres están vacías.
- Asigna los permisos adecuados.
- Dentro de tu carpeta (siendo el usuario =itam=), crea la carpeta =ufo= y la carpeta =gdelt=.
- Carga un archivo de cada dataset a esta carpeta usando la línea de comandos.

** Ejercicio

En este ejercicio usaremos =kite=, es una herramienta que nos permite cargar y administrar los metadatos de los archivos a Hadoop.


* YARN

** YARN

- La infraestructura de Hadoop =0.x= y =1.x= era monolítica, por eso fue rediseñada.
- =YARN=: /Yet Another Resource Negotiator/.
- La gestión de recursos es extraída de los paquetes de =MapReduce= para que puedan ser utilizadas por otros componentes.
- Aportaciones
  - Escalabilidad.
  - Compatibilidad con =MapReduce=.
  - Mejoras en la gestión del /cluster/.
  - Soporte para otros modelos de programación (además de =MapReduce=).
    - /Graph processing/
    - /Message Passing Interface/ (*MPI*).
    - Soporte para procesamiento /real-time/ o /near real-time/.
      - =MapReduce= es /batch-oriented/.
  - Agilidad.

** YARN

- Se dividieron las dos responsabilidades del /JobTracker/:
  - Gestión de recursos (/Resource Management/)
  - Asignación y vigilancia de trabajos (/Job scheduling-monitoring/)

- La idea es tener un /ResourceManager/ global y un /NodeManager/ por
  nodo esclavo, los cuales forman un sistema para la administración de
  aplicaciones distribuidas.

- El /ResourceManager/ tiene dos componentes principales:
  - /Scheduler/: Asigna los recursos para las aplicaciones (/pluggeable/).
  - /Application Manager/: Responsable de aceptar las solicitudes de
    trabajos, negociando al principio para ejecutar el /Application
    Master/ específico y provee un servicio de reinicio, por si el
    /Application Master/ falla.

- En cada nodo:

   - El /Application Master/: Negocia sus recursos con el /Scheduler/,
  monitorea sus avances y reporta su estatus.

   - El /NodeManager/ es el responsable de los contenedores,
     monitorear el uso de recursos y reportar todo al
     /ResourceManager/.

** Arquitectura MapReduce Hadoop 1.x

[[file:./imagenes/MRArch.png]]

** Arquitectura Hadoop 2.x

[[file:./imagenes/Selección_003.png]]


** Cambios 1.x -> 2.x

[[file:./imagenes/yarn.png]]


** Multiparadigma en Hadoop 2.x

 [[file:./imagenes/YARN.png]]


* Procesamiento

** Tipos

- MapReduce
- Spark
- Impala

** MapReduce en Hadoop
- Principal /framework/ de ejecución de =Apache Hadoop=.
- Inspirado en las operaciones *MAP* y *REDUCE* de los lenguajes funcionales.
- Modelo de programación para proceso de datos distribuido  y paralelo.
- Divide las tareas (/jobs/) en fases de /mapeo/ y fases de /reducción/.
- Los desarrolladores crean tareas /MapReduce/ para Hadoop usando datos guardados en el =HDFS=.

** MapReduce: Ventajas

   - /Fault-tolerant/.
   - Esconde los detalles de implementación a los programadores.
   - Escala con el tamaño de los datos.


** MapReduce

- Dos fases de procesamiento:
  - /key-value/ como Input y Output
  - El programador especifica:
    - Tipos de /key-value/
    - Funciones: =MAP= y =REDUCE=.


** Una pequeña regresión...

** map-reduce: Matemáticamente

#+BEGIN_EXAMPLE
map: (k1, v1) -> list(k2, v2)
#+END_EXAMPLE

- =map= Mapea (aplica una función /f/) un conjunto de entrada de pares /key-value/ a otro conjunto intermedio de /key-values/


** map-reduce: Matemáticamente

#+BEGIN_EXAMPLE
reduce: (k2, list(v2)) -> list(k3, v3)
#+END_EXAMPLE

- =reduce=  Aplica una función /g/ a todos los valores (/values/) asociados a una llave (/key/) y acumula el resultado. Emite pares de /key-values/.

** Python =map=

#+begin_src python :results output :export both
# Equivalente en for-loop

items = [1,2,3,4,5]
cuadrados = []
for x in items:
    cuadrados.append(x**2)

print cuadrados
#+end_src

#+RESULTS:
: [1, 4, 9, 16, 25]


#+begin_src python :results output :export both
# Usando la funcion map(function, sequence)

items = [1,2,3,4,5]

print list(map((lambda x: x**2), items))
#+end_src

#+RESULTS:
: [1, 4, 9, 16, 25]


** Python =reduce=

#+begin_src python :results output :export both
# Equivalente en for-loop
L = [1,2,3,4]
result = L[0]
for x in L[1:]:
    result = result*x

print result

#+end_src

#+RESULTS:
: 24

#+begin_src python :results output :export both
# Usando la funcion reduce(funcion, secuencia)
print reduce((lambda x,y: x*y), [1,2,3,4])
#+end_src

#+RESULTS:
: 24

** Python =map= y =reduce=

#+begin_src python :results output :export both
a = range(1, 4)
b = range(4, 9)
c = range(9, 15)
print "a ->  %s, b -> %s , c -> %s" % (a, b, c)

L1 = map(lambda x:len(x), [a,b,c])
print "L1 -> %s" % L1

L2 = reduce(lambda x, y: x+y, L1)
print "L2 -> %s" % L2
#+end_src

#+RESULTS:
: a ->  [1, 2, 3], b -> [4, 5, 6, 7, 8] , c -> [9, 10, 11, 12, 13, 14]
: L1 -> [3, 5, 6]
: L2 -> 14



** MapReduce y map-reduce

- Básicamente es lo mismo, pero...
- =map=, =reduce= (entre otras) son parte de lenguajes funcionales.
- =MapReduce= es la aplicación de esta idea aplicada a problemas /vergonzosamente/ /paralelos/.
  - Ver la carpeta =docs= para el artículo de *Google* sobre =MapReduce=.


** GNU Parallel

#+begin_src sh
find ./data/books -type f | parallel -j0 egrep -i  '\[\[:digit:\]\]' {} | awk '{s+=$1} END {print s}'
#+end_src

#+RESULTS:
: 1480715


- *¿Puedes identificar las partes =map= y =reduce=?*
- Esto ya es un =MapReduce=.


** MapReduce en Hadoop

- A nivel programático:
  - /Data/ de entrada
  - Programa MapReduce
  - Configuración
  - Subtareas: =map= y =reduce=


** MapReduce: /Mapper/

- Hadoop divide la entrade de datos al /job/ MapReduce en pedazos de tamaño fijo llamados /input splits/.
- Hadoop crea una tarea =map= para cada /input split/.
- =map= escribe al /file system/ local.
  - Si el =reducer= tiene éxito se borra la salida del /mapper/.

** Map only

[[file:./imagenes/map_only.png]]


** MapReduce: /Reducer/

- La entrada es la salida de (posiblemente) todos los /mappers/.
- Estas se transmiten vía red al nodo donde corre el /reducer/.
- La salida se guarda en el =HDFS=.

** Map, One reduce

[[file:./imagenes/map_one_reduce.png]]

** MapReduce

[[file:./imagenes/map_reduce.png]]


** MapReduce: /Combiner/

- Es una medida de optimización.
- Es para ahorrar ancho de banda.
- Una especie de /reducer/ local.
- No es parte (estrictamente) del MapReduce
  - Por eso no lo había mencionado.


** Ejercicio

- Diseñe el **MapReduce** para lo siguiente:
  - Encontrar el máximo de un conjunto de datos.
  - Encontrar el promedio y desviación estándar de unos datos.
  - Encontrar el top 10 de una cantidad.
  - Contar por grupo

* Apache Haoop 2.x: YARN



* Hello World!: Word count

** Word count

- Es el ejemplo /Hola Mundo/ de Apache Hadoop.
- No sólo eso, es el ejemplo que se utiliza en el trabajo seminal
  - *MapReduce: Simplified Data Processing on Large Clusters* /(2006)/.
  - En la carpeta =docs= como ya había dicho.
- Solamente 1 =Map= y 1 =Reduce=.


** Word count

- *mapper*
  - =k1= -> nombre de archivo
  - =v1= -> texto del archivo
  - =k2= -> palabra
  - =v2= -> "1"

- *reducer*
  - =k2= -> palabra
  - list(v2) -> (1,1,1,1,1,1,..., 1)

  Suma los "1" y produce una lista de

  - k3 -> palabra
  - v3 -> suma

** Word count

[[file:./imagenes/word_count.png]]

** Pseudocódigo

#+begin_example
map (String key, String value)
   for each word w in value
      Emit(w, 1)

reduce (String key, Iterator values)
   int wordcount = 0
   for each v in values
      wordcount += v
      Emit(key, wordcount)

#+end_example

** Mockup

- Ver los archivos =word_count.py= y =mapreduce.py= en la carpeta =mock=.

#+begin_example
chmod +x word_count.py
python word_count.py
#+end_example

- Este es un ejemplo de mentiritas, no usa Apache Hadoop.


* Pig

** Pig

- Proyecto de Apache
- Abstracción encima de Hadoop
  - /Pig Latin/ compila a =MapReduce=
  - En cierta forma /Pig Latin/ es para analistas, /data scientist/ y estadísticos.
  - =MapReduce=  es para programadores (aunque los /data scientist/ deberían de poder hacerlo también)

** Pig

- Pig es un /data flow programming language/
- Es decir,
  - Ejecuta paso a paso
  - Cada paso es una transformación de datos
- En cambio =SQL= es un conjunto de /constraints/ que en conjunto definen el resultado buscado.

** Pig

- ¿Qué cosas puede hacer?
  - =joins=
  - =sorts=
  - =filters=
  - =group by=
  - /User defined functions/ =UDF='s

** Pig

- ¿Qué cosas *puedo* hacer?

  - =ETLs=
    - Limpiar.
    - /Joins/ gigantes.

  - Búsqueda en /Raw/.

** Pig

- Componentes
  - /Pig Latin/
  - =Grunt=
    - Local
    - MapReduce
  - =Pig compiler=

** Pig

- Es posible ejecutar también /scripts/ de /Pig Latin/ (terminación =.pig=) sin entrar a =grunt=.

#+begin_example
pig script_file.pig

# Si quieren pasar parámetros
pig -p var=bla/bla var2=bla/bla/bla script_file.pig
#+end_example

- Y usarse desde programas en =Java= con la clase =PigServer=.
  - Como una especie de =JDBC=, pero para /Pig Latin/.


** Pig: /Building blocks/

Fields
#+begin_example
'Adolfo'
#+end_example

Tuplas
#+begin_example
('Adolfo', 3, 8.17, 23)
#+end_example

/Bags/
#+begin_example
{('Adolfo', 3, 8.17, 23), ('Paty', 3.14, 9, 'A')}
#+end_example

** Ejercicio

1. Crear una carpeta =rita= en el =HDFS=.
2. Agregar los siguientes archivos:
   - =airports.csv=
   - =plane_data.csv=
   - =carriers.csv=
3. Ejecutar =grunt=.

#+begin_example pig
# Pig latin puede ejecutar comandos del hdfs
cat rita/airports
# Especificando el separador (,) y el esquema (no es necesario)
airports = load 'rita/airports' using PigStorage(',') as (iata:chararray, ..., latitude:float, ...);
# Hasta este momento se ejecuta todo...
dump airports;
# El comando store guarda al HDFS y también ejecuta todo.
#+end_example

** Ejercicio

#+begin_example pig
a_imprimir = limit airports 5;
por_estado = group airports by state;
describe por_estado;
explain por_estado;
illustrate por_estado;
# itera sobre cada elemento del bag
conteo = foreach por_estado generate group count_star(airports);
ordenados = order conteo by $1 desc;
top_five = limit ordenado 5;
unicos = distinct conteos;
muestreo = sample por_estado 0.1;
filtrados = filter conteos by substring(group, 0, 2) == 'W';
mayores = filter conteos by $1 > 50;
#+end_example

** Ejercicio: Trucos del =foreach=
#+begin_example pig
# Proyectar
foreach airports generate iata, airport, country;

# Expresiones posicionales
# $1 -> iata
# $3 -> city
# $5 -> country

# Rangos
# ..country, iata..country, latitude..

# Tokenizar
tokens = foreach lineas generate tokenize(linea);
# Cada fila obtenida es un bag de palabras.
#+end_example

** Pig: JOINS

1. Cargamos fuente 1
2. Cargamos fuente 2
3. Unimos las fuentes (/bags/) mediante una llave
4. Súper simple

Pig soporta /inner joins/ (valor por omisión), /left outer joins/ (y
/right/ también) y /full outer/ joins.


#+begin_example pig
fuentes_unidas = join fuente1 by (keys) [left|right|full outer] fuente2 by (keys);
#+end_example

Además =Pig= soporta =cogroup= además de los =joins= (el =cogroup=
preserva la estructura de las fuentes y crea tuplas por cada llave)

#+begin_example pig
fuentes_unidas = cogroup fuente1 by (keys) fuente2 by (keys);
#+end_example


** Pig: Ejemplo de JOINs y COGROUPs

#+begin_example

# Fuentes de datos

mascotas: (dueño, mascotas)
----------------------
(Adolfo, tortuga)
(Adolfo, pez)
(Adolfo, gato)
(Paty, perro)
(Paty, gato)

amigos: (amigo1, amigo2)
----------------------
(Diana, Adolfo)
(Gabriel, Adolfo)
(Shanti, Paty)


COGROUP mascotas by dueño, amigos por amigo2;
---------------------------------------------
(Adolfo, {(Adolfo, tortuga), (Adolfo, pez), (Adolfo, gato)}, {(Diana, Adolfo), (Gabriel, Adolfo)})
(Paty, {(Paty, perro), (Paty, gato)}, {(Shanti, Paty)})

JOIN mascotas by dueño, amigos por amigo2;
-------------------------------------------
(Adolfo, tortuga, Diana)
(Adolfo, tortuga, Gabriel)
(Adolfo, pez, Diana)
(Adolfo, pez, Gabriel)
(Adolfo, gato, Diana)
(Adolfo, gato, Gabriel)
(Paty, perro, Shanti)
(Paty, gato, Shanti)
#+end_example

** Aclaraciones sobre GROUP y FLATTEN

- =FLATTEN= elimina un nivel anidamiento

#+begin_example pig
# Datos:
# (Adolfo, (tortuga, pez, gato))
# (Paty, (perro, gato))
# FLATTEN eliminaría los bags internos
(Adolfo, tortuga)
(Adolfo, pez)
(Adolfo, gato)
(Paty, perro)
(Paty, gato)
#+end_example

- =GROUP .. BY= organiza los /bags/ en /bags/
#+begin_example pig
# Siguiendo con los datos anteriores de mascotas
GROUP mascotas BY dueño;

# ( Adolfo, {(Adolfo, tortuga), (Adolfo, pez), (Adolfo, gato)} )
# ( Paty, {(Paty, perro), (Paty, gato)} )

#+end_example

- En cierto sentido =FLATTEN= y =GROUP .. BY= son operaciones inversas
  entre sí.

** Tarea

Crear un =wordcount= para los archivos en =data= usando =Pig=


* Hive

** Hive

- Proyecto de Apache.
- _Abstracción_ pra modelar y procesar datos en Hadoop.
- Proveé de una manera de estructurar datos guardados en el =HDFS=.
- Permite crear _queries_ muy similares a =SQL= (llamado =HQL=) y correrlos contra los datos.
- Contiene un almacén de metadatos (=HCatalog=), que además puede ser compartido con otras interfaces como =Pig=, =MapReduce=, etc.
- Da Acceso al =HDFS= y =HBASE=.

** Bibliografía recomendada

- Sitio web de Hive
- Hadoop: The Definitive Guide
- Programming Hive


** Arquitectura de Apache Hive

[[file:./imagenes/hive-remote.jpeg]]

** Ejercicio: Crear RITA en Hive

#+begin_example sql
CREATE EXTERNAL TABLE carriers(
code STRING,
description STRING
)
COMMENT 'Códigos de carriers'
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' STORED AS TEXTFILE;

CREATE EXTERNAL TABLE airports(
iata STRING,
airport STRING,
city STRING,
state STRING,
country STRING,
latitude FLOAT,
longitude FLOAT
)
COMMENT 'Códigos y localización de aeropuertos'
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' STORED AS TEXTFILE;

CREATE EXTERNAL TABLE planes_data(
tailnum STRING,
type STRING,
manufacturer STRING,
issue_date STRING,
model STRING,
status STRING,
aircraft_type STRING,
engine_type STRING,
year STRING
)
COMMENT 'Datos de algunos aviones mencionados en RITA'
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' STORED AS TEXTFILE;
#+end_example


** Ejercicio: Crear RITA en Hive

#+begin_example sql
CREATE EXTERNAL TABLE rita(
Year STRING,
Month STRING,
DayofMonth STRING,
DayOfWeek STRING,
DepTime STRING,
CRSDepTime STRING,
ArrTime STRING,
CRSArrTime STRING,
UniqueCarrier STRING,
FlightNum STRING,
TailNum STRING,
ActualElapsedTime INT,
CRSElapsedTime INT,
AirTime INT,
ArrDelay INT,
DepDelay INT,
Origin STRING,
Dest STRING,
Distance FLOAT,
TaxiIn INT,
TaxiOut INT,
Cancelled INT,
CancellationCode STRING,
Diverted INT,
CarrierDelay INT,
WeatherDelay INT,
NASDelay INT,
SecurityDelay INT,
LateAircraftDelay INT
)
COMMENT 'Base de datos conteniendo los vuelos de 1987 a 2008'
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' STORED AS TEXTFILE LOCATION '/user/hive/rita';
#+end_example

** Ejercicio: RITA y HIVE

#+begin_example sql
-- ¿Se crearon bien las tablas?
show tables;

-- ¿Se cargó bien rita?
select * from rita limit 5;

-- Cargamos airports
load data inpath 'hive/datawarehouse/rita/catalogs/airports.csv'
overwrite into table airports;

-- Probamos
select * from airports where iata='SAN';

-- ¿Y si hacemos un JOIN?

select * from rita join airports on (rita.Origin = airports.iata) limit 10;
-- ¿Qué pasó?
#+end_example

** Tarea

Crear un =wordcount= para los archivos en =data= usando =Hive=



* Ejercicios

** RITA, HIVE y PIG
#+begin_example pig
register /home/hduser/hadoop-src/pig-0.12.0/contrib/piggybank/java/piggybank.jar;
define replace org.apache.pig.piggybank.evaluation.string.REPLACE;
define substring org.apache.pig.piggybank.evaluation.string.SUBSTRING;
define s_split org.apache.pig.piggybank.evaluation.string.Split;
define reverse org.apache.pig.piggybank.evaluation.string.Reverse;

airports = LOAD '/user/nano/rita_catalogs/airports.csv'
USING PigStorage(',')
AS
(iata:chararray,airport:chararray,city:chararray,
state:chararray,country:chararray,latitude:float,longitude:float);

fixed_airports = foreach airports
                 generate replace(iata, '"', ''),
                          replace(airport, '"', ''),
                          replace(city, '"', ''),
                          replace(state, '"', ''),
                          replace(country, '"', ''),
                          latitude, longitude;

store fixed_airports into '/user/pig/airports-fixed' using PigStorage(',');
#+end_example

** RITA y HIVE: Joins

#+begin_example sql
load data inpath 'pig_fixed/airports/part-m-00000'
overwrite into table airports;

-- ¿Y ahora?
select * from airports where iata='SAN';

select * from rita join airports on (rita.Origin = airports.iata) limit 10;
#+end_example



** Tarea: Pig y Hive

- Crear una tabla de RITA limpia (usando =PIG= y =HIVE=)
- Ejecutar dos exploraciones de las tareas de analítica de PostgreSQL,
  uno usando =PIG= y otro usando =HIVE=.

** ¡Más ejercicios!


- Usando =RITA= (lo que tengan cargado en su nodo), calcule:
  - Con =Pig=:
    - El número de vuelos por aeropuerto.
    - ¿Cuál es el más activo?
  - Con =Hive=:
    - Número de =km= por avión.
    - ¿Cuál es el /top/ 5?
    - ¿Sería más fácil en =Pig=?





* HCatalog

** HCatalog

- Está incorporado a =Hive= desde la versión =0.11=.

- Es una capa administrativa de tablas y almacenamiento que permite
  que diferentes herramientas de procesamiento de datos (=Pig=,
  =MapReduce=) puedan leer y escribir más fácilmente del =HDFS=.
- Contiene una abstracción que presenta una vista relacional de los
  datos contenidos en el =HDFS=, asegurando que los usuarios no se
  preocupen dónde o en que formato están almacenados los datos.

** HCatalog

- Utiliza el =DDL= de =Hive=.
- Provee interfaces de escritura y lectura para =Pig=, =MapReduce= y
  =Hive=.
- Usa la línea de comandos para manejar la definición de los datos y
  metadatos.
- =HCatalog= presenta los datos de manera relacional.
- Los datos son guardados en tablas y las tablas en bases de datos.

- =WebHCat= es la interfaz API =REST= de =HCatalog=.

** HCatalog: Flujo de datos

- Usuario 1 copia datos al HDFS
#+begin_example
hadoop distcp file:///data/books/pg2047.txt hdfs://data/20140430/books
hcat "alter table books add partition (ds='20140430') location 'hdfs://data/20140430/books'"
#+end_example

- Usuario 2 usa =Pig= para limpiar y preparar los datos.
  - =HCatalog= mandará al =JMS= un mensaje de que la información está disponible.

#+begin_example
A = load 'books' using HCatLoader();
B = filter A by date = '20140430';
...
store Z into 'procesados' using HCatStorer("date=20140430");
#+end_example

- Usuario 3 realiza cierta analítica
#+begin_example
select col1, count(col3)
from procesados
where date  = '201340430'
group by col1;
#+end_example

* HBase

** HBase

- Es un almacén distribuido =NoSQL=.
- Proveé acceso aleatorio a los datos guardados en el =HDFS=.
  - A diferencia de =HDFS=, que es secuencial.

** Bibliografía

- Sitio web de HBase
- HBase: The definitive Guide
- HBase in Action

* Apache Sqoop

** Apache Sqoop

- Herramienta para importar eficientemente /data/ desde =RDBMS= a Hadoop (=HDFS,
  Hive, Hbase=) y viceversa.
- Soporta cualquier =RDBMS= que tenga conexión =JDBC= (=PostgreSQL, MySQL, Oracle, Teradata=, etc.).
- Tiene soporte nativo para =MySQL= y =PostgreSQL=.

** Apache Sqoop

[[file:./imagenes/sqoop.png]]


** Bibliografía

- Documentación de Apache Sqoop
- Apache Sqoop Cookbook

* Apache Flume

** Apache Flume

- Componente para la captura (_ingesta_) de datos basados en eventos a Hadoop.

** Bibliografía

- Documentación de Flume
- Using Flume


** Ejercicio: RITA del tingo al tango
y
* Apache Flume

* Apache Spark

* Oozie

* Hue


** Ejercicio: Armar un /cluster/

- El objetivo es reproducir el siguiente diagrama arquitectónico (por
  lo menos).

[[file:./imagenes/layout.png]]


- Use =Vagrant=, =chef= y =berkshelf=.


* Compresores

** Instalación

#+begin_example

# Snappy
sudo apt-get install libsnappy1 libsnappy-dev

#LZO
sudo apt-get install liblzo2-2 liblzo2-dev#+end_example
#+end_example

** Tipos

[[file:./imagenes/compresores.png]]

* Misceláneos

** Tips

- =Reduce= es regularmente más intensivo en cuanto consumo de recursos que =Map=
  - Usa =Combiners=.
  - Explora tus datos antes
    - Como están distribuidos es muy importante.
    - Quizá Hadoop no sea lo correcto.

- En la vida real, instala desde una distribución: *BigTop*, *Horton* o *Cloudera*.
  - Y =Vagrant=

** /Small File Problem/

* Disclaimer

Algunas imágenes se tomaron de los libros /Professional Hadoop Solutions/
de *Wrox* y de la página de [[http://hortonworks.com/hadoop/yarn/][Hortonworks]]. Las otras son mías.

Las tablas de la sección /cluster/ de Hadoop, se tomaron de [[http://hortonworks.com/][Hortonworks.]]
